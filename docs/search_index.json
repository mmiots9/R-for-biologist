[["index.html", "Learn R for biologist It‚Äôs easier than a western blot About", " Learn R for biologist It‚Äôs easier than a western blot Matteo Miotto 2025-05-06 About Hi, here we are. You really don‚Äôt know how happy I am for both of us: I‚Äôll learn how to ‚Äúteach‚Äù, while you‚Äôll learn a coding language that will revolutionize your life, I‚Äôm sure of it hahaha We‚Äôll see how to speed up analyses, how to automate them, how to untie ourselves from that cancer that is Excel, how to make beautiful graphs and how to gain time to do other things. First, you need to start with the installation and some basic concepts. Let‚Äôs start this beautiful journey! "],["installation.html", "1 Installation", " 1 Installation First of all, we need to install R to be able to use it. To do so, go to this link and follow the instruction for the installation. ‚Ä¶ Done?! Great, now you can start using R on your pc, directly from your terminal: but since it is an inconvenient and ancient way, let‚Äôs take advantage of the potential and functions of RStudio to be true professionals. Go here and download this amazing software. ‚Ä¶ Done?! Perfect, let‚Äôs open it. "],["first-step-in-rstudio.html", "2 First step in RStudio Console", " 2 First step in RStudio Don‚Äôt be intimidated by all these windows that you see open, we will slowly dig into them one by one. Console Let‚Äôs start from the console, that is the space where we will start writing the first commands and where their outputs are displayed. Let‚Äôs try a basic thing right away, using R as a calculator, type 2 + 2 on the console and press enter. 2+2 [1] 4 Easy right? It returns 4. Now you can start playing a bit with the various operations, you can also write long expressions and you will see how the result will be shown immediately. ‚Ä¶ Ok, but‚Ä¶ usually we need the results of some operation to be saved, or often we want to use the same number several times without having to remember it, and this is where the concept of variable comes into play. "],["variables.html", "3 Variables Create a variable Use a variable Variable names Overwriting variables List all variables Delete variables Type of variables Exercises", " 3 Variables As mentioned, we often need some number (eg a concentration), some table (eg table of differentially expressed genes), information (eg the name of a protein) etc. to be saved in R to be able to use them later in the analysis. This is where variables come into play, and now we‚Äôll see how to create them, how to reuse them, and what kinds of variables exist. Create a variable To create a variable we write name_of_the_variable &lt;- what_to_save (you can either use = instead of &lt;-, even if the former is usually used for declaring arguments in a function, but we‚Äôll see it later). Now, write this to the console, click Return/send on the keyborard, and see what happens: myvar &lt;- 5 On the console nothing happens, but something appeared in the window called Environment (if yours is different, it may be that there is the ‚ÄúList‚Äù view setting instead of ‚ÄúGrid‚Äù in the blue box, you can change it to your liking). Here in details the info given for each variable: Name: name of the variable Type: type of the variable (don‚Äôt worry, we‚Äôll see in a minute what this means) Length:: the length of the variable (how many items it contains) Size: how much memory that variable occupies Value: the value of our variable If we want to create multiple variables with the same value we can do this: var1 &lt;- var2 &lt;- var3 &lt;- 20 print(var1) [1] 20 print(var2) [1] 20 print(var3) [1] 20 Use a variable Ok, but once stored, how to we use a variable? Easy, we just need to type it in the console (or start writing the first letters of its name and press Tab to show RStudio suggestions). If, for example, we want to calculate the power of our variable we should write: myvar ** 2 [1] 25 And here is the result (to elevate to the power we can either use ** or ^). And what if we want to store this result? As before: myvar_power &lt;- myvar ** 2 print(myvar_power) [1] 25 Here we use print() function, but in R we can also just write the name of the variable to see it. Variable names As in everything, even in naming variables there are rules and guidelines. Don‚Äôt be scared, they are simple and will make your life easier, let‚Äôs see them together. Rules: Variable name CANNOT start with a character other than a letter Variable name can contain both letters and numbers (case sensitive, uppercase and lowercase matter) Variable name may contain as special characters only the dot . or the underscore _ Guidelines: Since the name of the variable must be useful, its name must suggest something: for example, the variable myvar was previously defined, whose meaning is equal to 0 (so avoid these names), while myvar_power is more indicative, as it tells us that it is raised to a power Variables are normally written in lowercase letters, except for those you want to remain constant in your analysis, which in other languages are written in uppercase (this does not make them immutable, but suggests this feature within the script) Use underscores rather than periods as special characters in variable names if you can If the variable name contains more than one word, you can separate them with an underscore (as in the example) or use the camel case (myvarPower) or the Pascal case (MyvarPower) Be consistent within the script: if you decide to use the Pascal case, always use the Pascal case in that script Overwriting variables Attention! Variables can be overwritten (unrecoverable action). To override a variable, simply assign that variable a new value: print(myvar) [1] 5 myvar &lt;- 9 print(myvar) [1] 9 Now myvar is equal to 9, and there is no way back‚Ä¶ This feature is useful for saving space and not cluttering up too much with variables that are okay to change often, but it can be risky. So be careful when naming variables. List all variables A useful way to avoid overwriting an important variable is to list the variables. We know that in RStudio they are all present in the Environment window, but what if we weren‚Äôt in RStudio but elsewhere (for example in the terminal)? The answer is simple, let‚Äôs use the ls() function ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;var1&quot; &quot;var2&quot; [8] &quot;var3&quot; Here are our variables. Note how I called this command with the name function: we will cover this concept later, for now you just need to know that they exist and that they can be identified immediately by the fact that after the name there is a pair of round brackets. Delete variables To delete a variable, use the rm() function and insert the variable to be deleted: # create a variable to_remove &lt;- 1213 # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;to_remove&quot; &quot;var1&quot; [8] &quot;var2&quot; &quot;var3&quot; # delete just-created variable rm(to_remove) # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;var1&quot; &quot;var2&quot; [8] &quot;var3&quot; As we see, in the second case the to_remove variable has been removed. What if I want to remove multiple variables? Let‚Äôs put multiple variable names inside the rm() function separated by commas: # create various variables to_remove &lt;- 1213 to_remove2 &lt;- 685 # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;to_remove&quot; &quot;to_remove2&quot; [8] &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; # delete just-created variables rm(to_remove, to_remove2) # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;var1&quot; &quot;var2&quot; [8] &quot;var3&quot; The two variables have been removed. But looking closely at these codes, we see that some start with # and are not evaluated. What are they? These are the comments, i.e.¬†messages that you will write in the scripts (and we will see later how to create them) to help you understand what you are doing. They are actual comments that you can add, and will not be ‚Äúevaluated‚Äù as code as the line starts with #. Type of variables So far so linear, right? Great, it will continue to be as easy üôÉ. Let‚Äôs see what are the basic types of variables that exist in R: Numeric: numbers, can be integer (whole numbers) or double (decimal numbers) Character: characters, therefore strings of letters (words, sentences, etc.) Boolean: TRUE or FALSE, are a special type of variable that R interprets in its own way, but super super super useful Factor: similar to character, but with peculiar features (and memory saving), often used for categorical variables such as male/female, heterozygous/wild-type We will see each type of variable in detail in next chapters. To find out what type a variable is we use the typeof() function: typeof(myvar) [1] &quot;double&quot; We see that myvar is a double (although it is an integer value), this is because R basically interprets every number as a double, so as to increase its precision and the possibility of operations between various numbers without having type problems. Exercises Ok, this chapter was long enough, let‚Äôs do some exercises to fix well these concepts. Exercise 3.1 Create 3 variables indicating the weights of 3 mice. Solution mice1 &lt;- 5.8 mice2 &lt;- 4.8 mice3 &lt;- 7.5 print(mice1) [1] 5.8 print(mice2) [1] 4.8 print(mice3) [1] 7.5 Exercise 3.2 Create the variable sum_weights as the sum of the weights of those 3 mice. Solution sum_weights &lt;- mice1 + mice2 + mice3 print(sum_weights) [1] 18.1 Exercise 3.3 Create 4 other variables for other 4 mice that weight 20 g. Then you realize you did a mistake and you choose to delete 3 of them and change the fourth to 7.7. Solution # create 4 variables mice4 &lt;- mice5 &lt;- mice6 &lt;- mice7 &lt;- 20 # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;mice1&quot; &quot;mice2&quot; &quot;mice3&quot; &quot;mice4&quot; [8] &quot;mice5&quot; &quot;mice6&quot; &quot;mice7&quot; &quot;myvar&quot; &quot;myvar_power&quot; &quot;sum_weights&quot; &quot;var1&quot; [15] &quot;var2&quot; &quot;var3&quot; # delete 3 of the just-created variables rm(mice5, mice6, mice7) # list all variables ls() [1] &quot;all_string&quot; &quot;gene1&quot; &quot;gene2&quot; &quot;mice1&quot; &quot;mice2&quot; &quot;mice3&quot; &quot;mice4&quot; [8] &quot;myvar&quot; &quot;myvar_power&quot; &quot;sum_weights&quot; &quot;var1&quot; &quot;var2&quot; &quot;var3&quot; # change the value of one variable mice4 &lt;- 7.7 print(mice4) [1] 7.7 Alright, if you have done all the exercises (and I‚Äôm sure you have), we can move on to the next chapter in which we briefly talk about scripts and saving the environment. "],["scripts-chapter.html", "4 Scripts Save Source a script", " 4 Scripts Ok, let‚Äôs take a break before all the theoretical chapters to discuss about scripts. I always say this word when talking to you about my job, but what are they? A script is essentially an ordered list of commands, used to not have to write again and again the same commands on the console. It can contain everything you type in the console, like functions, operations and even comments; in this way, you can do an analysis and store it as a ‚Äútext‚Äù file that you can use as reference (so you know what you have done), use for another analysis, edit, share with others‚Ä¶ You will always write code in scripts, for all these reasons. Let‚Äôs see how to create a script, write some code in it and run part of the script or even a small part of it in RStudio. In the video we see a series of actions: To create a script, click the button on top left, then select ‚ÄúR Script‚Äù (or File -&gt; New File -&gt; R Script) Start writing the commands (even comments) To run a single line click ‚ÄúRun‚Äù To run the whole script (R starts from the beginning) click ‚ÄúSource‚Äù) Suggestion: Comment, comment, comment everything. I know it is time consuming, but you will be so happy and greatful to read comments that help you understand what you‚Äôve done months (or weeks) before. Save To save a script ‚ÄúFile -&gt; Save‚Äù or the usual shortcut ‚ÄúCtrl-S‚Äù (Windows) or ‚ÄúCmd-S‚Äù (MacOS). Use them for all the exercise from now on! Source a script Source a script from the console instead of having to open it and source from RStudio is very useful when you want to save time and create re-usable pipelines or analysis. Sourcing a script means to execute all the line of codes in that script. It can be archieved by doing source(\"path_to_file\"). Now, head to the next chapter in which we will talk about numbers. "],["numbers.html", "5 Numbers Operations Rounding Tranform to type number Exercises", " 5 Numbers I know you know what numbers are, and we‚Äôve already seen that we have two types of numbers in R: integers and doubles; so, let‚Äôs move to see them in action. Operations First of all, here is how to do basic mathematical operation in R: # Plus, minus, multiply, divide 5 + 4 - 2 * 3 / 2 [1] 6 # Power 4 ** 3 [1] 64 # Logarithm log(100) # base e [1] 4.60517 log10(100) # base 10 [1] 2 log2(100) # base 2 [1] 6.643856 log(100, base = 3) # choose the base [1] 4.191807 # Natural exponential exp(2) [1] 7.389056 # Square root sqrt(9) [1] 3 An interesting operator is the modulus (%%) which returns the remainder of a division, for example: 7 %% 3 [1] 1 This can be useful to evaluate if a number is even or odd by calculating the remainder of the division x / 2 (so using x %% 2): if it is 0, the number x is even, otherwise it is odd. 11 %% 2 [1] 1 12 %% 2 [1] 0 Rounding Another thing that we usually want to do is to round decimal number, especially after log transformation or division. To do so, we have 3 functions: # Round to n decimal places round(x = 1/3, digits = 2) [1] 0.33 # Round to upper integer ceiling(10.2) [1] 11 # Round to lower integer floor(14.9) [1] 14 Look how ceiling and floor do not take into account the decimal part, even if it is greater or lower than 0.5. Tranform to type number Sometimes you want to transform a string that contains a number to a numeric type in R. I know we haven‚Äôt covered strings yet (next chapter will be on them), but let‚Äôs do a bit step forward now just to see this super useful function, that we use a lot when dealing with dataframes. To do so, we‚Äôll use the function called as.numeric() mystring &lt;- &quot;15&quot; # This is a character, can you guess why R interpret it as a character? typeof(mystring) [1] &quot;character&quot; mynumber &lt;- as.numeric(mystring) typeof(mynumber) [1] &quot;double&quot; Wow, you will find this super super super useful. Exercises Let‚Äôs now put in practice what we have seen in this chapter and in the previous (remember? the script‚Ä¶ I want you to write a script with these exercises and save it) Exercise 5.1 The results of a Real-Time PCR indicate that your triplicates for FOXP1 have these Ct: 22.4, 22.31, 22.24. Calculate the mean value and print it rounded to 2 decimal places. Solution # calculator solution mean_result_calc &lt;- (22.4 + 22.31 + 22.24) / 3 mean_result_calc &lt;- round(mean_result_calc, digits = 2) print(mean_result_calc) [1] 22.32 # BETTER solution rep1 &lt;- 22.4 rep2 &lt;- 22.31 rep3 &lt;- 22.24 n_rep &lt;- 3 mean_res_better &lt;- (rep1 + rep2 + rep3) / n_rep mean_res_better_round &lt;- round(mean_res_better, digits = 2) print(mean_res_better_round) [1] 22.32 The second solution is better because every number is stored in a variable, that you then use to calculate the mean value. Exercise 5.2 Now calculate the sd of the data of exercise 5.1 and print the value rounded to upper integer, to lower integer and to a 4-digit decimal. Solution # Use only better solution, first calculate the variance var_calc &lt;- ((rep1 - mean_res_better)^2 + (rep2 - mean_res_better)^2 + (rep3 - mean_res_better)^2) / (n_rep - 1) # Now let&#39;s calculate sd sd_calc &lt;- sqrt(var_calc) sd_calc_ceil &lt;- ceiling(sd_calc) print(sd_calc_ceil) [1] 1 sd_calc_floor &lt;- floor(sd_calc) print(sd_calc_floor) [1] 0 sd_calc_round &lt;- round(sd_calc, digits = 4) print(sd_calc_round) [1] 0.0802 Ok, now you‚Äôre ready to learn about strings, let‚Äôs go. "],["character.html", "6 Character Concatenate strings Substring Substitution Grep Transform to type character Exercises", " 6 Character When you analyze a dataset, you don‚Äôt have only numbers: you‚Äôll have gene names, protein names, mouse strains and other variables that R calls character (I will alsways call them strings, but remember that for R they are character). A character can be a single letter, a word or even a whole text. To create a character variable you have to include it in \"\" (double quotes) or in ''(single quotes): mychar_d &lt;- &quot;SEC24C&quot; typeof(mychar_d) [1] &quot;character&quot; mychar_s &lt;- &#39;SEC24C&#39; typeof(mychar_s) [1] &quot;character&quot; Even numbers can be considered as character if included by ‚Äú‚Äú: weight_n &lt;- 12 typeof(weight_n) [1] &quot;double&quot; weight_c &lt;- &quot;12&quot; typeof(weight_c) [1] &quot;character&quot; See? Just adding the double/single quotes it changes everything. IMPORTANT: R interprets everything that is between a pair of single/double quotes as character, so if you forget to close a quote, nothing will work. Moreover, you can‚Äôt use a single quote to close a character opened with a double quote and vice versa. Concatenate strings Sometimes you want to concatenate different string into one single string, and we can to it with two similar functions: paste() and paste0(). The unique difference among them is that in the former you can decide what character to use to separate what you are concatenating (default is a white space), while the latter does not insert any character. For example, let‚Äôs say you have a variable condition and a variable treatment, and you want to concatenate them to create the variable sample; you will do: condition &lt;- &quot;control&quot; treatment &lt;- &quot;vector&quot; sample &lt;- paste(condition, treatment, sep = &quot;.&quot;) # default is &quot; &quot; print(sample) [1] &quot;control.vector&quot; sample0 &lt;- paste0(condition, treatment) print(sample0) [1] &quot;controlvector&quot; You concatenate as many character as you want, just put them all inside that function. Substring I have to tell you about substring, but I never used this function in my life. It is used to slice the character and take only a part of it. The function is called substr(), let‚Äôs see how it works: substr(mychar_s, start = 2, stop = 4) [1] &quot;EC2&quot; It needs a start and a stop value, they are both included in the result (in this case, character at position 2, 3 and 4 are sliced). Remember: in R everything starts at 1, so the first element is at position 1. In other languages it starts from 0, but in R it starts from 1. Extra funtcion never used but that can be important if we use substr and we don‚Äôt know how long is a character is nchar() nchar(mychar_s) [1] 6 Let‚Äôs say we want to take from position 3 until the end, but we don‚Äôt know how long is a character, we can do it with: substr(mychar_s, start = 3, stop = nchar(mychar_s)) [1] &quot;C24C&quot; What happened here is that the result of the function nchar(mychar_s) is used as the value to indicate the stop. We will usually use these method in R, especially when we don‚Äôt want to use memory to store a value that is used only once (as in this case). Substitution Alright, let‚Äôs move to something way more useful for our analysis: the function sub(), and its big brother gsub(). They are used to substitute part of a character with another, the only difference is that the former changes only the first occurrence, while the latter every occurrence. Let‚Äôs say we have the sample variable written as \"control_vector_3\" and we want to get rid of the underscores, we will do: sample2 &lt;- &quot;control_variable_2&quot; sub_only &lt;- sub(pattern = &quot;_&quot;, # the part of the string to search to substitute replacement = &quot; &quot;, # what to use to replace it x = sample2) # variable in which to search print(sub_only) [1] &quot;control variable_2&quot; gsub_all &lt;- gsub(pattern = &quot;_&quot;, # the part of the string to search to substitute replacement = &quot; &quot;, # what to use to replace it x = sample2) # variable in which to search print(gsub_all) [1] &quot;control variable 2&quot; This is the basic way of using these functions, but they can be very useful in complex analysis, but we will see them later on. Grep Further parents of substitution, are grep() and its brother grepl(): they are used to check if a particular pattern is present in a character variable. The first one returns the index (we‚Äôll see this concept in few chapters) or the value of the variable that match the pattern, the second one returns a boolean value (TRUE or FALSE) indicating the presence of the pattern in the variable. Let‚Äôs see few example right away: gene_to_test &lt;- &quot;GAPDH&quot; pattern_to_check_1 &lt;- &quot;GA&quot; pattern_to_check_2 &lt;- &quot;PH&quot; grep_1 &lt;- grep(pattern = pattern_to_check_1, x = gene_to_test, value = T) # default is FALSE (returns index) print(grep_1) [1] &quot;GAPDH&quot; grep_2 &lt;- grep(pattern = pattern_to_check_2, x = gene_to_test, value = T) # default is FALSE (returns index) print(grep_2) character(0) grepl_1 &lt;- grepl(pattern = pattern_to_check_1, x = gene_to_test) print(grepl_1) [1] TRUE grepl_2 &lt;- grepl(pattern = pattern_to_check_2, x = gene_to_test) print(grepl_2) [1] FALSE We can see that the grep_2 gives character(0) as result, meaning that it is an empty character variable. You are going to use grep a lot during the analysis, even if you find it not so interesting now, trust me üòâ. Transform to type character As for numbers, we can also convert a variable into a string: we use the function as.character(). You will use this function when a column of a dataframe is read as numeric while you want it to be read as character instead. ml_to_add &lt;- 35 typeof(ml_to_add) [1] &quot;double&quot; ml_to_add &lt;- as.character(ml_to_add) typeof(ml_to_add) [1] &quot;character&quot; This is an important lesson: in R variables can be overwritten with other types of data (this can‚Äôt be done in other languages such as Java, C++, ecc.). This can be both handy and risky at the same time: handy because we can save memory by overwriting useless variables, risky because we can overwrite a variable without checking if the type is maintained (maybe it is important). Exercises Exercise 6.1 In a list of genes to test, you found a gene called ‚ÄúNRG_1‚Äù and one called ‚ÄúSST R‚Äù. In the report you have to present you want to print out ‚ÄúInvolved genes are NRG1 and SSTR2‚Äù. How to do it? Solution gene1 &lt;- &quot;NRG_1&quot; gene2 &lt;- &quot;SST R&quot; # correct gene names gene1 &lt;- gsub(pattern = &quot;_&quot;, replacement = &quot;&quot;, gene1) gene2 &lt;- gsub(pattern = &quot; R&quot;, replacement = &quot;R2&quot;, gene2) # concatenate all the string all_string &lt;- paste(&quot;Involved genes are&quot;, gene1, &quot;and&quot;, gene2) print(all_string) [1] &quot;Involved genes are NRG1 and SSTR2&quot; Exercise 6.2 In a variable you have the gene name ‚ÄúHCN1‚Äù and in another you have its number of aa (890, as numeric!!!). Print out the string ‚ÄúHCN1 protein is 890 aa long‚Äù Solution gene &lt;- &quot;HCN1&quot; aa_num &lt;- 890 # one string solution to_print &lt;- paste(gene, &quot;protein is&quot;, as.character(aa_num), &quot;aa long&quot;) print(to_print) [1] &quot;HCN1 protein is 890 aa long&quot; Don‚Äôt worry if you had done it in another way or if it didn‚Äôt work. We are here to learn, and these exercises are done to challenge you on things you have seen in different pieces. Head on to the next lesson, in which we are going to talk about booleans. "],["boolean-chapter.html", "7 Boolean Boolean as results of comparison Logical operator Exercises", " 7 Boolean We finally arrived to the last data type that needs a deeper explanation: the boolean (or logical). In programming languages, there are a particular type of data (the boolean) representing what we can imagine as true or false, in R they are represented by two values: TRUE (or T) and FALSE (or F). We can see an example here: t_value &lt;- TRUE f_value &lt;- FALSE typeof(t_value) [1] &quot;logical&quot; typeof(f_value) [1] &quot;logical&quot; R calls them logical (as in computer science). They cannot be explained well if not contextualized, so let‚Äôs see some basic practical application here, and be patient, in next lessons we will see another examples. Boolean as results of comparison The most used way to get a logical is to evaluate a comparison, such as compare numbers, compare words ecc. Quickly, for numbers: num1 &lt;- 3 num2 &lt;- 4 # greater than num1 &gt; num2 [1] FALSE # greater equal than num1 &gt;= num2 [1] FALSE # less than num1 &lt; num2 [1] TRUE # less equal than num1 &lt;= num2 [1] TRUE # equality num1 == num2 [1] FALSE # not equal to num1 != num2 [1] TRUE IMPORTANT: I hope you notice that we used == to identity comparison. And I hope you get why we didn‚Äôt use only =. If you don‚Äôt, remember that one equal sign assign a value to a variable, so in this case you would have overwritten num1 with the value of num2. And what about character? They behave in this way: # define some variables ch1 &lt;- &quot;Mapk13&quot; ch2 &lt;- &quot;MAPK13&quot; ch3 &lt;- &quot;Xist&quot; # Greater than ch1 &gt; ch2 [1] FALSE ch3 &gt; ch1 [1] TRUE &lt; and &gt; with character, contrary to popular belief, do not take into account the number of characters of the string, but the comparison is based on alphabetical order, with lowercase letters that come prior uppercase ones (that‚Äôs why Mapk13 is not greater than MAPK13 and Xist). For this reason, two characters are equal only if they have the same letter/number/special character, in the same order and in the same case. For example: ch4 &lt;- &quot;Peg3&quot; ch5 &lt;- &quot;Peg3&quot; ch6 &lt;- &quot;peg3&quot; ch4 == ch5 [1] TRUE ch5 == ch6 [1] FALSE Logical operator Up to now, we have seen logical as a result of comparisons, but what if we want to compare or combine comparisons? I know it sounds silly, but here is an example of implementation of this concept. We will have to do with dataframes, vectors, matrix and other stuff, and we will always filter for some conditions: let‚Äôs say that we want to extrapolate data that are below 10 but above 5. These are two comparisons: x &lt; 10 and x &gt; 5. Here we combine two logicals, derived from the two comparisons. The main logical operators, the one that will be useful for us, are AND, OR and NOT AND The AND operator works as follows: TRUE and TRUE = TRUE TRUE and FALSE = FALSE FALSE and TRUE = FALSE FALSE and FALSE = TRUE An easy trick to remember is: if they are identical (both FALSE or TRUE), the result is TRUE, otherwise is FALSE. In R, AND operator is the &amp;, some examples: expr &lt;- 50 (expr &lt; 60) &amp; (expr &gt; 40) # TRUE &amp; TRUE [1] TRUE (expr &lt; 60) &amp; (expr &lt; 40) # TRUE &amp; FALSE [1] FALSE OR The OR operator works as follows: TRUE and TRUE = TRUE TRUE and FALSE = TRUE FALSE and TRUE = TRUE FALSE and FALSE = FALSE An easy trick to remember is: if at least one is TRUE, then the result is TRUE, otherwise if are all FALSE, the result is FALSE. In R, OR operator is the |, some examples: expr &lt;- 50 (expr &lt; 60) | (expr &gt; 40) # TRUE | TRUE [1] TRUE (expr &lt; 60)| (expr &lt; 40) # TRUE | FALSE [1] TRUE NOT The not operator is used to negate an expression, we have seen an example before, when we compared to numbers to see if they were not equal (!=). It is used before the expression to evaluate, in this form: expr &lt;- 50 !(expr &lt; 60) | (expr &lt; 40) # NOT TRUE | FALSE [1] FALSE !((expr &lt; 60) &amp; (expr &lt; 40)) # NOT (TRUE &amp; FALSE) [1] TRUE Here we see two important things: The not operator must be put before a parenthesis (if it contains a comparison) or directly before a TRUE or FALSE variable As for mathematical expressions, order and parenthesis matters: parenthesis and then from left to right Exercises Write a R script with the following exercises, they are level pro (I know you can): Exercise 7.1 Write the expression you would use to evaluate the following statement: we want to see if the patient is in his childhood (2-8 years) and one of its weight is less than 45 kg (as threshold variable) or if its mother has diabetes (we know it is true), and if its nationality is not USA. Tip: here we have 8 variables. I know I didn‚Äôt give neither the age nor the weight nor the nationality; you can create these variables and give the values you want. This exercise is to practice the writing and logical part. Solution # create patient variables patient_age &lt;- 5 patient_weight &lt;- 66 mother_diabetes &lt;- TRUE patient_state &lt;- &quot;Italy&quot; # set thresholds and values age_inf_threshold &lt;- 2 age_sup_threshold &lt;- 8 weight_sup_threshold &lt;- 45 nationality &lt;- &quot;USA&quot; ((patient_age &gt; age_inf_threshold &amp; patient_age &lt; age_sup_threshold) | (patient_weight &lt; weight_sup_threshold | mother_diabetes)) &amp; (patient_state != nationality) [1] TRUE It is best practice to use more parenthesis to help the readability by both human and R Ok, next chapter will be on vectors, and we will do another big step towards practical applications and exercise, with real biological questions. "],["vectors.html", "8 Vectors Named vectors Indexing One function can be applied to all elements Functions specific of vectors Operations between vectors Exercises", " 8 Vectors I know that in the previous chapters you were thinking ‚Äúok, but I rarely have one data for a variable, I usually have multiple data‚Äù, and you are right, so let‚Äôs see the first form of data organization in R, the base of all: the vector. A vector is a collection of data of the same type, for example all the weights of a group of people, all the names of your genes of interest, the expression levels of your genes of interest. To create a vector in R is simple, just use the function c() and put inside every data you need: heights &lt;- c(160, 148, 158, 170) genes &lt;- c(&quot;Adcyp1&quot;, &quot;Tle4&quot;, &quot;Psd95&quot;, &quot;Bip&quot;, &quot;Sst&quot;) heights [1] 160 148 158 170 genes [1] &quot;Adcyp1&quot; &quot;Tle4&quot; &quot;Psd95&quot; &quot;Bip&quot; &quot;Sst&quot; I told you that the data inside of a vector must be of the same type, in fact my_info &lt;- c(14, &quot;Most&quot;, 45, 5, TRUE) my_info [1] &quot;14&quot; &quot;Most&quot; &quot;45&quot; &quot;5&quot; &quot;TRUE&quot; Transforms everything into strings because we have a string in it. To confirm it, we can ask R to tell us the type of data we have in a vector by using the function typeof(): typeof(heights) [1] &quot;double&quot; typeof(my_info) [1] &quot;character&quot; Named vectors There is a particular type of vectors called named vectors that come in handy especially when creating graphs: every value in the vector has a ‚Äúname‚Äù associated to it. Imagine like giving a unique name-tag to each value; for example, associate an expression value to each gene. There are 3 ways of creating a named vector, I will show you here from the most fast to the most complex: # 1st method (the best) # create gene and value vector first genes &lt;- c(&quot;Adcyp1&quot;, &quot;Tle4&quot;, &quot;Psd95&quot;, &quot;Bip&quot;, &quot;Sst&quot;) expr_values &lt;- c(12, 200, 40, 1, 129) # assign names to the vector names(expr_values) &lt;- genes expr_values Adcyp1 Tle4 Psd95 Bip Sst 12 200 40 1 129 You can see here that every expression value has its own name. # 2nd method (as good as the first) # create gene and value vector first genes &lt;- c(&quot;Adcyp1&quot;, &quot;Tle4&quot;, &quot;Psd95&quot;, &quot;Bip&quot;, &quot;Sst&quot;) expr_values &lt;- c(12, 200, 40, 1, 129) # create a structure expr_values &lt;- structure(genes, names = expr_values) expr_values 12 200 40 1 129 &quot;Adcyp1&quot; &quot;Tle4&quot; &quot;Psd95&quot; &quot;Bip&quot; &quot;Sst&quot; This is the preferred method when the values are not in a standalone vectors but, for example, are a column of a dataframe. # 3rd method, the worst # directly create the named vector expr_values &lt;- c(&quot;Adcyp1&quot; = 12, &quot;Tle4&quot; = 200, &quot;Psd95&quot; = 40, &quot;Bip&quot; = 1, &quot;Sst&quot; = 129) expr_values Adcyp1 Tle4 Psd95 Bip Sst 12 200 40 1 129 This takes so long to write and it is never used, as you will always have the values and the names as columns of a dataframe or individual vectors already defined or obtained through a function. But, what is the main advantage of using named vectors? The possibility of extracting values of interest, this is called indexing. Indexing Indexing is one of the most used features, if not the used one, to retrieve data from vector, matrix, dataframes, ecc. There are many ways, let‚Äôs start with the named vectors and then move on with other strategies. But first, a tip: the key element in indexing is a pair of squared bracket [], in which you specify what to retrieve. So, remember: parenthesis after a function, square brackets to index. Named vectors To extract values from a named vector, we can put inside the square brackets a character (even a character vector or a character variable) with the name of the value we want to extract: # one value expr_values[&quot;Tle4&quot;] Tle4 200 # a vector expr_values[c(&quot;Tle4&quot;, &quot;Psd95&quot;)] Tle4 Psd95 200 40 # a variable to_extract &lt;- &quot;Bip&quot; expr_values[to_extract] Bip 1 Slicing Another method is to specify the position of the values we want to extract. First, there are two things to keep in mind: In R numeration of index starts at 1, so the first element is at position 1, the second 2 ecc (in other programming language it starts at 0) The length of a vector, meaning the number of items it is composed by can be extrapolate using the function length() Having set these concepts, let‚Äôs do some examples (I know it can be boring, but these are the fundamentals of data analysis. you really thank me in the future). # get the length of a vector print(length(expr_values)) [1] 5 Now we know that our expression values vector contains 5 elements, we can now start to index it: # get first element expr_values[1] Adcyp1 12 # get first and third element expr_values[c(1, 3)] Adcyp1 Psd95 12 40 # get from element 2 to 4 expr_values[2:4] Tle4 Psd95 Bip 200 40 1 # get from element 3 to the end expr_values[3:length(expr_values)] Psd95 Bip Sst 40 1 129 # get every element but the third expr_values[-3] Adcyp1 Tle4 Bip Sst 12 200 1 129 Ok, now that we have seen some example, we can look at some of them in more details: expr_values[2:4]: we haven‚Äôt seen this yes, but the expression &lt;value1&gt;:&lt;value2&gt; creates a vector with numbers from value1 to value2 expr_values[3:length(expr_values)]: since the function length() returns a value, we can use this function directly into the square brackets expr_values[-3]: the minus indicates except Using logicals We can also use logical and boolean values to index a vector. This is one of the most used way, and you will use it quite a lot. Why? Let‚Äôs see it in action. expr_values[c(T, F, F, T, T)] Adcyp1 Bip Sst 12 1 129 What has happened? When indexed with boolean, only the values in which is TRUE (T) are returned, and this is super cool. Do you remember in the previous chapter how we get boolean as results from an expression?! Great, so we can use expressions that returns TRUE or FALSE and use them to index a vector: # retrieve values &lt; 59 expr_values[expr_values &lt; 59] Adcyp1 Psd95 Bip 12 40 1 We can use also a more complicated expression: # retrieve values &lt; 30 or &gt; 150 expr_values[(expr_values &lt; 30) | (expr_values &gt; 150)] Adcyp1 Tle4 Bip 12 200 1 Do you see how useful it could be in an analysis? I‚Äôm sure you do, so let‚Äôs move on! One function can be applied to all elements We‚Äôve just seen a feature of the vectors: we can apply a function to each element of the vector. Previously we have evaluated if each element of the vector was &lt; 59 (or &lt; 30 or &gt; 150). Now, we will see more examples, starting from numeric vectors. # operations can be performed to each value expr_values * 2 Adcyp1 Tle4 Psd95 Bip Sst 24 400 80 2 258 expr_values / 10 Adcyp1 Tle4 Psd95 Bip Sst 1.2 20.0 4.0 0.1 12.9 # tranform a numeric vector to a character one as.character(expr_values) [1] &quot;12&quot; &quot;200&quot; &quot;40&quot; &quot;1&quot; &quot;129&quot; With character vectors we can do: # calculate number of characters nchar(genes) [1] 6 4 5 3 3 # use grep to see which genes start with letter T grep(pattern = &quot;^T&quot;, x = genes, value = T) # ^ indicates the start of the line, can you guess why we used it? [1] &quot;Tle4&quot; Functions specific of vectors Up to now nothing new, we have not seen any new function. But now we will see some new functions specific for vectors, starting, as always, from numbers: Example 8.1 Let‚Äôs say we have a mice and we want to test the time spent in a cage, in particular we want to calculate the sum, the mean and the sd of the time spent not in the center of the cage. # create the named vector areas &lt;- c(&quot;center&quot;, &quot;top&quot;, &quot;right&quot;, &quot;bottom&quot;, &quot;left&quot;) time &lt;- c(14, 22, 29, 12, 2) names(time) &lt;- areas # extrapolate data not in center not_center &lt;- time[-(names(time) == &quot;center&quot;)] # calculate mean, sum and sd sum_time &lt;- sum(not_center) mean_time &lt;- mean(not_center) sd_time &lt;- sd(not_center) # print results print(paste(&quot;The mice spent&quot;, sum_time, &quot;seconds not in the center of the cage, with a mean of&quot;, mean_time, &quot;seconds in each area and a sd of&quot;, sd_time)) [1] &quot;The mice spent 65 seconds not in the center of the cage, with a mean of 16.25 seconds in each area and a sd of 11.7862914721581&quot; This example implemented lots of things we have seen up to now, and it shows how on numerical vectors we can calculate sum, mean and sd; but these are not the only functions, we have also var (variance), min, max and others. We are going to see them later when needed. Sorting a vector Another important function is sort() as it gives us the possibility to sort the values of the vectors. sort(expr_values) Bip Adcyp1 Psd95 Sst Tle4 1 12 40 129 200 sort(expr_values, decreasing = T) Tle4 Sst Psd95 Adcyp1 Bip 200 129 40 12 1 By default, it sorts in ascending order, we can change the behavior by setting decreasing = T. Let‚Äôs see a couple of trick with sorting: features &lt;- paste0(&quot;gene&quot;, 1:20) features [1] &quot;gene1&quot; &quot;gene2&quot; &quot;gene3&quot; &quot;gene4&quot; &quot;gene5&quot; &quot;gene6&quot; &quot;gene7&quot; &quot;gene8&quot; &quot;gene9&quot; &quot;gene10&quot; &quot;gene11&quot; &quot;gene12&quot; &quot;gene13&quot; &quot;gene14&quot; [15] &quot;gene15&quot; &quot;gene16&quot; &quot;gene17&quot; &quot;gene18&quot; &quot;gene19&quot; &quot;gene20&quot; sort(features) [1] &quot;gene1&quot; &quot;gene10&quot; &quot;gene11&quot; &quot;gene12&quot; &quot;gene13&quot; &quot;gene14&quot; &quot;gene15&quot; &quot;gene16&quot; &quot;gene17&quot; &quot;gene18&quot; &quot;gene19&quot; &quot;gene2&quot; &quot;gene20&quot; &quot;gene3&quot; [15] &quot;gene4&quot; &quot;gene5&quot; &quot;gene6&quot; &quot;gene7&quot; &quot;gene8&quot; &quot;gene9&quot; What can we see here? First of all, a cool method to create a vector of words with increasing numbers (the combination paste and 1:20); then, we see that sorting has put ‚Äúgene2‚Äù after all ‚Äúgene1X‚Äù, because it sorts in alphabetical order. For this reason, it is recommended to use 01, 02, 03 ecc if we know that we have more than 9 elements (this works also for computer file names). ::: {.example #sort-names} Here we want to sort the expression levels based on their names. ::: expr_values[sort(names(expr_values))] Adcyp1 Bip Psd95 Sst Tle4 12 1 40 129 200 Tadaaa, we used sort on the names of expression levels and used the sorted names to index the named vector. Unique values As the title suggests, there is a function (unique()) that returns tha unique values of a vector. It is useful in different situations, we will use it a lot. The usage is so simple: # create a vector with repeated values my_vector &lt;- c(1:10, 2:4, 3:6) # can you guess the values of this vector without typing it in R? unique(my_vector) [1] 1 2 3 4 5 6 7 8 9 10 Logical vectors sum and mean In the example 8.1 we have seen how to calculate the sum and the mean of numerical vectors, but it can be done also on vectors full of boolean, and it can be very useful. I‚Äôll show you this example: Example 8.2 We have a vector representing the response to a treatment of different patient, the vector is coded by logicals. Here is the vector: c(T, F, T, T, T, F, T, F, F, T, F, F, F, F, F, T, T). Calculate the number and the percentage of responders (2 decimal places). # 1. Create the vector response &lt;- c(T, F, T, T, T, F, T, F, F, T, F, F, F, F, F, T, T) # 2. Calculate the number of responders n_responders &lt;- sum(response) # 3. Calculate the percentage of responders p_responders &lt;- mean(response) * 100 p_responders &lt;- round(p_responders, 2) print(paste(&quot;There are&quot;, n_responders, &quot;responders, corresponding to&quot;, p_responders, &quot;% of total patients.&quot;)) [1] &quot;There are 8 responders, corresponding to 47.06 % of total patients.&quot; What happened here? The trick is that R interprets TRUE as 1 and FALSE as 0. Remember this also for future applications. Operations between vectors Don‚Äôt give up, I know this chapter ha been so long, but now we will see the last part: the most important operations we can do between vectors. Mathematical First of all, mathematical operations: we can do mathematical operations between vectors only if the vectors are the same size, otherwise it will raise an error. This because each operation is performed one element by the corrisponding element of the other vector. Example 8.3 Let‚Äôs say we have 3 vectors representing the total amount of aminoacids found in three different samples for 5 proteins. We want to calculate, for each protein, the fraction of aminoacids in each sample. # 1. Define starting vectors proteins &lt;- c(&quot;SEC24C&quot;, &quot;BIP&quot;, &quot;CD4&quot;, &quot;RSPO2&quot;, &quot;LDB2&quot;) sample1 &lt;- c(12, 52, 14, 33, 22) sample2 &lt;- c(5, 69, 26, 45, 3) sample3 &lt;- c(8, 20, 5, 39, 48) names(sample1) &lt;- names(sample2) &lt;- names(sample3) &lt;- proteins # 2. Calculate sum of aa for each protein sum_aa &lt;- sample1 + sample2 + sample3 # 3. Calculate the fraction for each sample sample1_fr &lt;- sample1 / sum_aa * 100 sample2_fr &lt;- sample2 / sum_aa * 100 sample3_fr &lt;- sample3 / sum_aa * 100 # 4. Print the results sample1_fr SEC24C BIP CD4 RSPO2 LDB2 48.00000 36.87943 31.11111 28.20513 30.13699 sample2_fr SEC24C BIP CD4 RSPO2 LDB2 20.000000 48.936170 57.777778 38.461538 4.109589 sample3_fr SEC24C BIP CD4 RSPO2 LDB2 32.00000 14.18440 11.11111 33.33333 65.75342 Ok, I know it seems difficult, but let‚Äôs analyze each step: Here the new step is that we can assign the same values to multiple variables by chaining assignment statements Since the vectors have the same size, we can sum them together. IMPORTANT: the operation is performed based on position, NOT names. So if our vectors would have had the names in different order, we should have ordered them Different and common elements Usually we want to compare two vectors to find distinct and common elements (eg. upregulated genes in two analysis). To do it, we can use two functions: intersect() (which find the common elements between two vectors), and setdiff() (which returns the element in the first vector not present in the second). # 1. Define 2 vectors upregulated_1 &lt;- c(&quot;NCOA1&quot;, &quot;CENPO&quot;, &quot;ASXL2&quot;, &quot;HADHA&quot;, &quot;ADGRF3&quot;) upregulated_2 &lt;- c(&quot;ADGRF3&quot;, &quot;SLC5A6&quot;, &quot;NRBP1&quot;, &quot;NCOA1&quot;, &quot;HADHA&quot;) # 2. Find common genes common &lt;- intersect(upregulated_1, upregulated_2) # 3. Find different genes only_1 &lt;- setdiff(upregulated_1, upregulated_2) only_2 &lt;- setdiff(upregulated_2, upregulated_1) # 4. Print results print(cat(&quot;Common genes are:&quot;, paste(common, collapse = &quot;, &quot;), &quot;\\n&quot;, &quot;Genes specifically upregulated in analysis 1 are:&quot;, paste(only_1, collapse = &quot;, &quot;), &quot;\\n&quot;, &quot;Genes specifically upregulated in analysis 2 are:&quot;, paste(only_2, collapse = &quot;, &quot;), &quot;\\n&quot;) ) Common genes are: NCOA1, HADHA, ADGRF3 Genes specifically upregulated in analysis 1 are: CENPO, ASXL2 Genes specifically upregulated in analysis 2 are: SLC5A6, NRBP1 NULL Here you are. We can add three more notions: cat() is like print, but it accept special characters when pasting a vector, an additional argument collapse = \"&lt;chr&gt;\" can be added. It tells R to collapse all the element of a vector in a single character element and separate them through (‚Äú,‚Äù for us) \"\\n\" means add a new line, so it tells to print the next sentence in a new line. It is a special character, so it works with cat/li&gt; %in% A slightly different function (if we can call it this way) is %in%. When comparing two vectors, it returns TRUE or FALSE for each element of the first vector based on the presence of that element in the second vector: upregulated_1 %in% upregulated_2 [1] TRUE FALSE FALSE TRUE TRUE Sometimes it is useful to index a vector based on another vector. We will see some usages. Match Last but not least, the match() function. It takes two vectors into consideration and returns, for each element of the first vector, the position of that element in the second vector. If an element is not present, it will return NA, we will describe this element in a dedicated chapter. So, how can it be useful? Usually it is done to rearrange and reorder a vector to match another vector. For example, let‚Äôs say that two vectors of example 8.3 have names in different order; prior to do all calculation we need to match the names order. # 1. Define starting vectors proteins1 &lt;- c(&quot;SEC24C&quot;, &quot;BIP&quot;, &quot;CD4&quot;, &quot;RSPO2&quot;, &quot;LDB2&quot;) proteins2 &lt;- c(&quot;CD4&quot;, &quot;RSPO2&quot;, &quot;BIP&quot;, &quot;LDB2&quot;, &quot;SEC24C&quot;) sample1 &lt;- c(12, 52, 14, 33, 22) sample2 &lt;- c(5, 69, 26, 45, 3) names(sample1) &lt;- proteins1 names(sample2) &lt;- proteins2 sample1 SEC24C BIP CD4 RSPO2 LDB2 12 52 14 33 22 sample2 CD4 RSPO2 BIP LDB2 SEC24C 5 69 26 45 3 As we can see, the names are in different order, so we want to fix this: idx &lt;- match(names(sample1), names(sample2)) idx [1] 5 3 1 2 4 We can use these indexes to index our sample2. sample2 &lt;- sample2[idx] sample1 SEC24C BIP CD4 RSPO2 LDB2 12 52 14 33 22 sample2 SEC24C BIP CD4 RSPO2 LDB2 3 26 5 69 45 Now they are in the same order, so we can continue the analysis. Exercises Great, let‚Äôs do some exercises. They wrap up lots of concept we‚Äôve just seen. However, I encourage you to try again every function we have studied so far. It doesn‚Äôt matter if you will do them in a different way, as long as the results are identical. In these chapters I want you to understand the steps, not to use the perfect and most efficient code. Exercise 8.1 We have received the data of the expression levels (in reads) of some genes of interest. We are interested in the difference between expression levels of mitochondrial vs non-mitochondrial genes; in particular we want to see how many reads maps to those categories (both counts and percentage). The starting vector is the following: c(‚ÄúSEC24C‚Äù = 52, ‚ÄúMT-ATP8‚Äù = 14, ‚ÄúLDB2‚Äù = 22, ‚ÄúMT-CO3‚Äù = 16, ‚ÄúMT-ND4‚Äù = 2, ‚ÄúNTMT1‚Äù = 33, ‚ÄúBIP‚Äù = 20, ‚ÄúMT-ND5‚Äù = 42) PS: Mitochondrial genes starts with MT-. Solution # 1. Create the vector expr_levels &lt;- c(&quot;SEC24C&quot; = 52, &quot;MT-ATP8&quot; = 14, &quot;LDB2&quot; = 22, &quot;MT-CO3&quot; = 16, &quot;MT-ND4&quot; = 2, &quot;NTMT1&quot; = 33, &quot;BIP&quot; = 20, &quot;MT-ND5&quot; = 42) # 2. Get names of mitochondrial genes mito_genes &lt;- grep(pattern = &quot;^MT-&quot;, x = names(expr_levels), value = T) # 3. Calculate total number of counts for each category total_mito &lt;- sum(expr_levels[mito_genes]) total_no_mito &lt;- sum(expr_levels[-(names(expr_levels) %in% (mito_genes))]) # 4. Calculate % perc_mito &lt;- round(total_mito / sum(expr_levels) * 100, 2) perc_no_mito &lt;- round(total_no_mito / sum(expr_levels) * 100, 2) # 5. Print results cat(&quot;Reads mapping to mitochondrial genes are&quot;, total_mito, &quot;(&quot;, perc_mito, &quot;%), while the ones mapping to other genes are&quot;, total_no_mito, &quot;(&quot;, perc_no_mito, &quot;%)&quot;) Reads mapping to mitochondrial genes are 74 ( 36.82 %), while the ones mapping to other genes are 149 ( 74.13 %) Exercise 8.2 You were given the mass spectrometry results of an analysis on 3 patients. These are the results: patient1 c(‚ÄúSEC24C‚Äù = 12, ‚ÄúCDH7‚Äù = 1, ‚ÄúLDB2‚Äù = 13, ‚ÄúSEM3A‚Äù = 16, ‚ÄúFEZF2‚Äù = 21, ‚ÄúNTMT1‚Äù = 43, ‚ÄúBIP‚Äù = 29, ‚ÄúHOMER‚Äù = 22), patient2 c(‚ÄúSEC24C‚Äù = 2, ‚ÄúCDH7‚Äù = 11, ‚ÄúSEM5A‚Äù = 13, ‚ÄúHCN1‚Äù = 22, ‚ÄúNTMT1‚Äù = 31, ‚ÄúBIP‚Äù = 12, ‚ÄúHOMER‚Äù = 8), patient3 c(‚ÄúSEC24B‚Äù = 20, ‚ÄúBIP‚Äù = 12, ‚ÄúHOMER‚Äù = 13, ‚ÄúSEM3A‚Äù = 49, ‚ÄúHCN1‚Äù = 16, ‚ÄúNTMT1‚Äù = 27). Calculate the expression mean of common genes. Solution # 1. Create the vectors patient1 &lt;- c(&quot;SEC24C&quot; = 12, &quot;CDH7&quot; = 1, &quot;LDB2&quot; = 13, &quot;SEM3A&quot; = 16, &quot;FEZF2&quot; = 21, &quot;NTMT1&quot; = 43, &quot;BIP&quot; = 29, &quot;HOMER&quot; = 22) patient2 &lt;- c(&quot;SEC24C&quot; = 2, &quot;CDH7&quot; = 11, &quot;SEM5A&quot; = 13, &quot;HCN1&quot; = 22, &quot;NTMT1&quot; = 31, &quot;BIP&quot; = 12, &quot;HOMER&quot; = 8) patient3 &lt;- c(&quot;SEC24B&quot; = 20, &quot;BIP&quot; = 12, &quot;HOMER&quot; = 13, &quot;SEM3A&quot; = 49, &quot;HCN1&quot; = 16, &quot;NTMT1&quot; = 27) # 2. Identify common genes common1_2 &lt;- intersect(names(patient1), names(patient2)) common_all &lt;- intersect(common1_2, names(patient3)) # 3. Subset for common genes patien1_sub &lt;- patient1[common_all] patien2_sub &lt;- patient2[common_all] patien3_sub &lt;- patient3[common_all] # 4. Calculate the mean for each gene common_mean &lt;- (patien1_sub + patien2_sub + patien3_sub) / 3 common_mean NTMT1 BIP HOMER 33.66667 17.66667 14.33333 You can see how having the info for each patient in a different vector is not as handy, for this reason for expression data we use matrices. In the next chapter we will talk about them. "],["matrices.html", "9 Matrices Create a matrix Rownames and colnames Indexing Functions for all matrix Apply a function to all rows or columns Exercises", " 9 Matrices Here we are at the second basic form of data organization in R, and the most important one for expression data: the matrix. As we have seen in the previous chapter, sometimes, especially for expression data, there is the need of having a more complex data structure than the vector. Matrices are 2-dimensional data objects containing the same type of data: usually we have features (genes, proteins, ecc) on rows and samples on columns. Create a matrix There are many ways to create a matrix, we will briefly see most of them (the ones we are going to use in real life). The first one, that is not so used (I know I just told you the opposite, but‚Ä¶) is to change a vector into a matrix using the function matrix() and setting the number : # 1. create a vector my_vector &lt;- 1:12 my_vector [1] 1 2 3 4 5 6 7 8 9 10 11 12 # 2. create the matrix my_matrix &lt;- matrix(my_vector, nrow = 3) my_matrix [,1] [,2] [,3] [,4] [1,] 1 4 7 10 [2,] 2 5 8 11 [3,] 3 6 9 12 There are few things to notice here: We can set nrow or ncol If the length of the vector is not a multiple of nrow or ncol, it starts to repeat the vector from the beginning to fill the matrix (try using matrix(my_vector, nrow = 5)) It fills the matrix by column by default, we can change the behavior setting byrow = T, try it Binding Another method, usually used, is to ‚Äúbind‚Äù two or more vectors to create a matrix through rbind() or cbind(). It is easier with an example. Do you remember in exercise 8.2 that we had three patients‚Äô expression levels in three different vectors?! Now we can comine them into a matrix in this way: # 1. Create the vectors patient1 &lt;- c(&quot;SEC24C&quot; = 12, &quot;CDH7&quot; = 1, &quot;LDB2&quot; = 13, &quot;SEM3A&quot; = 16, &quot;FEZF2&quot; = 21, &quot;NTMT1&quot; = 43, &quot;BIP&quot; = 29, &quot;HOMER&quot; = 22) patient2 &lt;- c(&quot;SEC24C&quot; = 2, &quot;CDH7&quot; = 11, &quot;LDB2&quot; = 13, &quot;SEM3A&quot; = 22, &quot;FEZF2&quot; = 21, &quot;NTMT1&quot; = 31, &quot;BIP&quot; = 12, &quot;HOMER&quot; = 8) patient3 &lt;- c(&quot;SEC24C&quot; = 8, &quot;CDH7&quot; = 3, &quot;LDB2&quot; = 22, &quot;SEM3A&quot; = 14, &quot;FEZF2&quot; = 13, &quot;NTMT1&quot; = 45, &quot;BIP&quot; = 37, &quot;HOMER&quot; = 2) # 2. Bind vectors r_patients &lt;- rbind(patient1, patient2, patient3) c_patients &lt;- cbind(patient1, patient2, patient3) r_patients SEC24C CDH7 LDB2 SEM3A FEZF2 NTMT1 BIP HOMER patient1 12 1 13 16 21 43 29 22 patient2 2 11 13 22 21 31 12 8 patient3 8 3 22 14 13 45 37 2 c_patients patient1 patient2 patient3 SEC24C 12 2 8 CDH7 1 11 3 LDB2 13 13 22 SEM3A 16 22 14 FEZF2 21 21 13 NTMT1 43 31 45 BIP 29 12 37 HOMER 22 8 2 Wow, those matrices are really cool! Let‚Äôs analyze the code: First of all, we have modified the vectors to be the same size and with the same order for the genes. This is crucial, otherwise we will mess up/li&gt; What would have been messed up? The column names in the first matrix and the row names in the second matrix. We will see these two features in a while. Just keep in mind that when you have to combine different named vectors, they should be in the same order. Try it with vectors not in the same order and see rbind() combine by rows, putting each vector in a row, while cbind does the opposite Transform into a matrix One of the most used function to create a matrix is to transform a data.frame (next chapter) into a matrix using the function as.matrix(). Let‚Äôs use mtcars, a data.frame available in R, and transform it in a matrix: # 1. See class of mtcars class(mtcars) [1] &quot;data.frame&quot; # 2. Transform to a matrix mt_mat &lt;- as.matrix(mtcars) # 3. See class of new variable class(mt_mat) [1] &quot;matrix&quot; &quot;array&quot; Here we introduced the concept of class in R. This is not important for us at the moment, neither will be in the future. Just think this way: typeof() is for the type of data inside a variable while the class is the ‚Äútype‚Äù of the structure of the variable. Ok, now that we‚Äôve seen how to create a matrix, we have to dig into some important concepts. Rownames and colnames Matrices can have row names and column names (think as the extension of a named vector). We have already seen these features in [#Binding], and here we will see them in details. Let‚Äôs start by visualizing the first matrix we have created: my_matrix [,1] [,2] [,3] [,4] [1,] 1 4 7 10 [2,] 2 5 8 11 [3,] 3 6 9 12 We can extract row names and column names with rownames() and colnames() functions, respectively: rownames(my_matrix) NULL colnames(my_matrix) NULL They have not been set, as we can see by the fact that the output of these functions is NULL and that when we printed the matrix we had ‚Äú[,1] [,2] [,3] [,4]‚Äù. Set and edit row and column names You have to think at row and column names as simple vectors, so we can create two vectors (on for rows and one for columns) and assign them to row and column names. Remember: they must be the same length as the rows and the columns. # 1. Create vectors my_row_names &lt;- c(&quot;First row&quot;, &quot;Second row&quot;, &quot;Third row&quot;) my_col_names &lt;- c(&quot;First column&quot;, &quot;Second column&quot;, &quot;Third column&quot;, &quot;Fourth column&quot;) # 2. Assign vectors to row and column names rownames(my_matrix) &lt;- my_row_names colnames(my_matrix) &lt;- my_col_names # 3. Print result my_matrix First column Second column Third column Fourth column First row 1 4 7 10 Second row 2 5 8 11 Third row 3 6 9 12 rownames(my_matrix) [1] &quot;First row&quot; &quot;Second row&quot; &quot;Third row&quot; colnames(my_matrix) [1] &quot;First column&quot; &quot;Second column&quot; &quot;Third column&quot; &quot;Fourth column&quot; Tadaaa. Now we have a more complete matrix. As row names and column names are vectors, you can easily change them all (or part of them). For example: # 1. Change third row names rownames(my_matrix)[3] &lt;- &quot;3rd row&quot; # 2. Print my_matrix First column Second column Third column Fourth column First row 1 4 7 10 Second row 2 5 8 11 3rd row 3 6 9 12 Indexing As for vectors, we can extract values of the matrix by indexing: we can get a single value, a vector of values and either smaller matrices. Slicing The first method is similar to vector slicing, the only exception here is that we have to set a value for the row/s and one for the column/s in the form [row/s, column/s]. Here some examples: # Get only a value, from second row and third column my_matrix[2, 3] [1] 8 # Get a vector of values, from first and third row and second column my_matrix[c(1, 3), 2] First row 3rd row 4 6 # Get a matrix, from second and third row and second and third column my_matrix[2:3, 2:3] Second column Third column Second row 5 8 3rd row 6 9 # Get ALL elements of a row (leave the column part empty) my_matrix[2:3,] First column Second column Third column Fourth column Second row 2 5 8 11 3rd row 3 6 9 12 You can use every slicing technique seen in slicing section of vectors. Using names If we have set rownames and/or column names, we can use them to index a matrix (similarly to named vectors), in the form [row/s, column/s]. # Get only a value, from second row and third column my_matrix[&quot;Second row&quot;, &quot;Third column&quot;] [1] 8 # Get a vector of values, from first and third row and second column my_matrix[c(&quot;First row&quot;, &quot;3rd row&quot;), &quot;Second column&quot;] First row 3rd row 4 6 # Get a matrix, from second and third row and second and third column my_matrix[c(&quot;Second row&quot;, &quot;3rd row&quot;), c(&quot;Second column&quot;, &quot;Third column&quot;)] Second column Third column Second row 5 8 3rd row 6 9 # Get ALL elements of a row (leave the column part empty) my_matrix[c(&quot;Second row&quot;, &quot;3rd row&quot;),] First column Second column Third column Fourth column Second row 2 5 8 11 3rd row 3 6 9 12 Here, you can‚Äôt use expansions (:) to select from row with name X to row with name X+3, it is only possible with numbers. In fact, if you remember, the command n:n+m create a vector of numbers from n to n + m. Try it! Using logicals Again, as for vectors we can use logicals to extrapolate values of a matrix. Let‚Äôs say we want to get the names of the rows in which the third column has a odd number, we can do as follow: # 1. Create a boolean vector is_odd &lt;- my_matrix[, &quot;Third column&quot;] %% 2 == 1 # 2. Slice the matrix based on the boolean vector sliced_odd &lt;- my_matrix[is_odd,] # 3. Get row names rownames(sliced_odd) [1] &quot;First row&quot; &quot;3rd row&quot; This will be so useful when talking about expression matrix (e.g.¬†when we want to extrapolate genes that have tot expression in patient X or Y). Functions for all matrix Alright, it is time for some practical applications to explore fundamental matrix functions. Head/Tail The first functions are head() and tail(). Let‚Äôs say we receive an expression table but they didn‚Äôt tell us much about it; we can use these two functions to look up the first and the last rows respectively. Here is how to use them: # 1. Load data (first time we see it) expr_data &lt;- read.csv(file = &quot;data/All_counts.csv&quot;, # name of the file header = T # usually column names are in the first row ) # 2. Type of class(expr_data) [1] &quot;data.frame&quot; # 3. Head head(expr_data) X wt1 wt2 wt3 wt4 wt5 wt6 ko1 ko2 ko3 ko4 ko5 ko6 1 DDX11L1 4 1 2 1 1 1 0 6 3 1 0 2 2 WASH7P 421 46 342 345 192 18 233 217 125 313 321 726 3 MIR6859-3 0 0 0 0 0 0 0 0 0 0 0 0 4 MIR6859-2 0 0 0 0 0 0 0 0 0 0 0 0 5 MIR6859-1 0 0 0 0 0 0 0 0 0 0 0 0 6 MIR6859-4 0 0 0 0 0 0 0 0 0 0 0 0 Ok, this is a brief example in which we have loaded a data matrix with the function read.csv() (as we know that the file is a .csv, but there are plenty of functions for all different type of files). We then checked the class of the expr_data and we get data.frame; lastly we used head() to see the structure of the first rows and we can say that: Gene symbols are in a column We have 6 wt samples and 6 ko samples Probably these are row counts as we do not have decimal values We can edit the previous code to fix some issues we encounter # 1. Gene names as rownames expr_data &lt;- read.csv(file = &quot;data/All_counts.csv&quot;, header = T, row.names = 1 # first column ) # 2. Transform to matrix expr_data &lt;- as.matrix(expr_data) # Check class(expr_data) [1] &quot;matrix&quot; &quot;array&quot; head(expr_data, n = 10) # we can specify the number of rows we want wt1 wt2 wt3 wt4 wt5 wt6 ko1 ko2 ko3 ko4 ko5 ko6 DDX11L1 4 1 2 1 1 1 0 6 3 1 0 2 WASH7P 421 46 342 345 192 18 233 217 125 313 321 726 MIR6859-3 0 0 0 0 0 0 0 0 0 0 0 0 MIR6859-2 0 0 0 0 0 0 0 0 0 0 0 0 MIR6859-1 0 0 0 0 0 0 0 0 0 0 0 0 MIR6859-4 0 0 0 0 0 0 0 0 0 0 0 0 MIR1302-2 0 0 0 0 0 0 0 0 0 0 0 0 MIR1302-11 0 0 0 0 0 0 0 0 0 0 0 0 MIR1302-9 0 0 0 0 0 0 0 0 0 0 0 0 MIR1302-10 0 0 0 0 0 0 0 0 0 0 0 0 Ok, fine. We are ready to go and explore our first expression matrix. Dim, nrow, ncol Head and tail are great friends, but you always want to know the dimensions of the matrix. It can be done using dim() function, which returns the number of rows and columns (this order!), or nrow() and ncol(). # dim dim(expr_data) [1] 26475 12 # nrow nrow(expr_data) [1] 26475 # ncol ncol(expr_data) [1] 12 We see that our matrix has 26475 rows and 12 columns. Summary and transpose After having seen the basic structure of the first and last lines, the number of rows and columns, we usually want to have some info about the distribution of the genes, the counts ecc. Here comes the function summary(): summary(expr_data) wt1 wt2 wt3 wt4 wt5 wt6 ko1 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 Min. : 0.0 Min. : 0.0 Min. : 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 Median : 19.0 Median : 16.0 Median : 16.0 Median : 15 Median : 16.0 Median : 17.0 Median : 13.0 Mean : 411.4 Mean : 324.4 Mean : 326.7 Mean : 274 Mean : 317.3 Mean : 316.6 Mean : 250.1 3rd Qu.: 272.0 3rd Qu.: 214.0 3rd Qu.: 218.0 3rd Qu.: 186 3rd Qu.: 212.0 3rd Qu.: 224.0 3rd Qu.: 179.0 Max. :327719.0 Max. :536733.0 Max. :377069.0 Max. :90474 Max. :352034.0 Max. :209954.0 Max. :61650.0 ko2 ko3 ko4 ko5 ko6 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 Median : 18.0 Median : 19.0 Median : 17.0 Median : 17.0 Median : 19.0 Mean : 378.2 Mean : 351.4 Mean : 373.8 Mean : 319.8 Mean : 423.4 3rd Qu.: 245.0 3rd Qu.: 246.0 3rd Qu.: 255.0 3rd Qu.: 233.0 3rd Qu.: 288.0 Max. :641616.0 Max. :497463.0 Max. :236803.0 Max. :62660.0 Max. :338679.0 For each column it returns some descriptive. But, ehi, on the columns we have the samples, we want this statistics for the genes, how can we do it? Here comes t() that transpose the matrix so that rows became columns and vice versa. expr_data_t &lt;- t(expr_data) expr_data_t[1:5, 1:8] DDX11L1 WASH7P MIR6859-3 MIR6859-2 MIR6859-1 MIR6859-4 MIR1302-2 MIR1302-11 wt1 4 421 0 0 0 0 0 0 wt2 1 46 0 0 0 0 0 0 wt3 2 342 0 0 0 0 0 0 wt4 1 345 0 0 0 0 0 0 wt5 1 192 0 0 0 0 0 0 Perfect, now we can do summary again (we do a subset of the data because we don‚Äôt want to do it for all genes, as it will be huge). set.seed(123) # 1. Get random numbers to select random genes r_numb &lt;- runif(n = 7, min = 1, max = ncol(expr_data_t)) # 2. Floor numbers to get integers r_numb &lt;- floor(r_numb) # 3. Get the summary of sliced expr_data_t summary(expr_data_t[, r_numb]) GPX2 BTN3A3 KRT17 USP17L2 PHF19 DBT PPM1B Min. :0.0000 Min. : 5.00 Min. :0.00 Min. :0.0 Min. : 10.0 Min. : 23.0 Min. : 73.0 1st Qu.:0.0000 1st Qu.: 15.00 1st Qu.:0.00 1st Qu.:0.0 1st Qu.: 20.0 1st Qu.: 415.2 1st Qu.: 392.5 Median :0.5000 Median : 24.00 Median :0.00 Median :0.0 Median : 97.0 Median : 753.0 Median : 562.0 Mean :0.5833 Mean : 35.67 Mean :0.25 Mean :0.5 Mean :112.2 Mean : 642.8 Mean : 632.7 3rd Qu.:1.0000 3rd Qu.: 32.50 3rd Qu.:0.00 3rd Qu.:1.0 3rd Qu.:168.5 3rd Qu.: 938.2 3rd Qu.: 838.0 Max. :2.0000 Max. :150.00 Max. :2.00 Max. :3.0 Max. :324.0 Max. :1107.0 Max. :1466.0 Here we have some interesting descriptive for each gene: we can see, for example, that DBT has more counts than GPX2, or that PPM1B has the higher Max. counts of all these 7 genes. There are two new functions here: set.seed and runif. Let‚Äôs start from the latter, it returns n random number uniformly distributed between min and max included. Do you understand why I used max = ncol(expr_data_t)? I‚Äôm sure you do. But to be sure that it is clear, I did it because I don‚Äôt want to hard code (insert the real number) of the number of columns of the expression matrix, because if I change matrix I have to change this number as well. So, I let R calculate it for me. As it is random, in order to let you have the same number as me, I set a seed. The theory behind ‚Äúrandomness‚Äù in computer science is complex, so I‚Äôm not going to deep it. If you want, there are plenty of resources on the internet on this topic. Here, just an example of the use of the seed. # 1. Two random vectors of 5 numbers without set.seed no_seed1 &lt;- runif(n = 5, min = 1, max = 20) no_seed2 &lt;- runif(n = 5, min = 1, max = 20) # 2. Two random vectors setting the seed set.seed(123) # here you can place any number, each generate a unique random series with_seed1 &lt;- runif(n = 5, min = 1, max = 20) set.seed(123) # here you can place any number, each generate a unique random series with_seed2 &lt;- runif(n = 5, min = 1, max = 20) print(no_seed1) [1] 17.955962 11.477265 9.675680 19.179834 9.613349 print(no_seed2) [1] 13.873842 11.880035 2.955569 18.096674 5.675667 print(with_seed1) [1] 6.463973 15.977798 8.770562 17.777331 18.868878 print(with_seed2) [1] 6.463973 15.977798 8.770562 17.777331 18.868878 Great! Remember, setting the seed is fundamental in analysis in which randomness take place, so they will always return the same results. Sum, Max, Min, ‚Ä¶ Alright, there are plenty of functions that returns useful values of all the data in a matrix, such as the max value, the min one, the sum of all the values, the mean of all the values etc. There is nothing easier than watch them in action. # 1. create a random value matrix set.seed(214) my_matrix2 &lt;- matrix(runif(n = 24, min = 1, max = 20), nrow = 4) my_matrix2 [,1] [,2] [,3] [,4] [,5] [,6] [1,] 7.079653 17.000935 12.786656 15.687742 1.260719 1.082183 [2,] 18.702227 14.015268 9.345658 4.225409 9.341111 11.385358 [3,] 10.809797 19.592980 13.632435 13.774924 19.548749 5.461854 [4,] 8.975963 7.808112 15.024569 6.271463 6.136042 16.151705 # 2. Calculate sum my_sum &lt;- sum(my_matrix2) print(paste(&quot;Sum of the values of my_matrix2:&quot;, my_sum)) [1] &quot;Sum of the values of my_matrix2: 265.101512494963&quot; # 3. Calculate min my_min &lt;- min(my_matrix2) print(paste(&quot;Min of the values of my_matrix2:&quot;, my_min)) [1] &quot;Min of the values of my_matrix2: 1.08218298689462&quot; # 4. Calculate max my_max &lt;- max(my_matrix2) print(paste(&quot;Max of the values of my_matrix2:&quot;, my_max)) [1] &quot;Max of the values of my_matrix2: 19.5929795864504&quot; # 5. Calculate mean my_mean &lt;- mean(my_matrix2) print(paste(&quot;Mean of the values of my_matrix2:&quot;, my_mean)) [1] &quot;Mean of the values of my_matrix2: 11.0458963539568&quot; Here, we calculate these stats for all the data in the matrix, but if I want to calculate it by rows or columns? Here it is Apply a function to all rows or columns You can easily think about summary function to answer the previous question. Right, I‚Äôll give it to you, but how if you want to handle the result? How about having only the vector of the mean values for each row? This is achieved through apply. Let‚Äôs see it in action, then I‚Äôll explain it. # 1. Calculate the mean for each column mean_col &lt;- apply(X = expr_data_t[, r_numb], MARGIN = 1, FUN = mean) # 2. Calculate the mean for each column mean_row &lt;- apply(X = expr_data_t[, r_numb], MARGIN = 2, FUN = mean) mean_col wt1 wt2 wt3 wt4 wt5 wt6 ko1 ko2 ko3 ko4 ko5 ko6 207.1429 178.2857 149.7143 101.8571 202.4286 209.0000 208.5714 231.8571 141.4286 225.2857 243.2857 343.4286 mean_row GPX2 BTN3A3 KRT17 USP17L2 PHF19 DBT PPM1B 0.5833333 35.6666667 0.2500000 0.5000000 112.1666667 642.8333333 632.6666667 See? It‚Äôs super easy. It requires the matrix (X), the function (mean), and the margin used to calculate over (1 for rows, 2 for columns). You can apply all the functions that take as input numeric vectors. If the matrix has row names or column names set, it returns a named vactor, otherwise it returns a normal vector. Exercises Now it‚Äôs you turn to familiarize with matrices. Exercise 9.1 You have collected expression levels of 4 genes of interest of 3 patients. Calculate the sum of the read for each patient, in order to normalize the values in further analysis. I‚Äôll give you the data. CXCR4 &lt;- c(&quot;P1&quot; = 25, &quot;P2&quot; = 12, &quot;P3&quot; = 14) LCT &lt;- c(&quot;P3&quot; = 12, &quot;P2&quot; = 10, &quot;P1&quot; = 17) PTPN7 &lt;- c(&quot;P1&quot; = 2, &quot;P3&quot; = 3, &quot;P2&quot; = 4) LHX9 &lt;- c(&quot;P2&quot; = 20, &quot;P1&quot; = 28, &quot;P3&quot; = 11) Solution # 1. Reorder the vectors to have the same order of patients LCT &lt;- LCT[names(CXCR4)] PTPN7 &lt;- PTPN7[names(CXCR4)] LHX9 &lt;- LHX9[names(CXCR4)] # 2. Create the matrix expr_mat_pat &lt;- rbind(CXCR4, LCT, PTPN7, LHX9) # 3. Calculate the sum of the reads for each patient read_sum_pat &lt;- apply(expr_mat_pat, MARGIN = 2, mean) print(read_sum_pat) P1 P2 P3 18.0 11.5 10.0 Exercise 9.2 We know that genes which mean is &lt; 5, they should be excluded from the analysis (it is an example). Re-do the calculation of exercise 9.1 excluding those genes. Solution # 1. Calculate the mean of the reads for each gene read_sum_gene &lt;- apply(expr_mat_pat, MARGIN = 1, mean) # 2. Get the name of the genes to keep gene_2_keep &lt;- names(read_sum_gene)[read_sum_gene &gt; 5] # 3. Use genes_2_keep to subset matrix and calculate sum read_sum_pat_filt &lt;- apply(expr_mat_pat[gene_2_keep, ], MARGIN = 2, mean) print(read_sum_pat_filt) P1 P2 P3 23.33333 14.00000 12.33333 Great, another big chapter has come to an end. We are closer and closer to your real world applications. For your expression (mRNA or proteins) data you could already start some analysis, but wait for the next chapter, in which we will talk about data.frame. "],["data-frames.html", "10 Data frames Create a data.frame Indexing Create a new column Delete a column Export a data frame", " 10 Data frames Matrices can only be filled with the exact same type of data in all columns. This is useful when dealing with expression data that have to be manipulated for statistics, but usually we do not have only numbers in a table. For example, we could have all the meta data associated with different samples: age, treatment, sex, number of replicates, number of cells, etc. These are all different types of data, so we need a different ‚Äústructure‚Äù that can store them for us. In R, this structure is called data.frame. Create a data.frame There are many ways to create a data.frame in R. We will explore them all, as you will use them in different situations. From matrix Let‚Äôs start from the least commonly utilized one (I know, you want to ask me ‚ÄúSo why are you annoying me with this stuff?!‚Äù). We can transform a matrix into a data.frame using the function as.data.frame(). Usually, you do so when you want to add a new column to the matrix with a data type that is different from the one of the matrix (e.g.¬†add a character column to a numeric matrix). Let‚Äôs see a quick example: # 1. Create a matrix my_matrix &lt;- matrix(rnorm(20), nrow = 4, ncol = 5) # 2. Check the class of my_matrix variable print(paste(&quot;Variable my_matrix is of class:&quot;, paste(class(my_matrix), collapse = &quot;, &quot;))) [1] &quot;Variable my_matrix is of class: matrix, array&quot; # 3. Transform the matrix into a data.frame my_df &lt;- as.data.frame(my_matrix) # 4. Check the class of my_df variable print(paste(&quot;Variable my_df is of class:&quot;, paste(class(my_df), collapse = &quot;, &quot;))) [1] &quot;Variable my_df is of class: data.frame&quot; And now? How do we add a new column? Eheh, keep calm, we will answer this question in a moment‚Ä¶ From vectors We have to first look at another method to create a data.frame, that is starting from different vectors (as for a matrix). The syntax is this: data.frame(\"&lt;name_of_the_column&gt;\" = &lt;vector&gt;, \"&lt;name_of_the_column2\" = &lt;vector2&gt;, ...). This is useful when you have different information in different vectors and you want to merge them all, for example: # 1. Set the seed for sample function set.seed(362) # 2. Create vectors age &lt;- c(33, 29, 32, 41, 45, 67, 44, 18, 22, 21, 37, 36, 39, 45, 19, 20, 28, 30, 48, 50, 66, 26, 55, 56) sex &lt;- sample(x = c(&quot;M&quot;, &quot;F&quot;), size = length(age), replace = T) treatment &lt;- rep(x = c(&quot;CTRL&quot;, &quot;Inh1&quot;, &quot;Inh2&quot;), each = length(age)/3) weight &lt;- round(c(rnorm(n = length(age)/3, mean = 85, sd = 20), rnorm(n = length(age)/3, mean = 75, sd = 5), rnorm(n = length(age)/3, mean = 65, sd = 12)), 1) # 3. Create the data.frame my_df &lt;- data.frame(&quot;age&quot; = age, &quot;sex&quot; = sex, treatment = treatment, weight = weight) # 4. Visualize first rows head(my_df) age sex treatment weight 1 33 F CTRL 108.9 2 29 M CTRL 98.1 3 32 F CTRL 76.1 4 41 M CTRL 92.9 5 45 M CTRL 69.0 6 67 F CTRL 93.4 From file You will usually load a table from a file (csv, txt, tsv, etc) that you have written or that someone has given you. We have already seen how to read a csv file in the previous chapter, so let‚Äôs do the same to load our weight data (Download). # 1. Load data weight_data &lt;- read.csv(&quot;data/Weights.csv&quot;, header = T) # 2. Visualize first rows head(weight_data) age sex treatment weight 1 33 F CTRL 108.9 2 29 M CTRL 98.1 3 32 F CTRL 76.1 4 41 M CTRL 92.9 5 45 M CTRL 69.0 6 67 F CTRL 93.4 By default, the table is loaded as a data.frame. Indexing Indexing a data.frame is exactly the same as indexing a matrix. For this reason, I will only do some examples here, and invite you to look at the indexing section in matrices chapter for a deeper explanation. # 1. Extract males data males_data &lt;- weight_data[weight_data$sex == &quot;M&quot;,] head(males_data) age sex treatment weight 2 29 M CTRL 98.1 4 41 M CTRL 92.9 5 45 M CTRL 69.0 7 44 M CTRL 90.2 8 18 M CTRL 65.6 9 22 M Inh1 68.0 # 2. Extract controls sex and age ctrl_sex_age &lt;- weight_data[weight_data$treatment == &quot;CTRL&quot;, c(&quot;sex&quot;, &quot;age&quot;)] head(ctrl_sex_age) sex age 1 F 33 2 M 29 3 F 32 4 M 41 5 M 45 6 F 67 Create a new column I‚Äôm proud of you and your patience! Here we are to answer previous questions about how to add a new column to a data.frame. There are different ways to add a column to an existing database. We can use the ‚Äú$‚Äù operator in this way: dataframe\\$new_column &lt;- vector. In this way we are telling R to insert vector as new_column in the data.frame. Similarly, we can use dataframe[\"&lt;new_column&gt;\"] &lt;- vector to do the same. Here an example: # 1. Add state column with value Italy for all rows weight_data$state &lt;- &quot;Italy&quot; # 2. Add city column with value Milan for all rows weight_data[&quot;city&quot;] &lt;- &quot;Milan&quot; # 3. Visualize first rows head(weight_data) age sex treatment weight state city 1 33 F CTRL 108.9 Italy Milan 2 29 M CTRL 98.1 Italy Milan 3 32 F CTRL 76.1 Italy Milan 4 41 M CTRL 92.9 Italy Milan 5 45 M CTRL 69.0 Italy Milan 6 67 F CTRL 93.4 Italy Milan Delete a column To delete a column from a data.frame, you should assign it the value NULL dataframe$column &lt;- NULL. If you want to delete multiple columns, you could use this form dataframe[, c(\"column1\", \"column2\", ...)] &lt;- NULL. Let‚Äôs now delete state and city columns from our dataframe: # 1. Delete rows weight_data[, c(&quot;state&quot;, &quot;city&quot;)] &lt;- NULL # 2. Visualize first rows head(weight_data) age sex treatment weight 1 33 F CTRL 108.9 2 29 M CTRL 98.1 3 32 F CTRL 76.1 4 41 M CTRL 92.9 5 45 M CTRL 69.0 6 67 F CTRL 93.4 Export a data frame Lastly, let‚Äôs imagine you have performed some operations on a data.frame (filtering, add columns, calculations etc) and you want to save it as a file, what you have to do is this: write.csv(x = weight_data, file = &quot;output/weight_modified.csv&quot;, quote = F, row.names = F) Let‚Äôs dissect this code: write.csv is the function used to save a csv file. You can also save a tsv file with write.tsv or write.table etc. There are plenty of them. I like csv so I‚Äôm using it. x is the data.frame to save file is the path were to save the dataframe quote is to tell whether to encapsulate each value into double quotes when saving (we don‚Äôt want it so we set it to F) row.names is to tell whether to write a column of the row names when saving. It is useful when the row names store useful information, such as patient ID, gene name etc. As in our case there is no information in row names, we set it to F. Great! We have now seen the basic operation to manipulate a data.frame object in R. Next step will be to do a real exploratory data analysis to discover new functions and start our real-world journey. "],["functions-packages-and-reproducibility.html", "11 Functions, packages and reproducibility Functions Packages Reproducibility", " 11 Functions, packages and reproducibility I‚Äôm sorry, I know last time I told you we would have start exploratory data analysis‚Ä¶ but there are some crucial concepts that you should know prior to start your analysis: functions, packages and reproducibility. Functions We have encountered them quite a few times in previous chapters. We know that in R functions can be used by writing the name of the function, followed by parenthesis with some values in it. I‚Äôm not here to describe how to write new functions etc (if you want to know more about it, this is a useful guide), but you have to know that the functions we‚Äôve seen (e.g.¬†mean(), max(), median(), length()) are so-called ‚Äúbuilt-in‚Äù, so they are accessible in R since its installation. However, there are lots and lots of functions created by different ‚Äúusers‚Äù around the world that are fundamental for different types of analysis but that are not ‚Äúpre-built‚Äù in R. They are collected in different packages. Packages So, you have to imagine a package as a box containing different functions that you want to use. ‚ÄúRight, and how can I use them?‚Äù Great question, let‚Äôs see how to install a package. Install a package To install a package you have to know in which repository it is stored (imagine a repository as a free App Store/Play Store). Most of the packages are store on CRAN (website), while the majority of genomic-related packages are store in Bioconductor (website). As we are going to use CRAN packages, here I show you how to install those packages. Let‚Äôs say we want to install tidyverse (which is a collection of packages that I love for data analysis), we will use the command install.packages(\"&lt;name_of_the_package&gt;\"). install.packages(&quot;tidyverse&quot;) I strongly suggest you to run this code and follow the instructions that pop up in the console. The process will take a while and if all worked fine, it should prompt ‚ÄúDONE The downloaded source packages are in‚Ä¶.‚Äù at the end. Load a package Installing a package is not sufficient to be able to use its functions, you have to load it in every session you want to use it. So, I now it sounds like a big deal, but it‚Äôs easier than it seems: at the beginning of your script you have to write library(&lt;name_of_the_package&gt;) for each package you want to load. For example, in our case we will write: library(tidyverse) ‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ ‚úî dplyr 1.1.2 ‚úî readr 2.1.4 ‚úî forcats 1.0.0 ‚úî stringr 1.5.0 ‚úî ggplot2 3.4.2 ‚úî tibble 3.2.1 ‚úî lubridate 1.9.2 ‚úî tidyr 1.3.0 ‚úî purrr 1.0.1 ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ‚úñ dplyr::filter() masks stats::filter() ‚úñ dplyr::lag() masks stats::lag() ‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors Now that we have loaded the package, we are able to use its functions. Reproducibility Here we are at the most important part of this chapter: data reproducibility. I mentioned you just few things about functions and packages, I know‚Ä¶ but those concepts are important to understand how data reproducibility works. You know that reproducibility is a key aspect of every experiment and analysis. When you are analyzing data with R there are few things that are mandatory for reproducibility: Write every step and code you run Use set.seed for randomization steps Use the same version of R and of the packages As we have already seen the first 2 points, we will now discuss about the third one. When you are installing a package, you are installing a certain version if it. In fact, during time, packages changes with new functionalities, fixed bugs and so on. For this reason, the results of an analysis done with version 1.0 of a package may be different from the ones using version 5.2‚Ä¶ it should not be the case, but some times it happens because the same functions may change a bit. So, how to control it? Control package versions To check which version of a package you have installed you can use the command packageVersion(\"&lt;name_of_the_package&gt;\"). For example: packageVersion(&quot;tidyverse&quot;) [1] &#39;2.0.0&#39; I have installed version 2.0.0 of tidyverse. DON‚ÄôT worry if your version is not the same as mine (I know it sounds controversial, but here we are explaining things, you should stick with you own version). Session info Another way is to look at the sessionInfo, which returns all the package loaded in the current session (remember, a session starts when you start R and ends when you exit or restart R). sessionInfo() R version 4.1.0 (2021-05-18) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS 13.7.5 Matrix products: default LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 dplyr_1.1.2 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 [9] ggplot2_3.4.2 tidyverse_2.0.0 loaded via a namespace (and not attached): [1] bslib_0.3.1 compiler_4.1.0 pillar_1.9.0 jquerylib_0.1.4 tools_4.1.0 digest_0.6.28 timechange_0.2.0 [8] jsonlite_1.8.4 evaluate_0.14 lifecycle_1.0.3 gtable_0.3.0 pkgconfig_2.0.3 rlang_1.1.1 cli_3.6.1 [15] rstudioapi_0.15.0 yaml_2.2.1 xfun_0.28 fastmap_1.1.0 withr_2.5.0 knitr_1.36 hms_1.1.3 [22] generics_0.1.1 vctrs_0.6.2 sass_0.4.0 tidyselect_1.2.0 grid_4.1.0 glue_1.6.2 R6_2.5.1 [29] fansi_0.5.0 rmarkdown_2.11 bookdown_0.24 tzdb_0.2.0 magrittr_2.0.3 scales_1.2.1 htmltools_0.5.2 [36] colorspace_2.0-2 utf8_1.2.2 stringi_1.7.5 munsell_0.5.0 Here it is reported the version of R, and of all the packages loaded during this session. You should use this command at the end of all your analyses, especially if you are using Markdown (we will see them soon, very soon) and provide it when you want to share the analysis to someone else (or publish a paper with an analysis performed in R). In fact, in ‚ÄúMaterial and Methods‚Äù section of a paper, you should write down the version of R and of the packages used for analyses. IMPORTANT: for all these reasons, you should NOT upgrade R or any packages you are using for an analysis, even if it asks you for any upgrade during packages installation. There are better ways to control package versions etc, but that is out of the scope of this book (if you are interested, go and learn about conda environments here). So, with these concepts in mind, let‚Äôs start our first data analysis in R in the next chapter. link versione pacchetto Importanza riproducibilit√† Versione pacchetti Conda environment (per approfondire), altrimenti non aggiornare mai. "],["tidyverse-manage.html", "12 Manage data with tidyverse R notebook Load packages and data Dataset structure Summary Dealing with missing values Filter a dataframe Sort for the value of a column Merge two columns Split two columns Merge two dataframes Exercises", " 12 Manage data with tidyverse Let‚Äôs start with our first data analysis (I know you were waiting for it!): in this chapter, we will use the magic world of tidyverse and rnotebook to explore a dataset. R notebook We have already seen how to store R codes into scripts. However, there are some disadvantages of using them: you cannot see the output of a code directly under it, you cannot see the graphs directly under the code, you cannot create multiline comments without having to comment each line etc. Here comes notebooks! Whithin RStudio, just click ‚ÄúFile‚Äù &gt; ‚ÄúNew File‚Äù &gt; ‚ÄúR notebook‚Äù to open a new notebook. As this book is not focused on it, and there is already a wonderful book about it, I invite you to read the first part of it to get used to this wonderful tool. The key feature you need to understand are: code is executed only in code chunks, and all what you type outside is considered as ‚Äúcomment‚Äù after every executed code chunks you can see the output of the chunk itself (super useful for data visualization and dataframe inspection) you can export the notebook as html/pdf and share it with your colleagues (I prefer html for many reasons) Dont‚Äô worry! At the end of each chapter you can download the notebook used for the analysis, so you can explore it, see how it works, and run it. Don‚Äôt be lazy, first write your own notebook, in which you can add whatever comment you want, and then download mine to compare it. Load packages and data At the beginning of every analysis, it is strongly suggested to load any external package you want to use, in our case tidyverse. Why using tidyverse? Because it is super useful, easy to learn, intuitive and it is the main package used for data analysis with R. We have seen in the last chapter how to install and load it in our session, so let‚Äôs do it: library(tidyverse) Next, we will load the dataset to analyze. It is taken from this paper, in particular it is Supplementary Table 13 (Download). df &lt;- read.csv(&quot;data/Supplementary_table_13.csv&quot;) head(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area dataset 1 1005_TCX AD 6 F 90 8 8.6 NA TL MAYO 2 1019_TCX AD 6 F 86 4 7.8 NA TL MAYO 3 1029_TCX AD 6 F 69 4 9.7 NA TL MAYO 4 1034_TCX AD 6 F 88 5 8.9 NA TL MAYO 5 1045_TCX AD 5 F 90 NA 8.4 NA TL MAYO 6 1046_TCX AD 6 F 72 2 9.0 NA TL MAYO Dataset structure The very first thing to do is to explore the dataset, in particular the number of rows/columns and the type of columns. We can do it through str(), which returns the dimensions of the dataframe and some info about each column. Here it is: str(df) &#39;data.frame&#39;: 1493 obs. of 10 variables: $ SampleID : chr &quot;1005_TCX&quot; &quot;1019_TCX&quot; &quot;1029_TCX&quot; &quot;1034_TCX&quot; ... $ Diagnosis: chr &quot;AD&quot; &quot;AD&quot; &quot;AD&quot; &quot;AD&quot; ... $ Braak : num 6 6 6 6 5 6 5.5 6 6 5 ... $ sex : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; ... $ AOD : num 90 86 69 88 90 72 85 82 77 90 ... $ PMI : num 8 4 4 5 NA 2 NA 15 NA NA ... $ RIN : num 8.6 7.8 9.7 8.9 8.4 9 8 10 8.6 7.9 ... $ CDR : num NA NA NA NA NA NA NA NA NA NA ... $ Area : chr &quot;TL&quot; &quot;TL&quot; &quot;TL&quot; &quot;TL&quot; ... $ dataset : chr &quot;MAYO&quot; &quot;MAYO&quot; &quot;MAYO&quot; &quot;MAYO&quot; ... You usually know what data you have in your dataset, which variables etc. As this is a downloaded table and it is my first time looking at it, I have to admit that we have to discover few things ahahah What can we say about this dataframe? It has 1493 observation and 10 variables Variable SampleID should be the unique identifier of the sample (we should check it) Diagnosis, sex, Area and dataset column can be categorical data, so we should convert them to factors We have NAs in some variables. I see your face asking yourself ‚ÄúWhat the hell are NAs?‚Äù. Don‚Äôt worry, we‚Äôll explain it in a bit Let‚Äôs now check if SampleID are unique identifiers; to do it, we evaluate whether the number of unique values in that column is equal to the number of total values of that column: length(unique(df$SampleID)) == length(df$SampleID) [1] TRUE Yes, they are unique identifiers. Change column type Next, we transform some chr columns to factor, this has 2 main advantages: first of all, factors are treated differently in some functions (expecially plotting) and they occupy less memory in you PC. ‚ÄúFactors?! Why are you telling me about this data type now?‚Äù Yes, this is another data type, but don‚Äôt worry, you just have to know the two advantages aforementioned and how to create it (we will see it now). An you know what?! As I know you‚Äôre doing great, let‚Äôs introduce our first function of the magic tidyverse world: mutate: df &lt;- df %&gt;% mutate(&quot;Diagnosis&quot; = factor(Diagnosis), &quot;sex&quot; = factor(sex), &quot;Area&quot; = factor(Area), &quot;dataset&quot; = factor(dataset)) str(df) &#39;data.frame&#39;: 1493 obs. of 10 variables: $ SampleID : chr &quot;1005_TCX&quot; &quot;1019_TCX&quot; &quot;1029_TCX&quot; &quot;1034_TCX&quot; ... $ Diagnosis: Factor w/ 2 levels &quot;AD&quot;,&quot;Control&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ Braak : num 6 6 6 6 5 6 5.5 6 6 5 ... $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ AOD : num 90 86 69 88 90 72 85 82 77 90 ... $ PMI : num 8 4 4 5 NA 2 NA 15 NA NA ... $ RIN : num 8.6 7.8 9.7 8.9 8.4 9 8 10 8.6 7.9 ... $ CDR : num NA NA NA NA NA NA NA NA NA NA ... $ Area : Factor w/ 6 levels &quot;BM10&quot;,&quot;BM22&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... $ dataset : Factor w/ 3 levels &quot;MAYO&quot;,&quot;MSBB&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Great! It worked! Let‚Äôs dissect this code: df &lt;-: we reassign the result to the same variable (overwriting it) df %&gt;%: this is a feature of purr, a package inside tidyverse, that takes what is at the left of %&gt;% and pass it as the first input of the function after the symbol. We could have written this code in just one line, but in this way is more readable mutata(: we use the function mutate, with df as first input (implicit thanks to %&gt;%). This function is used to create new column/s in the dataframe; if the column already exists (as in our case), it is overwritten \"Diagnosis\" = factor(Diagnosis) : I take this as example, as this function has this structure \"new_column\" = something. In this case, we create (overwrite) column Diagnosis with factor(Diagnosis). To note: in tidyverse functions, the column of the dataframe can be accessed by calling only the name of the column, and not df$name_of_the_column. factor(Diagnosis): it takes as input the column Diagnosis and transform it from chr to Factor And what can we say about the result? As you can see, the four columns we changed inside mutate has now changed their output: from chr they became Factor w/ n levels and a bunch of numbers instead of the values. I‚Äôm sure you have already understood what does it mean, but to be sure: with ‚Äúlevels‚Äù, R stores the unique values of that vector (remember, each column of a dataframe is a vector!), and then it uses, for each element, integers to refer to the corresponding levels. This ensure memory optimization (integers takes less memory/space than words). But, don‚Äôt worry, when you ask R to visualize the values of a factor, it returns the value and not the numbers: df$Diagnosis[1:5] [1] AD AD AD AD AD Levels: AD Control Reorder levels of a factor There is a cool feature about factors: you can order the levels of it. In fact, by default levels are in alphabetical order. We can see it here, using the function levels(): levels(df$Diagnosis) [1] &quot;AD&quot; &quot;Control&quot; Sometimes, it is useful to change the order of them: think about times (20 weeks comes before 3 days alphabetically), or having a control reference as first in plots (when plotting categorical variable, the order of the levels is the order in the plot). So, let‚Äôs see how we can change the order of the levels using factor: df$Diagnosis &lt;- factor(df$Diagnosis, levels = c(&quot;Control&quot;, &quot;AD&quot;), ordered = T) str(df) &#39;data.frame&#39;: 1493 obs. of 10 variables: $ SampleID : chr &quot;1005_TCX&quot; &quot;1019_TCX&quot; &quot;1029_TCX&quot; &quot;1034_TCX&quot; ... $ Diagnosis: Ord.factor w/ 2 levels &quot;Control&quot;&lt;&quot;AD&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ Braak : num 6 6 6 6 5 6 5.5 6 6 5 ... $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ AOD : num 90 86 69 88 90 72 85 82 77 90 ... $ PMI : num 8 4 4 5 NA 2 NA 15 NA NA ... $ RIN : num 8.6 7.8 9.7 8.9 8.4 9 8 10 8.6 7.9 ... $ CDR : num NA NA NA NA NA NA NA NA NA NA ... $ Area : Factor w/ 6 levels &quot;BM10&quot;,&quot;BM22&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... $ dataset : Factor w/ 3 levels &quot;MAYO&quot;,&quot;MSBB&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ‚ÄúYou are not using tidyverse now, why?‚Äù Right, I used the standard R syntax to highlight the syntax difference with tidyverse: in fact, here I had to put df$ into the function to explicit which column to convert. You can use whatever syntax you want, it does not matter at all; you will see shortly that sometimes tidyverse version is more clear. Having said that, we can now see in the result of str() that now Diagnosis is an Ord.factor w/ 2 levels ‚ÄúControl‚Äù&lt;‚ÄúAD‚Äù. Summary Ok, now that we have fixed some columns, let‚Äôs evaluate the values of the dataframe, we will use summary(): summary(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Length:1493 Control:626 Min. :0.000 F:890 Min. :53.00 Min. : 1.000 Min. : 1.00 Min. :0.000 Class :character AD :867 1st Qu.:3.000 M:603 1st Qu.:79.00 1st Qu.: 3.917 1st Qu.: 5.60 1st Qu.:0.500 Mode :character Median :4.000 Median :86.00 Median : 5.500 Median : 6.80 Median :2.000 Mean :3.909 Mean :83.59 Mean : 7.471 Mean : 6.67 Mean :2.368 3rd Qu.:6.000 3rd Qu.:90.00 3rd Qu.: 9.167 3rd Qu.: 7.80 3rd Qu.:4.000 Max. :6.000 Max. :90.00 Max. :38.000 Max. :10.00 Max. :5.000 NA&#39;s :97 NA&#39;s :45 NA&#39;s :563 Area dataset BM10 :233 MAYO :160 BM22 :238 MSBB :930 BM36 :232 ROSMAP:403 BM44 :227 DLPFC:403 TL :160 How does summary works? It depends on the type of data of the column: Character: it returns the length and the class (See SampleID) Factor: it returns the number of occurrence of each levels (see Diagnosis, sex, Area and dataset). That‚Äôs another reason to use factor over character/numeric when possible. You can easily see if there are imbalance in some categories/levels or not. Numeric: it returns some descriptive (See Braak and others). As you should know the data types you have inserted, you know the range of values you are expecting, so this function helps you to immediately evaluate the presence of some outliers. Boolean: it returns the occurrence of TRUE and FALSE (as a factor with 2 levels) Again, we have these NA‚Äôs‚Ä¶ and now it‚Äôs the time to explain them. Dealing with missing values OPS, I already spoilered it ahahah. NA is the way R represent missing values. In our example, we can see that we have different missing values in different columns, in particular in CDR where 1/3 of the total observations is missing. How to deal with missing values is up to you. First of all, if you have collected the data it is a control on whether all your data are present (if not, you can go back and check why it is missing a data and fill it with a value, if you have it). Usually, NA‚Äôs comes with downloaded data or data from different sources. You can decide if you want to delete all the observation with missing values, fill the missing values with another value (imputation) or accept missing values. There is not a standard on it, it is really up to you. If you want to exclude all rows that have a missing value in any of the column, you should use na.omit function (I won‚Äôt show it because I don‚Äôt want to apply this filter to our data, but it is important that you know this function). Instead, I propose an example on how to delete rows that have missing values in column PMI. We will use the function is.na() that evaluates values of a vector and returns a boolean (TRUE/FALSE) vector on whether a value is NA or not: # 1. Example of is.na is.na(c(1, NA, NA, 4, 9, 12)) [1] FALSE TRUE TRUE FALSE FALSE FALSE You should now tell me how to use it to exclude rows from a dataframe based on whether a value of a column is NA or not. Here is you should have answered: df &lt;- df[!is.na(df$PMI), ] summary(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Length:1448 Control:612 Min. :0.000 F:864 Min. :58.00 Min. : 1.000 Min. : 1.000 Min. :0.000 Class :character AD :836 1st Qu.:3.000 M:584 1st Qu.:79.00 1st Qu.: 3.917 1st Qu.: 5.600 1st Qu.:0.500 Mode :character Median :4.000 Median :86.00 Median : 5.500 Median : 6.700 Median :2.000 Mean :3.881 Mean :83.65 Mean : 7.471 Mean : 6.623 Mean :2.368 3rd Qu.:6.000 3rd Qu.:90.00 3rd Qu.: 9.167 3rd Qu.: 7.800 3rd Qu.:4.000 Max. :6.000 Max. :90.00 Max. :38.000 Max. :10.000 Max. :5.000 NA&#39;s :87 NA&#39;s :518 Area dataset BM10 :233 MAYO :117 BM22 :238 MSBB :930 BM36 :232 ROSMAP:401 BM44 :227 DLPFC:401 TL :117 Great, we have no more NAs in PMI. If you don‚Äôt remember what ! stands for, go back and check the boolean chapter. Filter a dataframe We have just filtered out some rows, and guess what?! Tidyverse has a function to filter values, it is called‚Ä¶‚Ä¶ filter. This function takes as input an expression that returns a vector of boolean. Let‚Äôs try it to retrieve a dataframe of control males with CDR greater than 4: df_male_ctrl_cdr4 &lt;- df %&gt;% filter(Diagnosis == &quot;Control&quot; &amp; sex == &quot;M&quot; &amp; CDR &gt; 4) summary(df_male_ctrl_cdr4) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area dataset Length:3 Control:3 Min. : NA F:0 Min. :89 Min. :2.667 Min. :4.900 Min. :5 BM10 :1 MAYO :0 Class :character AD :0 1st Qu.: NA M:3 1st Qu.:89 1st Qu.:2.667 1st Qu.:5.100 1st Qu.:5 BM22 :1 MSBB :3 Mode :character Median : NA Median :89 Median :2.667 Median :5.300 Median :5 BM36 :1 ROSMAP:0 Mean :NaN Mean :89 Mean :2.667 Mean :5.367 Mean :5 BM44 :0 3rd Qu.: NA 3rd Qu.:89 3rd Qu.:2.667 3rd Qu.:5.600 3rd Qu.:5 DLPFC:0 Max. : NA Max. :89 Max. :2.667 Max. :5.900 Max. :5 TL :0 NA&#39;s :3 We have just 3 observations that matches our request. We can also use functions to evaluate thresholds etc. For example, let‚Äôs take the female whose RIN is above 3rd quartile (75th quantile): df_female_rin3q &lt;- df %&gt;% filter(sex == &quot;F&quot; &amp; RIN &gt; quantile(df$RIN, p = 0.75)) summary(df_female_rin3q) SampleID Diagnosis Braak sex AOD PMI RIN CDR Length:199 Control: 87 Min. :0.000 F:199 Min. :58.00 Min. : 1.000 Min. : 7.900 Min. :0.000 Class :character AD :112 1st Qu.:3.000 M: 0 1st Qu.:83.00 1st Qu.: 3.500 1st Qu.: 8.100 1st Qu.:0.500 Mode :character Median :4.000 Median :88.00 Median : 4.750 Median : 8.500 Median :2.000 Mean :3.899 Mean :85.07 Mean : 5.855 Mean : 8.708 Mean :1.978 3rd Qu.:5.000 3rd Qu.:90.00 3rd Qu.: 6.500 3rd Qu.: 9.200 3rd Qu.:3.000 Max. :6.000 Max. :90.00 Max. :30.000 Max. :10.000 Max. :5.000 NA&#39;s :10 NA&#39;s :109 Area dataset BM10 :13 MAYO :45 BM22 : 9 MSBB :90 BM36 :14 ROSMAP:64 BM44 :54 DLPFC:64 TL :45 There are 199 female samples with a RIN above 3rd quartile. We get it thanks to quantile function, which accept a vector and returns the quartile values; if you want a particular quantile (as in our case), you can give as input p, which is the quantile you want to calculate. Sort for the value of a column You can also sort the entire dataframe based on the values of a column (or more columns if ties are present). We do it through arrange function of tidyverse. For example, let‚Äôs sort the dataframe based on RIN: df %&gt;% arrange(RIN) %&gt;% head(n = 10) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area dataset 1 hB_RNA_12624 AD 6 F 90 3.250000 1.0 4 BM10 MSBB 2 hB_RNA_12768 AD 6 M 82 6.833333 1.5 4 BM10 MSBB 3 hB_RNA_12695 AD 3 F 90 2.750000 1.8 3 BM10 MSBB 4 hB_RNA_8405 AD 5 F 85 9.166667 2.0 3 BM44 MSBB 5 hB_RNA_12615 AD 6 F 84 8.000000 2.1 3 BM10 MSBB 6 hB_RNA_10902 AD 5 F 73 7.333333 2.2 5 BM36 MSBB 7 hB_RNA_9005 AD 6 M 69 4.166667 2.2 5 BM44 MSBB 8 hB_RNA_10782 AD 5 F 90 11.833333 2.3 3 BM36 MSBB 9 hB_RNA_10782_L43C014 AD 5 F 90 11.833333 2.3 3 BM36 MSBB 10 hB_RNA_4341 AD 4 F 90 6.000000 2.4 4 BM44 MSBB There are few consideration about this code: We have sorted df based on RIN in an ascending order. If you want to sort the values in a descending way, you have to wrap the name of the column into desc, as arrange(desc(RIN)) We piped (the symbol %&gt;% is also called pipe) the output of arrange to head function, to get only top 10 rows to visualize Row 6 and 7 have the same RIN value, in this case we can add another column to arrange and use it to sort ties, as arrange(RIN, PMI) Merge two columns Sometimes, you want to merge two columns, and guess what?! There is a function for it ahahah Let‚Äôs merge Area and dataset columns using unite(): df &lt;- df %&gt;% unite(col = &quot;Area_dataset&quot;, Area, dataset, sep = &quot;_&quot;, remove = T) head(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area_dataset 1 1005_TCX AD 6 F 90 8 8.6 NA TL_MAYO 2 1019_TCX AD 6 F 86 4 7.8 NA TL_MAYO 3 1029_TCX AD 6 F 69 4 9.7 NA TL_MAYO 4 1034_TCX AD 6 F 88 5 8.9 NA TL_MAYO 6 1046_TCX AD 6 F 72 2 9.0 NA TL_MAYO 8 1085_TCX AD 6 F 82 15 10.0 NA TL_MAYO Tadaaa! Instead of two columns we now have just one. Looking at the code: col = was used to set the new column name Area, dataset were the two columns to merge. BUT, you can merge more than two columns, simply continue adding column names separated by a comma sep = \"_\" was used to set the separator between the values of the columns (in our case, the underscore) remove = T was used to tell R to remove the columns that we have merged. If set to FALSE, the original columns are kept in the dataframe and the merged column is added Split two columns Obviously, there is also a function to split a column based on a patter: separate. We use it to separate the newly created Area_dataset column into the original ones: df &lt;- df %&gt;% separate(col = Area_dataset, into = c(&quot;Area&quot;, &quot;dataset&quot;), sep = &quot;_&quot;, remove = F) %&gt;% mutate(&quot;Area&quot; = factor(Area), &quot;dataset&quot; = factor(dataset)) head(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area_dataset Area dataset 1 1005_TCX AD 6 F 90 8 8.6 NA TL_MAYO TL MAYO 2 1019_TCX AD 6 F 86 4 7.8 NA TL_MAYO TL MAYO 3 1029_TCX AD 6 F 69 4 9.7 NA TL_MAYO TL MAYO 4 1034_TCX AD 6 F 88 5 8.9 NA TL_MAYO TL MAYO 6 1046_TCX AD 6 F 72 2 9.0 NA TL_MAYO TL MAYO 8 1085_TCX AD 6 F 82 15 10.0 NA TL_MAYO TL MAYO Here it is! We have regained the original columns as well as the merged one. As you may have guessed, into = is used to set the names of the columns that are created by this function. Merge two dataframes To conclude this chapter, we will see how to merge two dataframes based on the values of a common column. This is useful when, for example, you have the output of a differential gene expression with ensemble gene id and another dataframe which associates the ensemble gene ids with gene names, chromosome position etc etc. In this way, you can bring the information of the second dataframe into the first one. In our case, I found in the paper the extended names of the Area abbreviations, and I want to add this info to our table. To do so, let‚Äôs first create the dataframe of abbreviations and full names: # 1. Create vectors abbreviations &lt;- c(&quot;BM10&quot;, &quot;BM22&quot;, &quot;BM36&quot;, &quot;BM44&quot;, &quot;DLPFC&quot;, &quot;TL&quot;) full_name &lt;- c(&quot;Broadmann area 10&quot;, &quot;Broadmann area 22&quot;, &quot;Broadmann area 36&quot;, &quot;Broadmann area 44&quot;, &quot;Dorsolateral prefronal cortex&quot;, &quot;Temporal lobe&quot;) # 2. Create dataframe area_df &lt;- data.frame(abbreviations, full_name) area_df abbreviations full_name 1 BM10 Broadmann area 10 2 BM22 Broadmann area 22 3 BM36 Broadmann area 36 4 BM44 Broadmann area 44 5 DLPFC Dorsolateral prefronal cortex 6 TL Temporal lobe Next, to insert full_name info into our original dataframe we have to use one of the join functions from dplyr package of the tidyverse universe: df &lt;- df %&gt;% left_join(y = area_df, by = c(&quot;Area&quot; = &quot;abbreviations&quot;)) head(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area_dataset Area dataset full_name 1 1005_TCX AD 6 F 90 8 8.6 NA TL_MAYO TL MAYO Temporal lobe 2 1019_TCX AD 6 F 86 4 7.8 NA TL_MAYO TL MAYO Temporal lobe 3 1029_TCX AD 6 F 69 4 9.7 NA TL_MAYO TL MAYO Temporal lobe 4 1034_TCX AD 6 F 88 5 8.9 NA TL_MAYO TL MAYO Temporal lobe 5 1046_TCX AD 6 F 72 2 9.0 NA TL_MAYO TL MAYO Temporal lobe 6 1085_TCX AD 6 F 82 15 10.0 NA TL_MAYO TL MAYO Temporal lobe Just a couple of comments on the code: y = is used to set the dataframe that has to be joined by = is used to declare which variable of the first dataframe is to match in the second dataframe (in our case Area from the first with abbreviations in the second) There are many join functions: left_join (the one we used): keep all the rows of the first dataframe, even if there is not the correspondence in the second on, and only the rows of the second one that have a match in the first right_join: keep all the rows of the second dataframe, even if there is not the correspondence in the first one, and only the rows of the first one that have a match in the second inner_join: keep only the rows that have common values between the two dataframe in the column used to merge full_join: keep every rows of both dataframes This concept is easier to explain with this image: Exercises Woah‚Ä¶ this was tough. Let‚Äôs do some exercises to strengthen these concepts. Exercise 12.1 The first exercise is not a practical exercise sorry‚Ä¶ I want you to read this very short paper on how to structure tabular data. I is mainly on Excel (which you know‚Ä¶ NEVER use it anymore to do analysis), but the key concepts must be applied to every tabular data you are creating. Next, take one of your data file from an experiment and try to be consistent with what you have just read. Exercise 12.2 Take our dataset and filter for males samples of Temporal lobe and female samples of Dorsolateral prefronal cortex. Change full_name to a factor Solution df_filtered &lt;- df %&gt;% filter((sex == &quot;M&quot; &amp; full_name == &quot;Temporal lobe&quot;) | (sex == &quot;F&quot; &amp; full_name == &quot;Dorsolateral prefronal cortex&quot;)) %&gt;% mutate(&quot;full_name&quot; = factor(full_name)) summary(df_filtered) SampleID Diagnosis Braak sex AOD PMI RIN CDR Length:318 Control:152 Min. :0.000 F:263 Min. :61.00 Min. : 1.000 Min. : 5.000 Min. : NA Class :character AD :166 1st Qu.:3.000 M: 55 1st Qu.:84.45 1st Qu.: 4.021 1st Qu.: 6.500 1st Qu.: NA Mode :character Median :4.000 Median :89.20 Median : 5.375 Median : 7.400 Median : NA Mean :3.751 Mean :86.37 Mean : 7.143 Mean : 7.271 Mean :NaN 3rd Qu.:5.000 3rd Qu.:90.00 3rd Qu.: 8.167 3rd Qu.: 8.000 3rd Qu.: NA Max. :6.000 Max. :90.00 Max. :33.000 Max. :10.000 Max. : NA NA&#39;s :25 NA&#39;s :318 Area_dataset Area dataset full_name Length:318 Length:318 MAYO : 55 Dorsolateral prefronal cortex:263 Class :character Class :character MSBB : 0 Temporal lobe : 55 Mode :character Mode :character ROSMAP:263 It is important to note that, since we have one condition OR another, and both have multiple conditions in them, we must use parenthesis to separate the conditions to evaluate. In fact, it works as in math: first parenthesis and then from left to right. This was just an introduction with some cool stuff you can do with tidyverse. In the next chapter we will see more functions that can help us in doing statistics and other useful operations. "],["aggregate-data-with-tidyverse.html", "13 Aggregate data with tidyverse Table Group by variables and summarize values of another Expand dataframe column using pivot wider Reshape a dataframe with pivot_longer Exercises", " 13 Aggregate data with tidyverse After having seen how to organize and manipulate a dataframe, let‚Äôs start analyzing them! In particular, in this chapter we will see how to aggregate data based on values of one or more columns. The first step, as always, is to load the packages and the data: # 1. Load packages library(tidyverse) # 2. Load data df &lt;- read.csv(&quot;data/Supplementary_table_13.csv&quot;) head(df) SampleID Diagnosis Braak sex AOD PMI RIN CDR Area dataset 1 1005_TCX AD 6 F 90 8 8.6 NA TL MAYO 2 1019_TCX AD 6 F 86 4 7.8 NA TL MAYO 3 1029_TCX AD 6 F 69 4 9.7 NA TL MAYO 4 1034_TCX AD 6 F 88 5 8.9 NA TL MAYO 5 1045_TCX AD 5 F 90 NA 8.4 NA TL MAYO 6 1046_TCX AD 6 F 72 2 9.0 NA TL MAYO Then, we will change the column types (as in last chapter): df &lt;- df %&gt;% mutate(&quot;Diagnosis&quot; = factor(Diagnosis, levels = c(&quot;Control&quot;, &quot;AD&quot;)), &quot;sex&quot; = factor(sex), &quot;Area&quot; = factor(Area), &quot;dataset&quot; = factor(dataset)) str(df) &#39;data.frame&#39;: 1493 obs. of 10 variables: $ SampleID : chr &quot;1005_TCX&quot; &quot;1019_TCX&quot; &quot;1029_TCX&quot; &quot;1034_TCX&quot; ... $ Diagnosis: Factor w/ 2 levels &quot;Control&quot;,&quot;AD&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ Braak : num 6 6 6 6 5 6 5.5 6 6 5 ... $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ AOD : num 90 86 69 88 90 72 85 82 77 90 ... $ PMI : num 8 4 4 5 NA 2 NA 15 NA NA ... $ RIN : num 8.6 7.8 9.7 8.9 8.4 9 8 10 8.6 7.9 ... $ CDR : num NA NA NA NA NA NA NA NA NA NA ... $ Area : Factor w/ 6 levels &quot;BM10&quot;,&quot;BM22&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... $ dataset : Factor w/ 3 levels &quot;MAYO&quot;,&quot;MSBB&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... We are now ready to start our very first analyses! Table The very first function we will explore is table. It is used on vectors or dataframe columns and it returns the numerosity of each different value of them; when used for 2 vectors/columns, it returns a contingency matrix. Let‚Äôs see: table(df$sex) F M 890 603 This is the simplest situation, it is not used so much (remember, summary does the same); but, it can be useful if combined with sum to get the fraction of each value. In fact, dividing the table for its sum, we can get the fraction of each value: # 1. Create the table tbl_sex &lt;- table(df$sex) # 2. Calculate the sum of the table sum_tbl_sex &lt;- sum(tbl_sex) # 3. Calculate the fraction tbl_sex / sum_tbl_sex F M 0.5961152 0.4038848 So, ~ 60% of samples comes from females and ~ 40% from males. Yes, we could have just written table(df$sex)/sum(table(df$sex)), one line, less stored variables. We can do the same with two columns, and you will find doing it a lot during your analysis: tbl_sex_diagnosis &lt;- table(&quot;sex&quot; = df$sex, &quot;Diagnosis&quot; = df$Diagnosis) tbl_sex_diagnosis Diagnosis sex Control AD F 341 549 M 285 318 We can also specify names associated to the vectors/columns we use, and they will be displayed in the output. This can be usefule when dealing with two variables whose values are 0/1 or TRUE/FALSE in both. And if I want to see the fraction of females and males that are AD or Control? Right, we can do it. If you try the previous approach, so dividing for the sum of the table, you will get the proportion of each category NOT divided by sex: tbl_sex_diagnosis / sum(tbl_sex_diagnosis) Diagnosis sex Control AD F 0.2283992 0.3677160 M 0.1908908 0.2129940 To get what you really want, you have to use one of colSums or rowSums depending on what you want to sum. Let‚Äôs see how they work: # 1. rowSums tbl_sex_diagnosis_rowsum &lt;- rowSums(tbl_sex_diagnosis) tbl_sex_diagnosis_rowsum F M 890 603 # 2. colSums tbl_sex_diagnosis_colsum &lt;- colSums(tbl_sex_diagnosis) tbl_sex_diagnosis_colsum Control AD 626 867 They perform the sum of the values in each row or the values in each column. So, we can use these values and divide our initial table to get the proportion of each sex value in each diagnosis and vice versa. Remember: when dividing a table for a vector, it performs the division in this way: each element of the first row are divided for the first element of the vector, each element of the second row for the second element of the vector and so on. tbl_sex_diagnosis / tbl_sex_diagnosis_rowsum Diagnosis sex Control AD F 0.3831461 0.6168539 M 0.4726368 0.5273632 We can see that ~38% of females are controls and ~62% of them have AD, while for men the proportion is quite balanced with ~47% of Control samples and ~53% of AD. We can also check what is the proportion of males and females in each Diagnosis condition. To do so, we have to apply a little trick: in fact, we have to transpose our initial table, otherwise the division would not be applied as we want (on each level of Diagnosis), but as before (on each level of sex). We then transpose the result to obtain a table with the same orientation as the initial one. t(t(tbl_sex_diagnosis) / tbl_sex_diagnosis_colsum) Diagnosis sex Control AD F 0.5447284 0.6332180 M 0.4552716 0.3667820 And here it is: ~54% of controls and ~63% of AD are females. Group by variables and summarize values of another The previous function is wonderful also to present data, but you will find yourself dealing with contingency table with more that 2 variables, so here I present you one of the greatest tool for analyzing dataframe: the magical combination of group_by() and summarize() from tidyverse. Let‚Äôs see an example and then analyze how it works: # Suppress summarise info options(dplyr.summarise.inform = FALSE) # creating a new df with summarized counts df_grouped &lt;- df %&gt;% group_by(sex, Diagnosis, dataset) %&gt;% summarise(counts = n()) df_grouped # A tibble: 12 √ó 4 # Groups: sex, Diagnosis [4] sex Diagnosis dataset counts &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 F Control MAYO 37 2 F Control MSBB 186 3 F Control ROSMAP 118 4 F AD MAYO 49 5 F AD MSBB 353 6 F AD ROSMAP 147 7 M Control MAYO 41 8 M Control MSBB 170 9 M Control ROSMAP 74 10 M AD MAYO 33 11 M AD MSBB 221 12 M AD ROSMAP 64 Great! We now have the number of samples for each combination of sex, Diagnosis and dataset. It is important to understand how this code works: options(dplyr.summarise.inform = FALSE): it is a option you can insert at the beginning of a markdown/script, after loading packages, that disable some useless messages when using summarize after group_by group_by(sex, Diagnosis, dataset): it tells dplyr (a tidyverse package) which groups are defined in this dataframe. In this case, we have 12 groups (2 sex * 2 Diagnosis * 3 dataset) summarise(): takes as input a dataframe with pre-defined groups and performs action on each of them counts = n(): creates a new column called counts filled by the result of function n(), which calculates the number of elements in each group Wow, this is cool‚Ä¶ but we can do more things with these functions: in fact, we can use other functions inside summarize() and pass a column in them; the column is divided into pre-defined group and the function is run on each of them. For example, let‚Äôs calculate the mean and sd values of PMI for each combination of sex and Diagnosis: df %&gt;% group_by(sex, Diagnosis) %&gt;% summarise(n = n(), mean = mean(PMI), sd = sd(PMI)) # A tibble: 4 √ó 5 # Groups: sex [2] sex Diagnosis n mean sd &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F Control 341 NA NA 2 F AD 549 NA NA 3 M Control 285 NA NA 4 M AD 318 NA NA Oh, what happened? Why are they all NA? That‚Äôs because if a vector contains NAs, functions like mean, min, max, sd etc returns NA, unless na.rm = T is provided to the functions, which remove NAs from the original vector. df %&gt;% group_by(sex, Diagnosis) %&gt;% summarise(n = n(), mean = mean(PMI, na.rm = T), sd = sd(PMI, na.rm = T)) # A tibble: 4 √ó 5 # Groups: sex [2] sex Diagnosis n mean sd &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F Control 341 7.98 5.81 2 F AD 549 6.21 4.26 3 M Control 285 9.68 6.99 4 M AD 318 7.10 4.72 Great! We can see that AD samples have a lower PMI value in both males and females. This can already be an indication that some differences are present. I guarantee you will find yourself doing this a lot in your life. To conclude this section, let‚Äôs use these functions to recreate the proportion table we calculate in the previous section: df %&gt;% group_by(sex, Diagnosis) %&gt;% summarize(counts = n()) %&gt;% group_by(sex) %&gt;% reframe(Diagnosis = Diagnosis, proportion = counts/sum(counts)) # A tibble: 4 √ó 3 sex Diagnosis proportion &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 F Control 0.383 2 F AD 0.617 3 M Control 0.473 4 M AD 0.527 Just a couple of considerations: After the first summarize, we have grouped again but only by sex, to calculate the proportion of each value of Diagnosis in each sex We used reframe instead of summarize just because summarize would have give a warning, but it acts the same in this scenario We insert Diagnosis = Diagnosis in reframe because otherwise we would have lost this information. Remember: summarize and reframe would return only the columns of the last group_by and the ones calculated in them. Expand dataframe column using pivot wider Even though the previous code works fine, I know that comparing in a table-like structure is better sometimes. So here is a function that can help us in the interpretation and in the presentation of the results: pivot_larger. It is easier to show than to explain: df %&gt;% group_by(sex, Diagnosis) %&gt;% summarize(counts = n()) %&gt;% group_by(sex) %&gt;% reframe(Diagnosis = Diagnosis, proportion = counts/sum(counts)) %&gt;% pivot_wider(id_cols = sex, names_from = Diagnosis, values_from = proportion) # A tibble: 2 √ó 3 sex Control AD &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F 0.383 0.617 2 M 0.473 0.527 That‚Äôs exactly the same as before! Code explanation: id_cols is used to define the columns used as ‚Äúrows‚Äù names_from is used to define the columns that defines column groups values_from is used to define the columns used to fill the table Let‚Äôs see another example, because the previous one could have been written in 3 lines of code as before. df_wider &lt;- df %&gt;% group_by(sex, Diagnosis, dataset) %&gt;% summarise(n = n(), mean = mean(PMI, na.rm = T), sd = sd(PMI, na.rm = T)) %&gt;% pivot_wider(id_cols = c(sex, dataset), names_from = Diagnosis, values_from = c(mean, sd), names_vary = &quot;slowest&quot;) df_wider # A tibble: 6 √ó 6 # Groups: sex [2] sex dataset mean_Control sd_Control mean_AD sd_AD &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F MAYO 7 7.69 6.34 5.80 2 F MSBB 8.60 5.79 5.76 3.91 3 F ROSMAP 7.25 5.18 7.25 4.51 4 M MAYO 5.51 6.42 8.6 5.83 5 M MSBB 11.6 7.44 7.11 4.74 6 M ROSMAP 7.34 4.05 6.58 4.24 Super cool! In this case we used two variables to define rows and two variables to fill values. Reshape a dataframe with pivot_longer Although the previous code returns a cool table, it is suitable only to present data, not for any further analysis. As you may remember from the exercise 12.1, this is not how raw data should be organized. If you find yourself in a situation in which raw data are like that, use pivot_longer() to fix it. df_wider %&gt;% pivot_longer(cols = mean_Control:sd_AD, names_to = c(&quot;statistics&quot;, &quot;Diagnosis&quot;), names_sep = &quot;_&quot;) # A tibble: 24 √ó 5 # Groups: sex [2] sex dataset statistics Diagnosis value &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 F MAYO mean Control 7 2 F MAYO sd Control 7.69 3 F MAYO mean AD 6.34 4 F MAYO sd AD 5.80 5 F MSBB mean Control 8.60 6 F MSBB sd Control 5.79 7 F MSBB mean AD 5.76 8 F MSBB sd AD 3.91 9 F ROSMAP mean Control 7.25 10 F ROSMAP sd Control 5.18 # ‚Ñπ 14 more rows Let‚Äôs explain the code prior to adjust it a bit: cols defines which value columns to take and reshape mean_Control:sd_AD means ‚Äúfrom column mean_Control to column sd_AD‚Äù names_to is used to define names for the new columns that stores the column names defined in cols. We set ‚Äústatistics‚Äù and ‚ÄúDiagnosis‚Äù names_sep is used when names_to has more that one value, it splits the names based on the given pattern (in our case, as the columns were like statistics_Diagnosis we used ‚Äú_‚Äú) Now, as it is a bit confusing having statistics column, we can combine the two pivot_ functions in this way: df_wider %&gt;% pivot_longer(cols = mean_Control:sd_AD, names_to = c(&quot;statistics&quot;, &quot;Diagnosis&quot;), names_sep = &quot;_&quot;) %&gt;% pivot_wider(id_cols = c(sex, dataset, Diagnosis), names_from = statistics, values_from = value) # A tibble: 12 √ó 5 # Groups: sex [2] sex dataset Diagnosis mean sd &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F MAYO Control 7 7.69 2 F MAYO AD 6.34 5.80 3 F MSBB Control 8.60 5.79 4 F MSBB AD 5.76 3.91 5 F ROSMAP Control 7.25 5.18 6 F ROSMAP AD 7.25 4.51 7 M MAYO Control 5.51 6.42 8 M MAYO AD 8.6 5.83 9 M MSBB Control 11.6 7.44 10 M MSBB AD 7.11 4.74 11 M ROSMAP Control 7.34 4.05 12 M ROSMAP AD 6.58 4.24 Tadaaaaa! I think this is enough for this chapter, here are some exercise to practice these functions. Exercises Exercise 13.1 This is a multi-question exercise, that resemble a real-life analysis. Considering our initial dataframe: Are there differences between datasets in RIN value? consider max, min and mean Which Area has the highest value of AOD? And, is there difference between sex in these values? How many NAs are present in CDR for each combination of Diagnosis and dataset? Sort the dataframe based on the average vale of PMI for each area, based also on sex Solution We will answer one question at a time. 1. # 1. Calculate mean, min and max values of RIN for each dataset df_RIN_dataset &lt;- df %&gt;% group_by(dataset) %&gt;% summarise(min = min(RIN, na.rm = T), mean = mean(RIN, na.rm = T), max = max(RIN, na.rm = T)) df_RIN_dataset # A tibble: 3 √ó 4 dataset min mean max &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 MAYO 5.3 8.11 10 2 MSBB 1 6.23 10 3 ROSMAP 5 7.11 9.9 Yes, it seems that MAYO dataset is the one with better RIN values, while MSBB has the worst samples. 2. # 1. Calculate max value of AOD for each dataset df_AOD_dataset &lt;- df %&gt;% group_by(dataset, sex) %&gt;% summarise(max_AOD = max(AOD, na.rm = T)) %&gt;% pivot_wider(id_cols = dataset, names_from = sex, values_from = max_AOD) df_AOD_dataset # A tibble: 3 √ó 3 # Groups: dataset [3] dataset F M &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 MAYO 90 90 2 MSBB 90 90 3 ROSMAP 90 90 No, there is not difference between dataset, even considering sex. 3. # 1. Calculate number of CDR NAs for each Diagnosis/dataset combination df_nas_diag_dataset &lt;- df %&gt;% group_by(Diagnosis, dataset) %&gt;% summarise(n_nas = sum(is.na(CDR))) %&gt;% pivot_wider(id_cols = Diagnosis, names_from = dataset, values_from = n_nas) df_nas_diag_dataset # A tibble: 2 √ó 4 # Groups: Diagnosis [2] Diagnosis MAYO MSBB ROSMAP &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 Control 78 0 192 2 AD 82 0 211 It seems that people that have filled MSBB dataset have done a better job in collecting data than the others ahahah Remember: is.na() returns a boolean vector, and the sum of a boolean vector is the number of TRUE values in it. 4. # 1. Sort dataframe based on mean PMI value for each Area/sex df_PMI_area_sex &lt;- df %&gt;% group_by(Area, sex) %&gt;% summarise(avg_PMI = mean(PMI, na.rm = T)) %&gt;% arrange(desc(avg_PMI)) df_PMI_area_sex # A tibble: 12 √ó 3 # Groups: Area [6] Area sex avg_PMI &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 BM44 M 9.25 2 BM36 M 9.16 3 BM10 M 9.11 4 BM22 M 8.69 5 DLPFC F 7.25 6 BM10 F 7.02 7 DLPFC M 6.99 8 BM36 F 6.70 9 TL F 6.66 10 TL M 6.64 11 BM22 F 6.64 12 BM44 F 6.60 Males BM44 samples have the higher mean of PMI. I‚Äôm so proud of you to have reached this chapter. What‚Äôs next? Plotting! Data should be visualized for a better understand of them. "],["plot-our-data-with-ggplot2.html", "14 Plot our data with ggplot2 Command structure Type of graph (geom_ and stat_) Change axis and colors (scale_) Add title and change labels (labs) Change appearence (theme) Save plot", " 14 Plot our data with ggplot2 What is the best way to present data/results to someone? Of course, it‚Äôs through plots. You can have the best results in the world, you can have discovered the cure for the cancer, but without an efficient way to present your findings, you will not be able to share them. So, here we are talking about ggplot2, which is one of the most used library for plotting in R (in my opinion, the best option to create plots, both in R and Python). Here is the official website of this magical library; it is part of the tidyverse and it has a lot of ‚Äúextensions‚Äù (packages built up on ggplot that add functionalities, but let‚Äôs start with the basic). To be honest, it would take an entire book to talk about ggplot2 (here an example); for this reason, here I will ‚Äújust‚Äù show you what I think is enough to create wonderful graphs for transimitting experiment results. In next chapters I may use some functions and functionalities that are not covered in this one, but if you are interested and want to master in ggplot, I recommend: practice, explore and search on the internet! Let‚Äôs now dive into the ocean of ggplot, starting by loading some data: we will use the same we used in previous chapters on tidyverse so you should be an expert on them üòÑ. # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) library(ggplot2) # 2. Load data df &lt;- read.csv(&quot;data/Supplementary_table_13.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;Diagnosis&quot; = factor(Diagnosis, levels = c(&quot;Control&quot;, &quot;AD&quot;)), &quot;sex&quot; = factor(sex), &quot;Area&quot; = factor(Area), &quot;dataset&quot; = factor(dataset)) str(df) &#39;data.frame&#39;: 1493 obs. of 10 variables: $ SampleID : chr &quot;1005_TCX&quot; &quot;1019_TCX&quot; &quot;1029_TCX&quot; &quot;1034_TCX&quot; ... $ Diagnosis: Factor w/ 2 levels &quot;Control&quot;,&quot;AD&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ Braak : num 6 6 6 6 5 6 5.5 6 6 5 ... $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ AOD : num 90 86 69 88 90 72 85 82 77 90 ... $ PMI : num 8 4 4 5 NA 2 NA 15 NA NA ... $ RIN : num 8.6 7.8 9.7 8.9 8.4 9 8 10 8.6 7.9 ... $ CDR : num NA NA NA NA NA NA NA NA NA NA ... $ Area : Factor w/ 6 levels &quot;BM10&quot;,&quot;BM22&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... $ dataset : Factor w/ 3 levels &quot;MAYO&quot;,&quot;MSBB&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Command structure Data are loaded, we are ready! Here, it is important to know how we should structure the code to create a plot with ggplot. The cool thing about this package, is that code is structured in a very intuitive way (at least, I find it very intuitive): this is the ‚Äúskeleton‚Äù of a ggplot code: ggplot(data = &lt;data&gt;, mapping = aes(&lt;misc&gt;)) + geom_&lt;type&gt;(mapping = aes(&lt;misc&gt;)) + scale_&lt;type&gt;() + labs() + coord_&lt;type&gt; + theme_&lt;type&gt; + theme() This is just an example, with lots of placeholders ( and ) that we will explore during this trip. You can see that each function is connected with the next one with a plus ‚Äú+‚Äù sign, that‚Äôs because you can imagine the structure of this code as a chain of commands, whose order is not that important (only the first two lines of code must be in those positions). Each function define different aspects of the graph, and you can have multiple function of the same type in the same command. Don‚Äôt worry, I know it sounds confusing, but by the end of the chapter you will understand everything. Important: to create a basic plot, only the first two lines of code are needed, the other components have default or calculated values, so we can omit them. ggplot(data = &lt;data&gt;) is the initiating function of the plot. It is fundamental and all ggplot codes must start with it. In this code, we can (usually we do) define which is the dataframe we will use to create the plot. In fact, the input of ggplot is a data.frame object. Type of graph (geom_ and stat_) Next, you have to decide which kind of graph you want to create, and you set it by choosing the corresponding geom_ function. For example, to create a lineplot you will use the geom_line function, to create a boxplot you will use the geom_boxplot function and so on. There are plenty of them and you can find the full list here; you will find yourself using mostly geom_bar (for barplots), geom_line, geom_boxplot, geom_histogram (for histograms), geom_density (for density plot), geom_text (for text) and geom_hline, geom_vline and geom_abline for horizontal, vertical and general lines. Mapping values to graph elements (aes) ‚ÄúOk, I‚Äôm on it. But where I can set which values to plot?‚Äù I know this is what you want to ask me; and here is the answer: what to plot must be given in the form of aes function to mapping argument. The basic elements of aes are x and y argument, where you define which variable should be plot on x axis and which one on y axis. Let‚Äôs make our first example of a plot by creating a dotplot to evaluate a possible correlation between AOD and PMI. Remember, a basic plot can be created just using ggplot() + geom_&lt;type&gt;()! aod_pmi &lt;- ggplot(data = df, mapping = aes(x = AOD, y = PMI)) + geom_point() aod_pmi Cool! We have created our first graph using ggplot. As you can see, as we have passed a data.frame to data = and ggplot is part of tidyverse, we can use column name as variables in the code. Now, we can create even a better code because each geom_ function has its own aesthetic properties that can be set. For example, we can change the size and the color of the dots in this plot: we can do by giving a value to all the points, or (and you will find this very cool), based on the values of another vector. What do I mean? Let‚Äôs see it together. # 1. Create a plot with color and size set equal for all points aod_pmi_all &lt;- ggplot(data = df, mapping = aes(x = AOD, y = PMI)) + geom_point(color = &quot;#BD4F6C&quot;, size = 3) # 2. Create a plot with color based on sex and size based on RIN aod_pmi_sex_rin_aes &lt;- ggplot(data = df, mapping = aes(x = AOD, y = PMI)) + geom_point(mapping = aes(color = sex, size = RIN)) # 3. Arrange the plots together (we will see this in details soon) suppressPackageStartupMessages(library(ggpubr)) aod_pmi_arr &lt;- ggarrange(aod_pmi_all, aod_pmi_sex_rin_aes, ncol = 2, nrow = 1, align = &quot;h&quot;, labels = &quot;AUTO&quot;) aod_pmi_arr Let‚Äôs focus on point 1 and 2 (the creation of the two plots): In graph A, we set the color and the size of all the points inside geom_point function but NOT in mapping argument, as in graph B. The color code I give is an hex code for colors; if you don‚Äôt know what hex code means, check this. I used this tool to create palettes and choose colors. Try it! When setting the mapping, you can think like ‚Äúfor EACH point, what characteristic should I map to it?‚Äù In this case, the color for each point is based on the value of sex column (factor) and the size on RIN (numeric). This can be useful to highlight differences based on different columns. Change the ‚Äústat‚Äù associated to the graph I have to introduce a more advance concept now: stat. Let‚Äôs first see an example, and then understand what stat are. Let‚Äôs create a barplot that summarize the number of males and females in the data.frame: males_females_barplot &lt;- ggplot(data = df) + geom_bar(mapping = aes(x = sex)) males_females_barplot ‚ÄúHey, wait a minute! The data.frame contains multiple entries, you gave only one mapping (x). How does it summarize the values?‚Äù That‚Äôs stat, the way ggplot ‚Äúoperates‚Äù on the given data. Each geom_ function has its own default stat and sometimes you want to change it. In this case, geom_bar has as defualt stat ‚Äúcount‚Äù, which means that for each x value, it counts the occurrence and plot the value. Other geom have ‚Äúidentity‚Äù as default transformation that associate an y value to each x value (as geom_point). I‚Äôm telling you this, because sometimes you may find yourself in the situation where you have a data.frame with x and y values, and you want to create a barplot with them. Let‚Äôs see an example by recreating the same plot as before, but with a different approach: # 1. Create a dataframe with the number of males and females sex_df &lt;- df %&gt;% group_by(sex) %&gt;% summarise(n = n()) # 2. Create the plot sex_barplot &lt;- ggplot(data = sex_df) + geom_bar(mapping = aes(x = sex, y = n), stat = &quot;identity&quot;) sex_bar_arr &lt;- ggarrange(males_females_barplot, sex_barplot, ncol = 2, nrow = 1, align = &quot;h&quot;, labels = &quot;AUTO&quot;) sex_bar_arr The two graphs are identical! This because changing the stat to ‚Äúidentity‚Äù in the second plot, allow us to plot dataframes that are already ‚Äúsummarized‚Äù. However, I suggest the first approach. Multiple geometries in the same plot We can use multiple geom_ on the same graph. For example, we can add two lines in the previous dotplot (AOD vs.¬†PMI) representing ‚Äúthresholds‚Äù. From a coding point of view, we could start again from the beginning, but it is easier to add the lines to the previous stored graph object: # 1. Define thresholds as 10th quantile aod_thresh &lt;- quantile(df$AOD, probs = 0.1, na.rm = T) pmi_thresh &lt;- quantile(df$PMI, probs = 0.1, na.rm = T) # 2. Add to graph aod_pmi_sex_rin_aes &lt;- aod_pmi_sex_rin_aes + geom_hline(yintercept = pmi_thresh, color = &quot;#FF0000&quot;, linetype = &quot;dashed&quot;, size = 2.5) + geom_vline(xintercept = aod_thresh, color = &quot;#0000FF&quot;, linetype = &quot;dotted&quot;, size = 3) aod_pmi_sex_rin_aes We have used geom_hline and geom_vline, giving the values of yintecept and xintercept respectively; we also set the color and the linetype (dashed and dotted). We should have the lowest 10% of PMI values below the red line and the lowest 10% of AOD values on the left of the blue line. Change axis and colors (scale_) The previous graphs are ok, they show what we want but we didn‚Äôt set anything apart from the data to show and the type of graph. Let‚Äôs now create a basic boxplot to investigate how RIN values changes based on the Area, and modify the axis ticks and setting the colors we want. rin_area_boxplot &lt;- ggplot(df) + geom_boxplot(aes(x = Area, fill = Area, y = RIN), na.rm = T, outlier.shape = 2) rin_area_boxplot Cool, we can see that the distributions of RIN values for some areas are different than the others. We have also changed the shape of the outliers, making them easier to identify. Let‚Äôs now modify some aspects: The y axis spans from the lower value to the highest value (this is the default behaviour of ggplot). Let‚Äôs change the limits of this axis We want also the breaks of y axis to be even numbers (2, 4, 6, 8, 10) Colorscheme is ok, but let‚Äôs change it All these things can be achieved usng different scale_&lt;type&gt; functions, in particular we will use scale_y_continuous to address the first two points and scale_fill_brewer for the last one. Can you guess which is the naming scheme of the scale_ functions? Let me answer it: it starts with scale_, than what to change (y or fill in this case) and the type of data we have (continuous and brewer). rin_area_boxplot_edit &lt;- ggplot(df) + geom_boxplot(aes(x = Area, fill = Area, y = RIN), na.rm = T, outlier.shape = 2) + scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2)) + scale_fill_brewer(palette = &quot;Set1&quot;) rin_area_boxplots_arr &lt;- ggarrange(rin_area_boxplot, rin_area_boxplot_edit, ncol = 2, nrow = 1, align = &quot;h&quot;, labels = &quot;AUTO&quot;) rin_area_boxplots_arr Great! we succesfully changed the y axis breaks and limits, and the fill colors. Let‚Äôs dissect the code a bit: limits = accepts a vector of two values, representing the lower and upper limits of the axis, breaks = accepts a vector of the values used to create the breaks. Concerning fill colors, we used a function that accept the name of a palette from RColorbrewer. This is a valid approach, even if I prefer another one that is more project-safe (and I suggest using it): I create one .csv file for each categorical variable I want with a column corresponding to the levels and one to the color code (or linetype, or shape) I load this file and create a named vectors with colors as values and labels as names I use scale_&lt;type&gt;_manual(values = named_vector). This function use the given values and matching the labels. This approach ensures that we use the same color/shape/linetype ecc for each label in a project and, most important, it is super easy to change something when we want: instead of change it in each code of the project, we change the value of the csv file and launch again all the scripts. Let‚Äôs do it: # 1. Load the file color_df &lt;- read.csv(&quot;refs/area_colors.csv&quot;) # 2. Create a named vector area_color_vector &lt;- structure(color_df$Color, names = color_df$Area) # 3. Create the plot rin_area_boxplot_manual &lt;- ggplot(df) + geom_boxplot(aes(x = Area, fill = Area, y = RIN), na.rm = T, outlier.shape = 2) + scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2)) + scale_fill_manual(values = area_color_vector) rin_area_boxplot_manual This is the color file I used. You can try different scales for different aspects of the plotting to create better plots; here I just show you an example, but I‚Äôm sure that we will encounter them in next chapters. Add title and change labels (labs) To make out plots easier to understand, we can set a title, a subtitle and change the labels of the axes through labs() function. Let‚Äôs see it: rin_area_boxplot_manual &lt;- rin_area_boxplot_manual + labs(title = &quot;RIN distribution across areas&quot;, subtitle = &quot;TL samples have higher RIN values compared to the others&quot;, x = &quot;&quot;, caption = &quot;2023 R for biologist&quot;, tag = &quot;ggplot2&quot;) rin_area_boxplot_manual Look, now it is much more detailed and professional. But still, we can improve it. How? with theme_ functions. Change appearence (theme) Through theme_ functions we can change the aspect of the axes, title, subtitle, legend, and so on‚Ä¶ I know you have familiarity with PowerPoint, so let‚Äôs imagine them as ‚ÄúPicture Format‚Äù menu. We have a set of pre-built themes, on top of which we can set our own settings. Let‚Äôs start woth a pre-built theme, because I love ggplot but I hate the background of the previous plot; I prefere using theme_classic for my plots, with little adjustments. rin_area_boxplot_manual &lt;- rin_area_boxplot_manual + theme_classic() rin_area_boxplot_manual Wow, now the background is gone and we have a cleaner plot üòç. This is a good starting point; let‚Äôs now remove the legend, as it is not adding any information, and highlight the title. rin_area_boxplot_manual &lt;- rin_area_boxplot_manual + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 14, hjust = 0.5)) rin_area_boxplot_manual Perfect! Now the title is bigger and in bold, and we do not have the legened anymore. LEt‚Äôs discuss about how we change the title: in ggplot, all the text elements of a plot (title, axis title, axis labels, etc) are controlled through element_text funtion, while all the line elements (axes, grid, axis ticks, etc) are controlled through element_line funtion. The argument of these functions are straightforward and super intuitive, there is no need to explain them now, you can explore yourself! Save plot Now that we have created our plot, it‚Äôs time to save it! ggplot comes with its own function to save plots: ggsave. Here is how to use it: ggsave(filename = &quot;example.pdf&quot;, plot = rin_area_boxplot_manual, device = &quot;pdf&quot;, width = 14, height = 14, units = &quot;cm&quot;) The argument are self-explanatory here, so you should not have any issue in saving all of you plots. Suggestion: play with dimensions in theme_ and ggsave functions to optimize your plot. Moreover, saving the file as pdf or svg let‚Äôs you easilly modify it with Gimp or Photoshop/Illustrator, scaling them without losing any resolution. Here we are at the end of this introduction of ggplot2. As I told you at the beginning, it would take an entire book to explain every byte of code for plotting; in this chapter, I gave you an introduction and suggest you to play and explore this magical world. However, dedicated chapters will come for each type of graph, but now it‚Äôs time to talk about statistical analyses. ‚Äôcause, yes‚Ä¶ plotting is good, but at the end of the day, they are a way to transmit what you found in your experiments and the significance associated to it. More detailed in the future "],["intro-stat-chapter.html", "15 Introduction to statistics in R Evaluate parametric assumptions", " 15 Introduction to statistics in R What a journey so far! And now, maybe the scariest thing for a researcher: statistics. Don‚Äôt worry, thanks to R it is easier than ever; in fact, R is the best programming language for statistics, it has lot of built-in functions and other packages for computing the right statistic you want. Here, I will do a brief introduction to statistical tests, focusing in next chapters on each type. So, first of all the choice of the right test is fundamental: here, you can see a scheme describing how to choose the right test depending on you design (you can download a pdf version here). This scheme is huge, but should be your mantra while doing analysis and, in particular, when you are designing your experiment. Why so much emphasis?! Because you should decide the test, the sample size etc prior to perform the experiments: experimental design makes 90% of the work. Remember it! That said, we can see that the most asked question in the schema is ‚ÄúParametric data?‚Äù, so let‚Äôs discuss about it and how to assess wheteher our data are parametric or not. Evaluate parametric assumptions Depending on if the data are parametric or not, we will use different tests. Here are the 4 assumptions for data to be parametric: Normality: data in each group must be normally distributed Homoschedasticity: there must be equal variances between groups you are going to compare Independency: data belonging to a group should be independent No outliers: there should not be any outlier in any group. We will now see how to evaluate all the points except for 3, as there is no way to ‚Äúmeasure‚Äù it and it depends on your design. First of all, let‚Äôs import packages and data. We will now use a different dataset for these chapters, as the previous one was a sample-descriptive one, while here we want something with results. Through &lt;a href‚Äùscript/Stat-test-dataset_create‚Äù download&gt;this script I created a custom dataset, which you can download here # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) library(ggplot2) # 2. Load data df &lt;- read.csv(&quot;data/Stat-test-dataset.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;sex&quot; = factor(sex), &quot;treatment&quot; = factor(treatment, levels = c(&quot;Untreated&quot;, &quot;T1&quot;)), &quot;Task1&quot; = factor(Task1), &quot;Task2&quot; = factor(Task2), ) str(df) &#39;data.frame&#39;: 600 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 2 1 2 1 2 1 2 1 ... $ age : int 3 3 3 3 3 3 3 3 3 3 ... $ treatment: Factor w/ 2 levels &quot;Untreated&quot;,&quot;T1&quot;: 1 1 2 2 1 1 2 2 1 1 ... $ weight : num 6.43 3.42 4.97 4.63 7.31 3.27 5.58 3.27 5.9 4.22 ... $ Task1 : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 2 2 2 2 1 2 ... $ Task2 : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 2 1 1 2 2 2 ... $ Task3 : num 222.4 202.3 36.7 221.8 178.8 ... Let‚Äôs explain this dataset: sex, age, weight (g) and treatment are self explanatory. These can be data from a disease mouse model treated with a test drug Task1 and Task2: this score indicates whether the mouse successfully (1) did the task or not (0) Task3: this indicates the time spent by the mouse to complete the task, in seconds You can produce some plots at this point, if you want to explore the dataset. We will now move on to explore the parametric assumptions. For this example, we will test if for comparing both weight and Task3 between T1-treated males at age == 30 vs.¬†Untreated we should use parametric or non-parametric tests; we will see in details all the others comparisons when we will Samplely see the statistical tests. So, just for now, we will extrapolate from the df the data of interest: male_30_weight_task3 &lt;- df %&gt;% filter(sex == &quot;Male&quot; &amp; age == 30) %&gt;% select(sex, age, treatment, weight, Task3) head(male_30_weight_task3) sex age treatment weight Task3 1 Male 30 T1 19.38 290.04 2 Male 30 Untreated 19.17 123.19 3 Male 30 T1 20.31 98.74 4 Male 30 Untreated 18.76 148.15 5 Male 30 T1 18.85 312.12 6 Male 30 Untreated 19.98 183.93 Alright, here are the data we need. I used a function that we didn‚Äôt see in the tidyverse chapter: select. It is used to select only specific columns of the dataset; in this case we use it to extrapolate columns of interest. Visual inspection Visual inspection is always recommended, as it can give us an idea of how our data are distributed and the eventual presence of outliers. We will see boxplot, density plot and QQ plot (read about it here). Here the code to produce the three plots: # 1. Create boxplots male_30_weight_boxplot &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = weight, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_boxplot &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = Task3, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 2. Create density plots male_30_weight_density &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = weight, fill = treatment), alpha = 0.6) + labs(x = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_density &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = Task3, fill = treatment), alpha = 0.6) + labs(x = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 3. Create QQ plots male_30_weight_qq &lt;- ggplot(male_30_weight_task3, aes(sample = weight, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme() male_30_task3_qq &lt;- ggplot(male_30_weight_task3, aes(sample = Task3, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme() ggarrange(male_30_weight_boxplot, male_30_weight_density, male_30_weight_qq, male_30_task3_boxplot, male_30_task3_density, male_30_task3_qq, ncol = 3, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;hv&quot;) Yet from these graph we can start to say that while for weight we can think of using parametric tests (after having removed possible outliers in T1-treated), for Task3 this is not possible as T1-treated distribution is far away from being normal or having same variances. Now, we apply some statistical tests to evaluate all the assumptions. No outliers As outliers may influence tests for normality and homoschedasticity, we start evaluating possible outliers in our data. To do so, I like to use InterQuartile Range (IQR) method: data that are 1.5 * IQR below 25th quantile or 1.5 * IQR over 75th quantile are considered as outlier. I usually use a function that takes as input a vector of values and returns one of booleans indicating whether the value is an outlier (TRUE) or not (FALSE), and create a column in the dataframe with this information. This is the function: is_outlier &lt;- function(x) { # 1. Check if x is numeric if (!is.numeric(x)) {stop(&quot;x must be a numeric vector&quot;)} # 2. Check if at least 3 values in x if (length(x) &lt; 3) {stop(&quot;x must have at least 3 values&quot;)} # 3. Calculate IQR q1 &lt;- quantile(x, 0.25, na.rm = T) q3 &lt;- quantile(x, 0.75, na.rm = T) iqr &lt;- q3 - q1 # 4. Calculate thresholds lower_threshold &lt;- q1 - 1.5 * iqr upper_threshold &lt;- q3 + 1.5 * iqr # 5. Create boolean vector return (x &lt; lower_threshold | x &gt; upper_threshold) } If you want to see the explanation of the function steps, expand following section. Code explanation Here, I created a function called is_outlier that accepts one input (x), which must be a numeric vector of at least 3 values. These conditions are evaluated in the first two steps. It is always a good practice to evaluate the inputs validity at the beginning of the function, and stop the execution whether not all of them are valid; in this way, we can immediately stop the function, not performing possible highly computational tasks that the function should perform after. Once checked the input, we calculated IQR using quantile function that we already seen. Then, we compute the thresholds and return a boolean vector after having evaluated if values are below lower threshold or above upper one. Let‚Äôs now use this function to find possible outliers in our data: male_30_weight_task3 &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% mutate(&quot;outlier_weight&quot; = is_outlier(weight), &quot;outlier_Task3&quot; = is_outlier(Task3)) head(male_30_weight_task3) # A tibble: 6 √ó 7 # Groups: sex, age, treatment [2] sex age treatment weight Task3 outlier_weight outlier_Task3 &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; 1 Male 30 T1 19.4 290. FALSE FALSE 2 Male 30 Untreated 19.2 123. FALSE FALSE 3 Male 30 T1 20.3 98.7 FALSE FALSE 4 Male 30 Untreated 18.8 148. FALSE FALSE 5 Male 30 T1 18.8 312. FALSE FALSE 6 Male 30 Untreated 20.0 184. FALSE FALSE Great, we can now see how many outliers we have per group and which are, to then decide what to do with them. We can start looking at how many outliers we have: n_out_df &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% summarise(&quot;n_out_weight&quot; = sum(outlier_weight), &quot;n_out_Task3&quot; = sum(outlier_Task3)) n_out_df # A tibble: 2 √ó 5 # Groups: sex, age [1] sex age treatment n_out_weight n_out_Task3 &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; 1 Male 30 Untreated 0 0 2 Male 30 T1 3 0 Alright, we have 3 outliers for weight in T1-treated group. Let‚Äôs see which are those values: male_30_weight_task3 %&gt;% filter(outlier_weight) # A tibble: 3 √ó 7 # Groups: sex, age, treatment [1] sex age treatment weight Task3 outlier_weight outlier_Task3 &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; 1 Male 30 T1 17.4 216. TRUE FALSE 2 Male 30 T1 16.8 122. TRUE FALSE 3 Male 30 T1 21.4 119. TRUE FALSE They are exactly the values we have identified in the boxplot before! Now, we have to decide what to do with these values; there are many options depending on your design, number of samples, and other factors. In this case, for example, we could see if those mice are outliers at other ages (if we have evaluated the same mice over time). As we have good number of samples for each group, we can decide to exclude these mice at this time point: we can delete those rows or replace values with NA. For now, we use the latter, as we want to keep Task3 values for the remaining part of the examples; usually, you should evaluate one variable at a time, so you can remove values directly. male_30_weight_task3$weight[male_30_weight_task3$outlier_weight] &lt;- NA We can now re-evaluate outliers: # 1. Create boxplots male_30_weight_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = weight, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = Task3, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 2. Create density plots male_30_weight_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = weight, fill = treatment), alpha = 0.6) + labs(x = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = Task3, fill = treatment), alpha = 0.6) + labs(x = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 3. Create QQ plots male_30_weight_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = weight, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() male_30_task3_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = Task3, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() ggarrange(male_30_weight_boxplot_filt, male_30_weight_density_filt, male_30_weight_qq_filt, male_30_task3_boxplot_filt, male_30_task3_density_filt, male_30_task3_qq_filt, ncol = 3, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;hv&quot;) We can see that we still have one outlier for weight in T1-treated. This is perfectly normal, as we have changed the quantiles by removing 3 values. Here, it is up to you to decide what to do based on your experience, other data etc. In this case, I will exclude it as weight usually is normally distributed. male_30_weight_task3 &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% mutate(&quot;outlier_weight&quot; = is_outlier(weight)) n_out_df &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% summarise(&quot;n_out_weight&quot; = sum(outlier_weight, na.rm = T)) n_out_df # A tibble: 2 √ó 4 # Groups: sex, age [1] sex age treatment n_out_weight &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 Male 30 Untreated 0 2 Male 30 T1 1 male_30_weight_task3$weight[male_30_weight_task3$outlier_weight] &lt;- NA We can now re-evaluate outliers: # 1. Create boxplots male_30_weight_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = weight, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = Task3, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 2. Create density plots male_30_weight_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = weight, fill = treatment), alpha = 0.6) + labs(x = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = Task3, fill = treatment), alpha = 0.6) + labs(x = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 3. Create QQ plots male_30_weight_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = weight, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() male_30_task3_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = Task3, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() ggarrange(male_30_weight_boxplot_filt, male_30_weight_density_filt, male_30_weight_qq_filt, male_30_task3_boxplot_filt, male_30_task3_density_filt, male_30_task3_qq_filt, ncol = 3, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;hv&quot;) Again, let‚Äôs for the last time try to exclude the outlier: male_30_weight_task3 &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% mutate(&quot;outlier_weight&quot; = is_outlier(weight)) n_out_df &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% summarise(&quot;n_out_weight&quot; = sum(outlier_weight, na.rm = T)) n_out_df # A tibble: 2 √ó 4 # Groups: sex, age [1] sex age treatment n_out_weight &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; 1 Male 30 Untreated 0 2 Male 30 T1 1 male_30_weight_task3$weight[male_30_weight_task3$outlier_weight] &lt;- NA We can now re-evaluate outliers: # 1. Create boxplots male_30_weight_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = weight, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_boxplot_filt &lt;- ggplot(male_30_weight_task3) + geom_boxplot(aes(x = treatment, y = Task3, fill = treatment), outlier.shape = 2) + labs(x = &quot;Treatment&quot;, y = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 2. Create density plots male_30_weight_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = weight, fill = treatment), alpha = 0.6) + labs(x = &quot;Weight (g)&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) male_30_task3_density_filt &lt;- ggplot(male_30_weight_task3) + geom_density(aes(x = Task3, fill = treatment), alpha = 0.6) + labs(x = &quot;Time (s)&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() + theme(axis.ticks.x = element_blank()) # 3. Create QQ plots male_30_weight_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = weight, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 weight&quot;) + theme_classic() male_30_task3_qq_filt &lt;- ggplot(male_30_weight_task3, aes(sample = Task3, color = treatment)) + geom_qq() + geom_qq_line() + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;, title = &quot;Male age = 30 Task3&quot;) + theme_classic() ggarrange(male_30_weight_boxplot_filt, male_30_weight_density_filt, male_30_weight_qq_filt, male_30_task3_boxplot_filt, male_30_task3_density_filt, male_30_task3_qq_filt, ncol = 3, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;hv&quot;) Great, we now have not any outlier and we can move on with the other checks. Normality We now check if our data are normally distributed. There are two main tests that are used for this: Shapiro-Wilk normality test and Kolmogorov-Smirnov test. The first one is specific for normal distributions, while the second one compares two samples and returns whether they belong to the same distribution, by setting the second sample as a normal distribution, it comes that is like Shapiro-Wlik test. As all frequentest statistical tests, these two have null and alternative hypothesis and returns p.values: the null hypothesis is that the distribution of the sample is normal (or comes from the same distribution in the case we have two samples). So, a p.value &gt; 0.05 (or another user-defined threshold) means we cannot refuse the null hypothesis,so the sample has a normal distribution (or comes from the same distribution as the other sample). However, they both hae limitatios when dealing with large samples (n &gt;150/200); in those cases, you could look at the plots and decide, together with the fact that Central Limit Theorem often comes into play in these cases; you should test homoschedasticity as well. We will now see the structure of the results of these test, and then apply them to our data. Let‚Äôs create two samples, one normally distributed and one not. set.seed(111) # 1. Create normally distributed sample quantiles &lt;- seq(0.01, 0.99, length.out = 100) norm_sample &lt;- qnorm(quantiles, mean = 10, sd = 2) # 2. Create non-normally distributed sample non_norm_sample &lt;- runif(n = 100, min = 1, max = 20) * runif(n = 100, min = 0, max = 1) Now, we just visualize the data as we would do in a real-life scenario: norm_sample_density &lt;- ggplot() + geom_density(aes(norm_sample)) + labs(x = &quot;&quot;, title = &quot;Normally distributed data&quot;) + theme_classic() non_norm_sample_density &lt;- ggplot() + geom_density(aes(non_norm_sample)) + labs(x = &quot;&quot;, title = &quot;Non normally distributed data&quot;) + theme_classic() ggarrange(norm_sample_density, non_norm_sample_density, ncol = 1, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;v&quot;) Great! We can clearly see by visual inspection that the first is normmally distributed and the second one not, but we are here to see statistical tests. Shapiro-Wilk normality test We start with shapiro.test: # 1. Shapiro-Wilk test norm_sample_shapiro_res &lt;- shapiro.test(x = norm_sample) non_norm_sample_shapiro_res &lt;- shapiro.test(x = non_norm_sample) norm_sample_shapiro_res Shapiro-Wilk normality test data: norm_sample W = 0.9977, p-value = 0.9999 non_norm_sample_shapiro_res Shapiro-Wilk normality test data: non_norm_sample W = 0.91693, p-value = 9.714e-06 Running the function is very simple, the only input you have to give is the numeric vector. What we can see in the results? Data: which data we have evaluated W: a measure of how well the ordered and standardized sample quantiles fit the standard normal quantiles. The statistic will take a value between 0 and 1 with 1 being a perfect match. p.value: p value associated to the test. If greater than the threshold, we cannot reject the null hypothesis So, we can say that the Shapiro-Wilk test confirms that the first sample is normally distributed, while the second one not. Kolmogorov-Smirnov test Let‚Äôs now see the Kolgomorov-Smirnov test. The function is ks.test, and it differs a lot from the previous one, so let‚Äôs see some examples and then comment them: # 1. Test normal data norm_sample_ks_res &lt;- ks.test(x = norm_sample, &quot;pnorm&quot;) # 2. Test non-normal data non_norm_sample_ks_res &lt;- ks.test(x = non_norm_sample, &quot;pnorm&quot;) # 3. Test if the two belongs to the same distributions comparison_ks_res &lt;- ks.test(x = norm_sample, y = non_norm_sample) norm_sample_ks_res One-sample Kolmogorov-Smirnov test data: norm_sample D = 1, p-value &lt; 2.2e-16 alternative hypothesis: two-sided non_norm_sample_ks_res One-sample Kolmogorov-Smirnov test data: non_norm_sample D = 0.74044, p-value &lt; 2.2e-16 alternative hypothesis: two-sided comparison_ks_res Two-sample Kolmogorov-Smirnov test data: norm_sample and non_norm_sample D = 0.63, p-value &lt; 2.2e-16 alternative hypothesis: two-sided ‚ÄúWait, what has happened? Why they are all not normal?! The first one should have a p-value &gt; 0.05‚Äù. You are perfectly right, that is because we didn‚Äôt specify anything about the normal distribution, just ‚Äúpnorm‚Äù, and it assumes that you want to compare your data with the standard normal distribution (mean = 0, sd = 1). To fix it, we can provide to the function, the info about mean and sd of the distribution we want (usually, the mean and the sd of our data). So, we can do this: # 1. Test normal data norm_sample_ks_res &lt;- ks.test(x = norm_sample, &quot;pnorm&quot;, mean = mean(norm_sample), sd = sd(norm_sample)) # 2. Test non-normal data non_norm_sample_ks_res &lt;- ks.test(x = non_norm_sample, &quot;pnorm&quot;, mean = mean(norm_sample), sd = sd(norm_sample)) # 3. Test if the two belongs to the same distributions comparison_ks_res &lt;- ks.test(x = norm_sample, y = non_norm_sample) norm_sample_ks_res One-sample Kolmogorov-Smirnov test data: norm_sample D = 0.010407, p-value = 1 alternative hypothesis: two-sided non_norm_sample_ks_res One-sample Kolmogorov-Smirnov test data: non_norm_sample D = 0.63413, p-value &lt; 2.2e-16 alternative hypothesis: two-sided comparison_ks_res Two-sample Kolmogorov-Smirnov test data: norm_sample and non_norm_sample D = 0.63, p-value &lt; 2.2e-16 alternative hypothesis: two-sided üéâ This is exactly what we were expecting! You can choose whichever test you want. I personally prefer Shapiro-Wilk, but it is up to you. We can now evaluate our example samples with outlier excluded; I will show you now how to perform both test and get only the pvalues to create a new data.frame: normality_df &lt;- male_30_weight_task3 %&gt;% group_by(sex, age, treatment) %&gt;% summarise(&quot;shapiro_p_value&quot; = shapiro.test(weight)$p.value, &quot;ks_p_value&quot; = ks.test(weight, &quot;pnorm&quot;, mean = mean(weight, na.rm = T), sd = sd(weight, na.rm = T))$p.value, &quot;shapiro_p_value_Task3&quot; = shapiro.test(Task3)$p.value, &quot;ks_p_value_Task3&quot; = ks.test(Task3, &quot;pnorm&quot;, mean = mean(Task3), sd = sd(Task3))$p.value) normality_df # A tibble: 2 √ó 7 # Groups: sex, age [1] sex age treatment shapiro_p_value ks_p_value shapiro_p_value_Task3 ks_p_value_Task3 &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Male 30 Untreated 0.389 0.938 0.0957 0.374 2 Male 30 T1 0.0649 0.696 0.0474 0.590 Results are in contrast‚Ä¶ This is a reason why I prefer Shapiro-Wilk test, whose results are more in line with what we can see by visual inspection: both weight distributions are normal, while T1-treated samples have not a normal distribution of Task3 values. Homogeneity of variances (homoschedaticity) Finally, we can evaluate the homogeneity of variances between samples. We will use the Bartlett test, with the function bartlett.test. It needs two main inputs: a value vector and a vector of groups into which divide the values. Here, a significant p-value means that the variances are NOT homogeneous. Here is an example with our data: # 1. Perform test on weight weight_bartlett_result &lt;- bartlett.test(male_30_weight_task3$weight, male_30_weight_task3$treatment) # 2. Perform test on Task3 Task3_bartlett_result &lt;- bartlett.test(male_30_weight_task3$Task3, male_30_weight_task3$treatment) weight_bartlett_result Bartlett test of homogeneity of variances data: male_30_weight_task3$weight and male_30_weight_task3$treatment Bartlett&#39;s K-squared = 4.7979, df = 1, p-value = 0.02849 Task3_bartlett_result Bartlett test of homogeneity of variances data: male_30_weight_task3$Task3 and male_30_weight_task3$treatment Bartlett&#39;s K-squared = 32.09, df = 1, p-value = 1.472e-08 From these results, we can say that the variances are not homogeneous. As an extra exercise, let‚Äôs add these info to the plots: # 1. Get labels position weight_max_label &lt;- max(male_30_weight_task3$weight, na.rm = T) + 1 Task3_max_label &lt;- max(male_30_weight_task3$Task3, na.rm = T) + 50 # 1. Create boxplots male_30_weight_boxplot_filt &lt;- male_30_weight_boxplot_filt + geom_text(data = normality_df, mapping = aes(x = treatment, y = weight_max_label, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shapiro_p_value, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(weight_bartlett_result$p.value, 3))) male_30_task3_boxplot_filt &lt;- male_30_task3_boxplot_filt + geom_text(data = normality_df, mapping = aes(x = treatment, y = Task3_max_label, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shapiro_p_value_Task3, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(weight_bartlett_result$p.value, 3))) ggarrange(male_30_weight_boxplot_filt, male_30_weight_density_filt, male_30_weight_qq_filt, male_30_task3_boxplot_filt, male_30_task3_density_filt, male_30_task3_qq_filt, ncol = 3, nrow = 2, labels = &quot;AUTO&quot;, align = &quot;hv&quot;) Alright, in this chapter we have seen just an example on how to evaluate whether to use or not parametric tests in further analyses. Another important take home message of this chapter is: choose the right test for your analysis and planning and experimental design are fundamental steps. From next chapter on, we will see in details the statistical tests we can perform. "],["association-between-categorical-variables.html", "16 Association between categorical variables Fisher exact test Chi-Squared test with Yates correction Chi-Squared test without correction", " 16 Association between categorical variables We‚Äôll start our little journey on statistical tests in R with the ones used to evaluate the association between categorical variables. There are two tests used to test if two categorical variables are independent or not: Fisher‚Äôs exact test and Chi-Squared test. They interpretation of the p-value of these tests is the same, as in both cases a p-value &lt; threshold (e.g.¬†0.05) means that we reject the null hypothesis that the two variables are independent. The first one (Fisher‚Äôs) is used only when we have a small sample size 2x2 contingency matrix (so when the two categorical variables have 2 categories each) and at least one expected value is &lt; 5. In the same scenario, but with all expected values &gt; 5, Chi-Squared test with Yates correction is used, while in all other scenarios it is used Chi-Squared test without correction. Here, we will see some examples on when to use these tests. Let‚Äôs load the required libraries and our data first: # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(ggplot2)) suppressPackageStartupMessages(library(ggmosaic)) suppressPackageStartupMessages(library(vcd)) # 2. Load data df &lt;- read.csv(&quot;data/Stat-test-dataset.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;sex&quot; = factor(sex), &quot;treatment&quot; = factor(treatment, levels = c(&quot;T1&quot;, &quot;Untreated&quot;)), &quot;Task1&quot; = factor(Task1, levels = c(1, 0)), &quot;Task2&quot; = factor(Task2, levels = c(1, 0)), ) str(df) &#39;data.frame&#39;: 600 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 2 1 2 1 2 1 2 1 ... $ age : int 3 3 3 3 3 3 3 3 3 3 ... $ treatment: Factor w/ 2 levels &quot;T1&quot;,&quot;Untreated&quot;: 2 2 1 1 2 2 1 1 2 2 ... $ weight : num 6.43 3.42 4.97 4.63 7.31 3.27 5.58 3.27 5.9 4.22 ... $ Task1 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 2 1 1 1 1 2 1 ... $ Task2 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 1 1 2 2 1 1 1 ... $ Task3 : num 222.4 202.3 36.7 221.8 178.8 ... This time, I also loaded the ggmosaic package, we will use it to create mosaic plots of our data. You can install it through install.packages(\"ggmosaic\"). Now that we have our data loaded, here are the comparisons we want to do: In Male at age 3 that have successfully done Task2, is there an association between the treatment and the success on Task1? In Female at age 15 that have successfully done Task1, is there an association between the treatment and the success on Task2? In Untreated mice, is there any association between the age and the success on Task1? Now, for each comparison, we have to decide which test we want to use. To evaluate sample size and number of categories we will use table: # 1. Filter data male_3_task2yes_df &lt;- df %&gt;% filter(sex == &quot;Male&quot;, age == 3, Task2 == 1) female_15_task1yes_df &lt;- df %&gt;% filter(sex == &quot;Female&quot;, age == 15, Task1 == 1) untreated_df &lt;- df %&gt;% filter(treatment == &quot;Untreated&quot;) # 2. Create contingency tables male_3_task2yes_table &lt;- table(male_3_task2yes_df$treatment, male_3_task2yes_df$Task1) female_15_task1yes_table &lt;- table(female_15_task1yes_df$treatment, female_15_task1yes_df$Task2) untreated_table &lt;- table(untreated_df$age, untreated_df$Task1) # 3. Get sample size male_3_task2yes_sample_size &lt;- sum(male_3_task2yes_table) female_15_task1yes_sample_size &lt;- sum(female_15_task1yes_table) untreated_sample_size &lt;- sum(untreated_table) # 4. Get max number of categories male_3_task2yes_max_cat &lt;- max(dim(male_3_task2yes_table)) female_15_task1yes_max_cat &lt;- max(dim(female_15_task1yes_table)) untreated_max_cat &lt;- max(dim(untreated_table)) Now that we have all the info, let‚Äôs check them: In Male at age 3 that have successfully done Task2, is there an association between the treatment and the success on Task1? print(male_3_task2yes_table) 1 0 T1 4 13 Untreated 5 13 cat(&quot;Sample size:&quot;, male_3_task2yes_sample_size, &quot;\\nMax number of categories:&quot;, male_3_task2yes_max_cat, &quot;\\n&quot;) Sample size: 35 Max number of categories: 2 What can we say? First of all, it is always recommended to look at the contingency table as starting point Sample size is low, it is a 2x2 contingency matrix. So we have to look at the expected values to decide whether to use Fisher‚Äôs exact test or Chi-Squared test with Yates correction To do so, we will use chisq.test function, extrapolating the expected values from the results. I know, it is ‚Äúsilly‚Äù that we have to actually perform chi-squared test to get the expected values‚Ä¶ male_3_task2yes_expected &lt;- chisq.test(male_3_task2yes_table)$expected cat(&quot;Minimum expected value is:&quot;, min(male_3_task2yes_expected)) Minimum expected value is: 4.371429 As it is &lt; 5, we will use Fisher‚Äôs exact test in this scenario. In Female at age 15 that have successfully done Task1, is there an association between the treatment and the success on Task2? print(female_15_task1yes_table) 1 0 T1 26 3 Untreated 7 14 cat(&quot;Sample size:&quot;, female_15_task1yes_sample_size, &quot;\\nMax number of categories:&quot;, female_15_task1yes_max_cat, &quot;\\n&quot;) Sample size: 50 Max number of categories: 2 What can we say? Sample size is low, it is a 2x2 contingency matrix. So we have to look at the expected values to decide whether to use Fisher‚Äôs exact test or Chi-Squared test with Yates correction female_15_task1yes_expected &lt;- chisq.test(female_15_task1yes_table)$expected cat(&quot;Minimum expected value is:&quot;, min(female_15_task1yes_expected)) Minimum expected value is: 7.14 As it is &gt; 5, we will use Chi-Squared test with Yates correction in this scenario. In Untreated mice, is there any association between the age and the success on Task1? print(untreated_table) 1 0 3 20 56 7 39 35 15 42 34 30 69 5 cat(&quot;Sample size:&quot;, untreated_sample_size, &quot;\\nMax number of categories:&quot;, untreated_max_cat, &quot;\\n&quot;) Sample size: 300 Max number of categories: 4 What can we say? Sample size is low, it is a 4x2 contingency matrix. So we have to use Chi-Squared test Fisher exact test Now that we have decided which test to use, let‚Äôs start evaluating if there is an association between the treatment and the success on Task1 in Male at age 3 that have successfully done Task2. We already have the contingency table, and we know we should use the Fisher‚Äôs exact test; but, let‚Äôs do a mosaic plot that can be the then updated with the statistics and can be used a figure to show the results: male_3_task2yes_mosaic &lt;- ggplot(male_3_task2yes_df) + geom_mosaic(aes(x = product(treatment), fill = Task1), alpha = 1) + scale_y_discrete(expand = expansion(mult = c(0,0))) + labs(y = &quot;&quot;, x = &quot;Treatment&quot;, title = &quot;Task1 success rate&quot;) + scale_fill_manual(values = c(&quot;1&quot; = &quot;#76B041&quot;, &quot;0&quot; = &quot;#D9481C&quot;)) + theme_classic() + theme(axis.line = element_blank(), plot.title = element_text(face = &quot;bold&quot;, hjust = 0.07, size = 14), axis.ticks.x = element_blank()) male_3_task2yes_mosaic Great, let‚Äôs have a look again at the contingency table, and then perform Fisher‚Äôs exact test: male_3_task2yes_table 1 0 T1 4 13 Untreated 5 13 male_3_task2yes_fisher_res &lt;- fisher.test(male_3_task2yes_table) male_3_task2yes_fisher_res Fisher&#39;s Exact Test for Count Data data: male_3_task2yes_table p-value = 1 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.1281067 4.7303300 sample estimates: odds ratio 0.8051144 How to interpret these data: odds ratio: it indicates the odds of success in Task1 of T1-treated mice compared to untreated, so it seems that the former perform worst than the latter in this task p-value: in this case, it indicates that the two variables are independent, so that there is no influence of treatment in the outcome of Task1 in Male at P3 that have successfully performed in Task2 Phi (œÜ) Coefficient The Phi (œÜ) Coefficient is used as a measure of association, indicating the strength and direction of association between two binary categorical variables in a 2x2 contingency table. It ranges from -1 to 1, with values closer to -1 or 1 indicating stronger associations. Our results have already indicated that there is no association of the two variables, but we will calculate it as it may be useful in other cases, where the Fisher‚Äôs exact test is significant. To do so, we will use the assocstats function from package vcd (this function returns multiple scores, we will take only phi): male_3_task2yes_fisher_phi &lt;- assocstats(male_3_task2yes_table)$phi male_3_task2yes_fisher_phi [1] 0.04858192 As expected, Phi Coefficient is very low. Let‚Äôs update our plot with these info: n_untreated &lt;- sum(male_3_task2yes_df$treatment == &quot;Untreated&quot;) n_t1 &lt;- sum(male_3_task2yes_df$treatment == &quot;T1&quot;) pvalue &lt;- case_when(male_3_task2yes_fisher_res$p.value &gt; 0.05 ~ as.character(round(male_3_task2yes_fisher_res$p.value, 3)), male_3_task2yes_fisher_res$p.value &lt; 0.0001 ~ &quot;&lt; 0.0001&quot;, male_3_task2yes_fisher_res$p.value &lt; 0.001 ~ &quot;&lt; 0.001&quot;, male_3_task2yes_fisher_res$p.value &lt; 0.01 ~ &quot;&lt; 0.01&quot;, male_3_task2yes_fisher_res$p.value &lt; 0.05 ~ &quot;&lt; 0.05&quot;, ) caption &lt;- paste0(&quot;Task1 success rate (1: success; 0: fail) of Male P3 mice who have successfully performed in Task2 (n = &quot;, male_3_task2yes_sample_size, &quot;), in the two treatment conditions (Untreated n = &quot;, n_t1, &quot;, T1-treated n = &quot;, n_untreated, &quot;). Statistics calculated through Fisher&#39;s exact test.&quot;) male_3_task2yes_mosaic &lt;- male_3_task2yes_mosaic + labs(subtitle = paste0(&quot;œÜ: &quot;, round(male_3_task2yes_fisher_phi, 3), &quot;, OR: &quot;, round(male_3_task2yes_fisher_res$estimate, 3), &quot;, p-value: &quot;, pvalue), caption = str_wrap(caption, width = 125)) + theme(plot.subtitle = element_text(size = 10, hjust = 0.07), plot.caption = element_text(hjust = 0, debug = F, margin = margin(t= 20)), plot.caption.position = &quot;plot&quot;) male_3_task2yes_mosaic Great! We have a nice figure to present! Chi-Squared test with Yates correction Now, let‚Äôs evaluate the scenario that has to be be assessed through Chi-Squared test with Yates correction. As in Fisher‚Äôs exact test example, we‚Äôll make the mosaic plot first: female_15_task1yes_mosaic &lt;- ggplot(female_15_task1yes_df) + geom_mosaic(aes(x = product(treatment), fill = Task2), alpha = 1) + scale_y_discrete(expand = expansion(mult = c(0,0))) + labs(y = &quot;&quot;, x = &quot;Treatment&quot;, title = &quot;Task2 success rate&quot;) + scale_fill_manual(values = c(&quot;1&quot; = &quot;#76B041&quot;, &quot;0&quot; = &quot;#D9481C&quot;)) + theme_classic() + theme(axis.line = element_blank(), plot.title = element_text(face = &quot;bold&quot;, hjust = 0.07, size = 14), axis.ticks.x = element_blank()) female_15_task1yes_mosaic Wow, here the difference is striking. Let‚Äôs see the contingency matrix and perform the test: female_15_task1yes_table 1 0 T1 26 3 Untreated 7 14 female_15_task1yes_chisq_res &lt;- chisq.test(female_15_task1yes_table, correct = T) female_15_task1yes_chisq_res Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: female_15_task1yes_table X-squared = 14.799, df = 1, p-value = 0.0001196 Compared to Fisher‚Äôs exact test, here we look just at p-value as odds-ratio are not calculated by default (this is because we can calculate OR only in 2x2 contingency matrix, and chisq.test can accept greater matrices). In this case, we can say that the outcome of Task1 can be due to treatment (remember: correlation or absence of independence is not causation!). As this is a 2x2 contingency matrix and the odds ratio, we can use Phi Coefficient to measure the association between the two variables: female_15_task1yes_phi &lt;- assocstats(female_15_task1yes_table)$phi female_15_task1yes_phi [1] 0.5868188 There is quite a good positive association! female_15_task1yes_or &lt;- (female_15_task1yes_table[1, 1] * female_15_task1yes_table[2, 2]) / (female_15_task1yes_table[1, 2] * female_15_task1yes_table[2, 1]) female_15_task1yes_or [1] 17.33333 Wow! The OR is huge! This has to be inserted in the plot: n_untreated &lt;- sum(female_15_task1yes_df$treatment == &quot;Untreated&quot;) n_t1 &lt;- sum(female_15_task1yes_df$treatment == &quot;T1&quot;) pvalue &lt;- case_when(female_15_task1yes_chisq_res$p.value &gt; 0.05 ~ as.character(round(female_15_task1yes_chisq_res$p.value, 3)), female_15_task1yes_chisq_res$p.value &lt; 0.0001 ~ &quot;&lt; 0.0001&quot;, female_15_task1yes_chisq_res$p.value &lt; 0.001 ~ &quot;&lt; 0.001&quot;, female_15_task1yes_chisq_res$p.value &lt; 0.01 ~ &quot;&lt; 0.01&quot;, female_15_task1yes_chisq_res$p.value &lt; 0.05 ~ &quot;&lt; 0.05&quot;, ) caption &lt;- paste0(&quot;Task2 success rate (1: success; 0: fail) of Female P15 mice who have successfully performed in Task1 (n = &quot;, female_15_task1yes_sample_size, &quot;), in the two treatment conditions (Untreated n = &quot;, n_t1, &quot;, T1-treated n = &quot;, n_untreated, &quot;). Statistics calculated through Fisher&#39;s exact test.&quot;) female_15_task1yes_mosaic &lt;- female_15_task1yes_mosaic + labs(subtitle = paste0(&quot;œÜ: &quot;, round(female_15_task1yes_phi, 3), &quot;, OR: &quot;, round(female_15_task1yes_or, 3), &quot;, p-value: &quot;, pvalue), caption = str_wrap(caption, width = 125)) + theme(plot.subtitle = element_text(size = 10, hjust = 0.07), plot.caption = element_text(hjust = 0, debug = F, margin = margin(t= 20)), plot.caption.position = &quot;plot&quot;) female_15_task1yes_mosaic That‚Äôs another cool picture to show at your PI! Chi-Squared test without correction Finally, let‚Äôs see if there is any association between the age and the success on Task1 in untreated mice through Chi-Squared test. As always, here is the mosaic plot: untreated_mosaic &lt;- ggplot(untreated_df) + geom_mosaic(aes(x = product(age), fill = Task1), alpha = 1) + scale_y_discrete(expand = expansion(mult = c(0,0))) + labs(y = &quot;&quot;, x = &quot;Age&quot;, title = &quot;Task1 success rate&quot;) + scale_fill_manual(values = c(&quot;1&quot; = &quot;#76B041&quot;, &quot;0&quot; = &quot;#D9481C&quot;)) + theme_classic() + theme(axis.line = element_blank(), plot.title = element_text(face = &quot;bold&quot;, hjust = 0.07, size = 14), axis.ticks.x = element_blank()) untreated_mosaic It seems that the success rate in Task1 increases over time in untreated mice. Let‚Äôs see the contingency matrix and perform the test: untreated_table 1 0 3 20 56 7 39 35 15 42 34 30 69 5 untreated_chisq_res &lt;- chisq.test(untreated_table, correct = T) untreated_chisq_res Pearson&#39;s Chi-squared test data: untreated_table X-squared = 69.362, df = 3, p-value = 5.846e-15 From this test, it is evident that the association between age and success in Task1 is significant. Let‚Äôs quantify it. Cramer‚Äôs V For contingency matrix that are bigger than 2x2 we cannot use Phi Coefficient to calculate the association, but we have to use Cramer‚Äôs V. The possible values are the same as Phi, and also the interpretation. To calculate it, the function is the same as for Phi, but here we take the value of Cramer‚Äôs V. untreated_cramer &lt;- assocstats(untreated_table)$cramer untreated_cramer [1] 0.4808398 There is quite a good positive association also here! Let‚Äôs update our plot: n_3 &lt;- sum(untreated_df$age == 3) n_7 &lt;- sum(untreated_df$age == 7) n_15 &lt;- sum(untreated_df$age == 15) n_30 &lt;- sum(untreated_df$age == 30) pvalue &lt;- case_when(untreated_chisq_res$p.value &gt; 0.05 ~ as.character(round(untreated_chisq_res$p.value, 3)), untreated_chisq_res$p.value &lt; 0.0001 ~ &quot;&lt; 0.0001&quot;, untreated_chisq_res$p.value &lt; 0.001 ~ &quot;&lt; 0.001&quot;, untreated_chisq_res$p.value &lt; 0.01 ~ &quot;&lt; 0.01&quot;, untreated_chisq_res$p.value &lt; 0.05 ~ &quot;&lt; 0.05&quot;, ) caption &lt;- paste0(&quot;Task1 success rate (1: success; 0: fail) of untreated mice (n = &quot;, untreated_sample_size, &quot;), over time (P3 n = &quot;, n_3, &quot;, P7 n = &quot;, n_7, &quot;, P15 n = &quot;, n_15, &quot;, P30 n = &quot;, n_30, &quot;). Statistics calculated through Fisher&#39;s exact test.&quot;) untreated_mosaic &lt;- untreated_mosaic + labs(subtitle = paste0(&quot;Cramer&#39;s V: &quot;, round(untreated_cramer, 3), &quot;, p-value: &quot;, pvalue), caption = str_wrap(caption, width = 125)) + theme(plot.subtitle = element_text(size = 10, hjust = 0.07), plot.caption = element_text(hjust = 0, debug = F, margin = margin(t= 20)), plot.caption.position = &quot;plot&quot;) untreated_mosaic Amazing, we have ansewred all the questions through these 3 test. Now you are able to use them in your personal analysis! "],["comparing-two-groups.html", "17 Comparing two groups T-test Welch‚Äôs test Mann-Whitney and Wilcoxon Add values to graph", " 17 Comparing two groups Another chapter, another test(s). Here, we will see how to compare two groups with continuous values: t test and Welch‚Äôs t-test (parametric data) and Wilkoxon and Mann-Whitney (non-parametric data). We have already seen how to decide whether to use parametric and non-parametric tests (if you don‚Äôt remember, go and check the Intro to statistics chapter), so let‚Äôs start by loading our data and decide what we want to compare. # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(ggplot2)) suppressPackageStartupMessages(library(ggpubr)) suppressPackageStartupMessages(library(gginnards)) suppressPackageStartupMessages(library(glue)) # 2. Load data df &lt;- read.csv(&quot;data/Stat-test-dataset.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;sex&quot; = factor(sex), &quot;treatment&quot; = factor(treatment, levels = c(&quot;T1&quot;, &quot;Untreated&quot;)), &quot;Task1&quot; = factor(Task1, levels = c(1, 0)), &quot;Task2&quot; = factor(Task2, levels = c(1, 0)), ) str(df) &#39;data.frame&#39;: 600 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 2 1 2 1 2 1 2 1 ... $ age : int 3 3 3 3 3 3 3 3 3 3 ... $ treatment: Factor w/ 2 levels &quot;T1&quot;,&quot;Untreated&quot;: 2 2 1 1 2 2 1 1 2 2 ... $ weight : num 6.43 3.42 4.97 4.63 7.31 3.27 5.58 3.27 5.9 4.22 ... $ Task1 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 2 1 1 1 1 2 1 ... $ Task2 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 1 1 2 2 1 1 1 ... $ Task3 : num 222.4 202.3 36.7 221.8 178.8 ... Now that we have our data loaded, there are the question we want to address: Do untreated females at P30 weight more than the ones at P15? At P3, is there a difference in weight in T1-treated mice between males and females? Do P30 males behave differently in Task3 based on treatment? Let‚Äôs now decide which test to use for each question. We can start by looking at the boxplots and then check parametric assumptions (it‚Äôs important to do so!). But first, we have to subset our dataframe: # 1. Filter data female_t1_weight_df &lt;- df %&gt;% filter(age %in% c(15, 30) &amp; sex == &quot;Female&quot; &amp; treatment == &quot;T1&quot;) %&gt;% select(sex, age, treatment, weight) %&gt;% mutate(age = factor(age)) t1_3_weight_df &lt;- df %&gt;% filter(age == 3 &amp; treatment == &quot;T1&quot;) %&gt;% select(sex, age, treatment, weight) male_3_task3_df &lt;- df %&gt;% filter(age == 3 &amp; sex == &quot;Male&quot;) %&gt;% select(sex, age, treatment, Task3) Then, we will look at the boxplot: female_t1_weight_boxplot &lt;- ggplot(female_t1_weight_df) + geom_boxplot(aes(x = age, y = weight, fill = age, group = age)) + labs(x = &quot;Age&quot;, y = &quot;Weight (g)&quot;, title = &quot;T1-treated females weight&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16)) t1_3_weight_boxplot &lt;- ggplot(t1_3_weight_df) + geom_boxplot(aes(x = sex, y = weight, fill = sex, group = sex)) + labs(x = &quot;Sex&quot;, y = &quot;Weight (g)&quot;, title = &quot;T1-treated P3 weight&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16)) male_3_task3_boxplot &lt;- ggplot(male_3_task3_df) + geom_boxplot(aes(x = treatment, y = Task3, fill = treatment, group = treatment)) + labs(x = &quot;Treatment&quot;, y = &quot;Task3 (s)&quot;, title = &quot;Male P3 Task3&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16)) ggarrange(female_t1_weight_boxplot, t1_3_weight_boxplot, male_3_task3_boxplot, ncol = 3, nrow = 1, labels = &quot;AUTO&quot;, align = &quot;h&quot;) Great, we can see some differences. And yes, there are some outliers in B., but we do not want to remove them now, you can try it if you want (it can be a useful exercise). We now check the normality and homoschedasticity for each group, and then insert those info in the plot: # 1. Shapiro test female_t1_weight_shapiro &lt;- female_t1_weight_df %&gt;% group_by(age) %&gt;% summarise(shap = shapiro.test(weight)$p.value) t1_3_weight_shapiro &lt;- t1_3_weight_df %&gt;% group_by(sex) %&gt;% summarise(shap = shapiro.test(weight)$p.value) male_3_task3_shapiro &lt;- male_3_task3_df %&gt;% group_by(treatment) %&gt;% summarise(shap = shapiro.test(Task3)$p.value) # 2. Bartlett test female_t1_weight_bartlett &lt;- bartlett.test(female_t1_weight_df$weight, female_t1_weight_df$age)$p.value t1_3_weight_bartlett &lt;- bartlett.test(t1_3_weight_df$weight, t1_3_weight_df$sex)$p.value male_3_task3_bartlett &lt;- bartlett.test(male_3_task3_df$Task3, male_3_task3_df$treatment)$p.value # 3. Get labels position female_t1_weight_max &lt;- max(female_t1_weight_df$weight, na.rm = T) + 1 t1_3_weight_max &lt;- max(t1_3_weight_df$weight, na.rm = T) + 1 male_3_task3_max &lt;- max(male_3_task3_df$Task3, na.rm = T) + 20 # 1. Create boxplots female_t1_weight_boxplot &lt;- female_t1_weight_boxplot + geom_text(data = female_t1_weight_shapiro, mapping = aes(x = age, y = female_t1_weight_max, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shap, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(female_t1_weight_bartlett, 3))) t1_3_weight_boxplot &lt;- t1_3_weight_boxplot + geom_text(data = t1_3_weight_shapiro, mapping = aes(x = sex, y = t1_3_weight_max, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shap, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(t1_3_weight_bartlett, 3))) male_3_task3_boxplot &lt;- male_3_task3_boxplot + geom_text(data = male_3_task3_shapiro, mapping = aes(x = treatment, y = male_3_task3_max, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shap, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(male_3_task3_bartlett, 3))) ggarrange(female_t1_weight_boxplot, t1_3_weight_boxplot, male_3_task3_boxplot, ncol = 3, nrow = 1, labels = &quot;AUTO&quot;, align = &quot;h&quot;) Amazing! We have to use simple t-test in A, Welch‚Äôs t-test in B and Mann-Whitney test in C. Let‚Äôs see how to perform these tests in R. T-test T-test is one of the most famous test and the one that most of people try to apply every time, even if it‚Äôs not possible (sigh, but after this course, I know you won‚Äôt be one of them). You can use the t-test to compare: The mean of a sample and the expected mean of the population The difference of the mean values of two dependent populations (paired t-test) The difference of the mean values of two independent populations (unpaired t-test) In our case, we want to evaluate whether the untreated females at P30 weight more than the ones at P15. As the samples are independent, we will use the unpaired t-test, but don‚Äôt worry, I will explain you all the possible scenarios and how to write the proper code. The function to use is t.test(), and there are plenty of cool arguments that we can use to apply the right test in the right conditions. Let‚Äôs look how we apply it in our case, and then explain each bit of code: t_test_res &lt;- t.test(formula = female_t1_weight_df$weight ~ female_t1_weight_df$age, alternative = &quot;less&quot;, mu = 0, paired = F, var.equal = T) Wow, lots of things to explain: formula: you can provide a formula like continuous ~ categorical. The categorical varibale should have maximum 2 levels, and the comparison is made as 1st level vs.¬†2nd level (in our case, P15 over P30). This is not the only way you can give data to this function, you can also just provide two continuous vectors separated by a comma, without calling formula. alternative: it indicates which is the alternative hypothesis (‚Äútwo.sided‚Äù default, ‚Äúless‚Äù or ‚Äúmore‚Äù). As we wanted to know if P30 females weight more that P15, and that the comparison was made P15 over P30, we used ‚Äúless‚Äù. mu: is the value of the difference we want to test (if two samples) or the mean (if one sample). We choose 0 in this case, but we could have written -5 if we wanted to know if P30 females weight 5g more than P15 ones. Pay attention to the sign! paired: TRUE (paired t-test) or FALSE (unpaired t-test, default). Remember that in paired t-test, the two samples must be of the same size var.equal: TRUE (student t-test) or FALSE (Welch‚Äôs t-test, default). We wanted to use the student t-test as we tested that the variances of the two groups are the same. And now, let‚Äôs see the results. Remember to store the results in a variable, so we can get all the values that we needed for further analysis/plots. t_test_res Two Sample t-test data: female_t1_weight_df$weight by female_t1_weight_df$age t = -47.074, df = 73, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means between group 15 and group 30 is less than 0 95 percent confidence interval: -Inf -7.279732 sample estimates: mean in group 15 mean in group 30 9.310811 16.857632 We have a lot of information. I think that there is not much to say, as it is kind of self-explanatory, with type of test, p-value, confidence interval, alternative hypothesis and mean values. We can confirm that this is significant (as expected, otherwise females are not growing and that‚Äôs a problem). Welch‚Äôs test To address the second question, we have to use Welch‚Äôs t-test, as the two samples have normal distributions, but different variances. We have just seen how to perform Welch‚Äôs t-test just by setting var.equal = FALSE in t.test() function. So, we talks are over, here is the answer to our question: # 1. Perform the test welch_res &lt;- t.test(formula = t1_3_weight_df$weight ~ t1_3_weight_df$sex, alternative = &quot;two.sided&quot;, mu = 0, paired = F, var.equal = F) # 2. Show the results welch_res Welch Two Sample t-test data: t1_3_weight_df$weight by t1_3_weight_df$sex t = -7.9115, df = 61.568, p-value = 5.889e-11 alternative hypothesis: true difference in means between group Female and group Male is not equal to 0 95 percent confidence interval: -2.313433 -1.380081 sample estimates: mean in group Female mean in group Male 3.556757 5.403514 And yes! There is difference between males and females at P3, with males that weight significantly more than females. Mann-Whitney and Wilcoxon We will now address the last question through Mann-Whitney test, which is the non-parametric analogous of the unpaired t-test (we will not cover the Wilcoxon, which is the non-parametric analogous of the paired t-test, but you will see now hot to perform it as well). The function we will use is wilcox.test(). Yeah, I know it sounds weird, but that‚Äôs it. It takes the same arguments as the parametric counterpart, so we won‚Äôt see them again. To notice, if we set paired = T, we will perform the Wilcoxon test, otherwise we will perform Mann-Whitney. # 1. Perform the test mann_res &lt;- wilcox.test(formula = male_3_task3_df$Task3 ~ male_3_task3_df$treatment, alternative = &quot;two.sided&quot;, mu = 0, paired = F) # 2. Show the results mann_res Wilcoxon rank sum test with continuity correction data: male_3_task3_df$Task3 by male_3_task3_df$treatment W = 189, p-value = 5.277e-08 alternative hypothesis: true location shift is not equal to 0 The output is slightly different, but we still have p-value and alternative hypothesis. Remember: with non-parametric tests, you are not comparing the mean directly. That‚Äôs why here are not indicated. Add values to graph We can now add the results of the statistical tests to the previous plot; but first, we will use gginnards package to removes the shapiro test values on the plot, because we want to put statistics there, moving shapiro results in the plot description. I will now write the whole code, you have already seen much of this stuff; the only difference is that I will use delete_layers() to remove Shapiro results: # 1. Remove shapiro results female_t1_weight_boxplot &lt;- delete_layers(female_t1_weight_boxplot, &quot;GeomText&quot;) t1_3_weight_boxplot &lt;- delete_layers(t1_3_weight_boxplot, &quot;GeomText&quot;) male_3_task3_boxplot &lt;- delete_layers(male_3_task3_boxplot, &quot;GeomText&quot;) # 2. Create a function to map *, **, *** and ns to statistics pvalue_to_plot &lt;- function(x) { res &lt;- case_when( x &lt;= 0.001 ~ &quot;***&quot;, x &lt;= 0.01 ~ &quot;**&quot;, x &lt;= 0.05 ~ &quot;*&quot;, .default = &quot;ns&quot; ) return(res) } # 3. Add pvalues of statistics and remove Bartlett results female_t1_weight_boxplot &lt;- female_t1_weight_boxplot + geom_segment(aes(x = 1, y = female_t1_weight_max, xend = 2, yend = female_t1_weight_max)) + geom_text(aes(x = 1.5, y = female_t1_weight_max + 0.5, label = pvalue_to_plot(t_test_res$p.value))) + labs(subtitle = &quot;&quot;) t1_3_weight_boxplot &lt;- t1_3_weight_boxplot + geom_segment(aes(x = 1, y = t1_3_weight_max, xend = 2, yend = t1_3_weight_max)) + geom_text(aes(x = 1.5, y = t1_3_weight_max + 0.5, label = pvalue_to_plot(welch_res$p.value))) + labs(subtitle = &quot;&quot;) male_3_task3_boxplot &lt;- male_3_task3_boxplot + geom_segment(aes(x = 1, y = male_3_task3_max, xend = 2, yend = male_3_task3_max)) + geom_text(aes(x = 1.5, y = male_3_task3_max + 5, label = pvalue_to_plot(mann_res$p.value))) + labs(subtitle = &quot;&quot;) # 4. Create one unique plot to annotate all_plots &lt;- ggarrange(female_t1_weight_boxplot, t1_3_weight_boxplot, male_3_task3_boxplot, ncol = 3, nrow = 1, labels = &quot;AUTO&quot;, align = &quot;h&quot;) # 5. Create the plot description description &lt;- glue(&quot;A. Weight difference between T1-treated females at P15 vs. P30 (n = {sum(female_t1_weight_df$age == 15)} and {sum(female_t1_weight_df$age == 30)} respectively, Shapiro-Wilk p-value = {round(female_t1_weight_shapiro$shap[female_t1_weight_shapiro$age == 15], 3)} and {round(female_t1_weight_shapiro$shap[female_t1_weight_shapiro$age == 30], 3)} respectively, Bartlett&#39;s test p-value = {round(female_t1_weight_bartlett, 3)}, t-test p-value = {t_test_res$p.value}). B. Weight difference between T1-treated males and females at P3 (n = {sum(t1_3_weight_df$sex == &#39;Male&#39;)} and {sum(t1_3_weight_df$sex == &#39;Female&#39;)} respectively, Shapiro-Wilk p-value = {round(t1_3_weight_shapiro$shap[t1_3_weight_shapiro$sex == &#39;Male&#39;], 3)} and {round(t1_3_weight_shapiro$shap[t1_3_weight_shapiro$sex == &#39;Male&#39;], 3)} respectively, Bartlett&#39;s test p-value = {round(t1_3_weight_bartlett, 3)}, Welch&#39;s t-test p-value = {welch_res$p.value}). C. Task3 time difference between T1-treated and untreated males at P3 (n = {sum(male_3_task3_df$treatment == &#39;T1&#39;)} and {sum(male_3_task3_df$treatment == &#39;Untreated&#39;)} respectively, Shapiro-Wilk p-value = {round(male_3_task3_shapiro$shap[male_3_task3_shapiro$treatment == &#39;T1&#39;], 3)} and {round(male_3_task3_shapiro$shap[male_3_task3_shapiro$treatment == &#39;Untreated&#39;], 3)} respectively, MAnn-Whitney p-value = {mann_res$p.value}).&quot;) # 6. Annotate plots all_plots &lt;- annotate_figure(all_plots, bottom = str_wrap(description, 135)) all_plots We can still improve our figure, but I think it sends a clear message. I think that this is all you need to know to start your analysis when you have two groups. In the next chapter, we will see how to deal with more than 2 groups. "],["comparing-more-groups.html", "18 Comparing more groups ANOVA Kruskal Wallis", " 18 Comparing more groups Last time, we have compared two groups to get if there were differences in their distributions. In this chapter, we will see how to compares more groups. As mentioned before, if we want to compare groups with a parametric test, we will use the ANOVA, otherwise we will use the Kruskal Wallis test; then, if the test results significant, we will apply post-hoc test to compare the different groups (Tukey‚Äôs or Pairwise t-test after ANOVA, or Dunn‚Äôs test after Kruskal Wallis test). Let‚Äôs start by loading our data and decide what we want to compare. # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(ggplot2)) suppressPackageStartupMessages(library(ggpubr)) suppressPackageStartupMessages(library(gginnards)) suppressPackageStartupMessages(library(glue)) options(dplyr.summarise.inform = FALSE) # 2. Load data df &lt;- read.csv(&quot;data/Stat-test-dataset.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;sex&quot; = factor(sex), &quot;treatment&quot; = factor(treatment, levels = c(&quot;T1&quot;, &quot;Untreated&quot;)), &quot;Task1&quot; = factor(Task1, levels = c(1, 0)), &quot;Task2&quot; = factor(Task2, levels = c(1, 0)), ) str(df) &#39;data.frame&#39;: 600 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 2 1 2 1 2 1 2 1 ... $ age : int 3 3 3 3 3 3 3 3 3 3 ... $ treatment: Factor w/ 2 levels &quot;T1&quot;,&quot;Untreated&quot;: 2 2 1 1 2 2 1 1 2 2 ... $ weight : num 6.43 3.42 4.97 4.63 7.31 3.27 5.58 3.27 5.9 4.22 ... $ Task1 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 2 1 1 1 1 2 1 ... $ Task2 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 1 1 2 2 1 1 1 ... $ Task3 : num 222.4 202.3 36.7 221.8 178.8 ... Now that we have our data loaded, there are the question we want to address: Do untreated males weight more than untreated females mice at all time points? Do T1-treated females Task3 completing time changes over time? Let‚Äôs now decide which test to use for each question. We can start by looking at the boxplots and then check parametric assumptions (it‚Äôs important to do so!). But first, we have to subset our dataframe: # 1. Filter data untreated_weight_df &lt;- df %&gt;% filter(treatment == &quot;Untreated&quot;) %&gt;% select(sex, age, treatment, weight) %&gt;% mutate(age = factor(age)) females_t1_task3_df &lt;- df %&gt;% filter(sex == &quot;Female&quot; &amp; treatment == &quot;T1&quot;) %&gt;% select(age, treatment, Task3) %&gt;% mutate(age = factor(age)) Then, we will look at the boxplot: untreated_weight_boxplot &lt;- ggplot(untreated_weight_df) + geom_boxplot(aes(x = sex, y = weight, fill = sex, group = sex)) + labs(x = &quot;&quot;, y = &quot;Weight (g)&quot;, title = &quot;Untreated mice weights&quot;) + facet_wrap(~ age, ncol = 4, scales = &quot;free_y&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16)) females_t1_task3_boxplot &lt;- ggplot(females_t1_task3_df) + geom_boxplot(aes(x = age, y = Task3, fill = age, group = age)) + labs(x = &quot;Age&quot;, y = &quot;Task3 (s)&quot;, title = &quot;T1-treated females Task3 completition time&quot;) + scale_fill_brewer(palette = &quot;PuBu&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16)) ggarrange(untreated_weight_boxplot, females_t1_task3_boxplot, ncol = 1, nrow = 2, labels = &quot;AUTO&quot;) Great, we can see some differences. And yes, there are some outliers in B., but we do not want to remove them now, as it is out of the scope of this chapter, but you could try. We now check the normality and homoschedasticity for each group, and then insert those info in the plot: # 1. Shapiro test untreated_weight_shapiro &lt;- untreated_weight_df %&gt;% group_by(age, sex) %&gt;% summarise(shap = shapiro.test(weight)$p.value) females_t1_task3_shapiro &lt;- females_t1_task3_df %&gt;% group_by(age) %&gt;% summarise(shap = shapiro.test(Task3)$p.value) # 2. Bartlett test untreated_weight_bartlett &lt;- bartlett.test(untreated_weight_df$weight, untreated_weight_df$age, untreated_weight_df$sex)$p.value females_t1_task3_bartlett &lt;- bartlett.test(females_t1_task3_df$Task3, females_t1_task3_df$age)$p.value # 3. Get labels position untreated_weight_shapiro_pos &lt;- untreated_weight_df %&gt;% group_by(age) %&gt;% summarise(max = max(weight) + 1) females_t1_task3_shapiro_pos &lt;- females_t1_task3_df %&gt;% group_by(age) %&gt;% summarise(max = max(Task3) + 10) untreated_weight_shapiro &lt;- untreated_weight_shapiro %&gt;% left_join(untreated_weight_shapiro_pos, by = &quot;age&quot;) females_t1_task3_shapiro &lt;- females_t1_task3_shapiro %&gt;% left_join(females_t1_task3_shapiro_pos, by = &quot;age&quot;) # 4. Update boxplots untreated_weight_boxplot &lt;- untreated_weight_boxplot + geom_text(data = untreated_weight_shapiro, mapping = aes(x = sex, y = max, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shap, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(untreated_weight_bartlett, 3))) females_t1_task3_boxplot &lt;- females_t1_task3_boxplot + geom_text(data = females_t1_task3_shapiro, mapping = aes(x = age, y = max, label = paste(&quot;Shapiro-Wilk\\n&quot;, round(shap, 3))) ) + labs(subtitle = paste(&quot;Bartlett test p-value:&quot;, round(females_t1_task3_bartlett, 3))) ggarrange(untreated_weight_boxplot, females_t1_task3_boxplot, ncol = 1, nrow = 2, labels = &quot;AUTO&quot;) Alright! We can use ANOVA to answer the first question, while we have to use Kruskal Wallis test for the second one. ANOVA So, to get if untreated males weight more than untreated females mice at all time points, we should perform a two-way ANOVA test, as we are interested in both sex and age, and in their interaction (to see if the effect of sex changes based on age). To perform this test, we will use the aov() function, which takes as inputs formula and data. An important note about the formula is how it is written (it will be the same also when we will see other tests): One-way ANOVA: dependent variable ~ independent variable Two-way ANOVA: here situation changes based on what you are interested on. If only on main effects of the two variables dependent variable ~ independent variable 1 + independent variable 2, if interestend only in the interaction dependent variable ~ independent variable 1:independent variable 2, if in both main effects and interaction dependent variable ~ independent variable 1*independent variable 2 Let‚Äôs do it: aov_res &lt;- aov(formula = weight ~ sex*age, data = untreated_weight_df) summary(aov_res) Df Sum Sq Mean Sq F value Pr(&gt;F) sex 1 293 293.4 367.986 &lt;2e-16 *** age 3 7874 2624.5 3291.977 &lt;2e-16 *** sex:age 3 2 0.8 1.004 0.391 Residuals 292 233 0.8 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To better see the results, it is useful to store them in a variable and call summary() out of them. In this way we have all the summary statistics for this test: degree of freedom for each variable (Df), sum of the square root of the variance (Sum Sq), mean square root (Mean Sq), F statistics value and the p-value (Pr(&gt;F)). From this, we can say that both sex and age main effects are significant, and that the effect of the sex does NOT change at different time point, so we are expecting differences at all age. Let‚Äôs test it with post-hoc tests. Post-hoc test As ANOVA test is significant for the variable we are interested in (sex), we will perform post-hoc tests. I said tests here just to show you how to perform them, but you can either perform Tukey‚Äôs test or pairwise t-test. We can start by Tukey‚Äôs test. It needs the object of the results of the ANOVA and, optionally, which term/s we are interested in (sex:age) for us. tukey_res &lt;- TukeyHSD(aov_res, which = &quot;sex:age&quot;) tukey_res Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = weight ~ sex * age, data = untreated_weight_df) $`sex:age` diff lwr upr p adj Male:3-Female:3 2.03526316 1.4098676 2.6606588 0 Female:7-Female:3 2.01284495 1.3832379 2.6424520 0 Male:7-Female:3 3.90879090 3.2791838 4.5383980 0 Female:15-Female:3 5.91421053 5.2888149 6.5396061 0 Male:15-Female:3 7.66368421 7.0382886 8.2890798 0 Female:30-Female:3 13.32798009 12.6983730 13.9575871 0 Male:30-Female:3 15.56311522 14.9335082 16.1927223 0 Female:7-Male:3 -0.02241821 -0.6520253 0.6071889 1 Male:7-Male:3 1.87352774 1.2439207 2.5031348 0 Female:15-Male:3 3.87894737 3.2535518 4.5043430 0 Male:15-Male:3 5.62842105 5.0030255 6.2538166 0 Female:30-Male:3 11.29271693 10.6631099 11.9223240 0 Male:30-Male:3 13.52785206 12.8982450 14.1574591 0 Male:7-Female:7 1.89594595 1.2621554 2.5297365 0 Female:15-Female:7 3.90136558 3.2717585 4.5309726 0 Male:15-Female:7 5.65083926 5.0212322 6.2804463 0 Female:30-Female:7 11.31513514 10.6813446 11.9489257 0 Male:30-Female:7 13.55027027 12.9164797 14.1840608 0 Female:15-Male:7 2.00541963 1.3758126 2.6350267 0 Male:15-Male:7 3.75489331 3.1252863 4.3845004 0 Female:30-Male:7 9.41918919 8.7853986 10.0529797 0 Male:30-Male:7 11.65432432 11.0205338 12.2881149 0 Male:15-Female:15 1.74947368 1.1240781 2.3748693 0 Female:30-Female:15 7.41376956 6.7841625 8.0433766 0 Male:30-Female:15 9.64890469 9.0192976 10.2785118 0 Female:30-Male:15 5.66429587 5.0346888 6.2939029 0 Male:30-Male:15 7.89943101 7.2698239 8.5290381 0 Male:30-Female:30 2.23513514 1.6013446 2.8689257 0 At all time points, female weight less than males. However, I don‚Äôt find this results so clear. In fact, I personally prefer to perform pairwise t test: # 1. Create an interaction factor of sex and age interaction_factor &lt;- interaction(untreated_weight_df$sex, untreated_weight_df$age) # 2. Perform pairwise t-tests pairwise_res &lt;- pairwise.t.test(untreated_weight_df$weight, interaction_factor, p.adjust.method = &quot;bonferroni&quot;) pairwise_res Pairwise comparisons using t tests with pooled SD data: untreated_weight_df$weight and interaction_factor Female.3 Male.3 Female.7 Male.7 Female.15 Male.15 Female.30 Male.3 &lt; 2e-16 - - - - - - Female.7 &lt; 2e-16 1 - - - - - Male.7 &lt; 2e-16 4.5e-16 3.2e-16 - - - - Female.15 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 - - - Male.15 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 2.1e-14 - - Female.30 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 - Male.30 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 P value adjustment method: bonferroni Tadaaa! I find this table way clearer. Let‚Äôs now create a beautiful figure. In this case, as it is a timeline, I like to create some lineplots with mean and sd. # 1. Create a function to map *, **, *** and ns to statistics pvalue_to_plot &lt;- function(x) { res &lt;- case_when( x &lt;= 0.001 ~ &quot;***&quot;, x &lt;= 0.01 ~ &quot;**&quot;, x &lt;= 0.05 ~ &quot;*&quot;, .default = &quot;ns&quot; ) return(res) } # 2. Create df of statistics untreated_weight_stats_pos &lt;- untreated_weight_shapiro_pos %&gt;% mutate(&quot;stats&quot; = pvalue_to_plot(c(pairwise_res$p.value[&quot;Male.3&quot;, &quot;Female.3&quot;], pairwise_res$p.value[&quot;Male.7&quot;, &quot;Female.7&quot;], pairwise_res$p.value[&quot;Male.15&quot;, &quot;Female.15&quot;], pairwise_res$p.value[&quot;Male.30&quot;, &quot;Female.30&quot;]))) # 3. Create the plot untreated_weight_lineplot &lt;- ggplot(untreated_weight_df) + stat_summary(aes(x = age, y = weight, col = sex, group = sex), fun = &quot;mean&quot;, geom = &quot;line&quot;) + stat_summary(aes(x = age, y = weight, col = sex, group = sex), fun.data = &quot;mean_sd&quot;, geom = &quot;errorbar&quot;, width = 0.1) + stat_summary(aes(x = age, y = weight, col = sex, group = sex), fun = &quot;mean&quot;, geom = &quot;point&quot;, size = 2) + geom_text(data = untreated_weight_stats_pos, mapping = aes(x = age, y = max, label = stats)) + labs(x = &quot;&quot;, y = &quot;Weight (g)&quot;, title = &quot;Untreated mice weights&quot;) + theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;, size = 16)) untreated_weight_lineplot I find it very clear, and almost ready to be published! Kruskal Wallis It‚Äôs now time to check whether T1-treated females Task3 completing time changes over time. We‚Äôll use kruskal.test() function, which needs as inputs the dependent variable and the independent one (remember: kruskal wallis is the corresponding of the one-way ANOVA, there is not a non parametric test resembling the two-way ANOVA). kruskal_res &lt;- kruskal.test(x = females_t1_task3_df$Task3, g = females_t1_task3_df$age) kruskal_res Kruskal-Wallis rank sum test data: females_t1_task3_df$Task3 and females_t1_task3_df$age Kruskal-Wallis chi-squared = 28.733, df = 3, p-value = 2.549e-06 We can say that T1-treated females Task3 time changes in time. How? let‚Äôs check it through post-hoc test. Post-hoc test The non-parametric post-hoc test is the Dunn‚Äôs test. We need to install the dunn.test library (I won‚Äôt show how to do so, as you should know‚Ä¶). The function is dunn.test and requires different inputs: x and g as kruskal.test, method which indicates the p-value correction method. By default, it prints out the output, even when assigning it to a variable; for this reason, we will encapsulate all into capture.output function which will capture the output and put into a file (usually, we will instead use nullfile() to not put it anywhere). library(dunn.test) capture.output(dunn_res &lt;- as.data.frame( dunn.test(x = females_t1_task3_df$Task3, g = females_t1_task3_df$age, method = &quot;bonferroni&quot;) ), file = nullfile()) dunn_res chi2 Z P P.adjusted comparisons 1 28.73258 -3.267054 5.433646e-04 3.260188e-03 15 - 3 2 28.73258 -5.265649 6.984754e-08 4.190852e-07 15 - 30 3 28.73258 -1.976887 2.402723e-02 1.441634e-01 3 - 30 4 28.73258 -3.445552 2.849468e-04 1.709681e-03 15 - 7 5 28.73258 -0.156790 4.377052e-01 1.000000e+00 3 - 7 6 28.73258 1.832353 3.344941e-02 2.006964e-01 30 - 7 So here we are. It seems that it is only at age of 15 that the T1-treated females change their Task3 completition time. Let‚Äôs now put these info into our plot, to make a great figure. I know that there are some packages to perform it, but I want to show how to do it manually. # 1. Create a df for the statistics dunn_stat_df &lt;- dunn_res %&gt;% filter(P.adjusted &lt;= 0.05) %&gt;% separate(col = comparisons, into = c(&quot;x&quot;, &quot;xend&quot;), sep = &quot; - &quot;) %&gt;% mutate(stats = pvalue_to_plot(P.adjusted), y_segment = c(max(females_t1_task3_df$Task3) + c(5, 20, 35)), y_stats = y_segment + 5) %&gt;% select(x, xend, stats, y_segment, y_stats) %&gt;% rowwise() %&gt;% mutate(x_stats = mean(match(c(x, xend), levels(females_t1_task3_df$age))) ) # 2. Remove Shapiro results from plot females_t1_task3_boxplot &lt;- delete_layers(females_t1_task3_boxplot, &quot;GeomText&quot;) # 3. Add stats females_t1_task3_boxplot &lt;- females_t1_task3_boxplot + geom_segment(data = dunn_stat_df, mapping = aes(x = x, xend = xend, y = y_segment, yend = y_segment), col = &quot;black&quot;) + geom_text(data = dunn_stat_df, mapping = aes(x = x_stats, y = y_stats, label = stats), col = &quot;black&quot;) + labs(subtitle = &quot;&quot;) females_t1_task3_boxplot Another great figure to add to our paper! And that‚Äôs all for this chapter; in the next one we will see correlation. "],["correlation.html", "19 Correlation Pearson‚Äôs test", " 19 Correlation Here we are to one of the most controversial statistical test: the correlation. Why controversial? ‚ÄúPull off the band-aid‚Äù and let‚Äôs answer this question at first: usually correlation is used by some scientist as a synonym of causation, but it is not like this! To express this concept, we have to understand what correlation is. Correlation is the measure of the relation of changes of data between two variables; it can answer these questions: ‚ÄúIf A changes, does B do the same?‚Äù, ‚ÄúIf A increases, does B increase? Or decrease?‚Äù So, many people then say ‚ÄúAn increase in A leads to an increase of B‚Äù, but it is totally wrong if we do not know and demonstrates that B is caused by A. Remember: correlation is not causation. In fact, A and B could be two variables that in reality depends on another variable C, but that are not one causative to the other. To make an example, let‚Äôs think about the expression of two genes that are downstream the same transcription factor: they both increase when the level of that transcription factor decrease. We cannot say that an increment of one of the two genes causes an increment of the levels of the second one! I know this may sound obvious, but you will see (and read) lots of people trying to explain causation through correlation‚Ä¶ but the test to use are others, with different ground knowledge of the phenomena. So why using correlation? When you don‚Äôt know the true causative relation between two variables and you want to see if there is any kind of relationship between the variances of them. Moreover, it is very useful when you want to perform dimensionality reduction or you want to create a model, as you want to get the correlated variables to exclude one of them, as two variables that are one the linear transformed of the other are can be reduced to just one, to optimize the model (as the second one does not add any information). Correlation is expressed by a correlation index, which is a value ranging from -1 to 1. A positive correlation means that when one increases, the other does too; instead, a negative correlation means that when one increases, the other one decreases. A correlation score of 0 is usually found when it does not matter if a variable increases or decreases, the other one remains almost the same. There are 3 types of test we can use, based on the type of data we have: : Pearson: variables should be continuous, with normal distribution and a linear relation Spearman: (at least) one of the two variables is ordinal Kendall: both variables are ordinal We‚Äôll see in detail the Pearson test, as in our example we have two continuous variables, then we will briefly see the other two tests. Let‚Äôs load the data: # 1. Load packages suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(ggplot2)) suppressPackageStartupMessages(library(ggpubr)) suppressPackageStartupMessages(library(gginnards)) suppressPackageStartupMessages(library(glue)) # 2. Load data df &lt;- read.csv(&quot;data/Stat-test-dataset.csv&quot;) # 3. Change come column types df &lt;- df %&gt;% mutate(&quot;sex&quot; = factor(sex), &quot;treatment&quot; = factor(treatment, levels = c(&quot;T1&quot;, &quot;Untreated&quot;)), &quot;Task1&quot; = factor(Task1, levels = c(1, 0)), &quot;Task2&quot; = factor(Task2, levels = c(1, 0)), ) str(df) &#39;data.frame&#39;: 600 obs. of 7 variables: $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 2 1 2 1 2 1 2 1 ... $ age : int 3 3 3 3 3 3 3 3 3 3 ... $ treatment: Factor w/ 2 levels &quot;T1&quot;,&quot;Untreated&quot;: 2 2 1 1 2 2 1 1 2 2 ... $ weight : num 6.43 3.42 4.97 4.63 7.31 3.27 5.58 3.27 5.9 4.22 ... $ Task1 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 2 1 1 1 1 2 1 ... $ Task2 : Factor w/ 2 levels &quot;1&quot;,&quot;0&quot;: 1 2 2 1 1 2 2 1 1 1 ... $ Task3 : num 222.4 202.3 36.7 221.8 178.8 ... Pearson‚Äôs test We want to know if weight and Task3 are correlated in both males and females. The first thing to do is, as always, plot the variables to have a look at them: ggplot(df) + geom_point(aes(x = weight, y = Task3, color = sex)) + labs(y = &quot;Task3 (s)&quot;, x = &quot;Weight (g)&quot;, color = &quot;Sex&quot;) + theme_classic() Just visually, we can say that there is no positive or negative correlation between them. To claim it, let‚Äôs apply the Pearson‚Äôs test: the function to use is cor.test(), specifying method = \"pearson\"; the order of the variables is not important. pearson_res &lt;- cor.test(df$weight, df$Task3, method = &quot;pearson&quot;) pearson_res Pearson&#39;s product-moment correlation data: df$weight and df$Task3 t = 1.2917, df = 598, p-value = 0.197 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.02741317 0.13223302 sample estimates: cor 0.05274695 You should now be able to understand these results, even if it is the first time looking at them: p-value is &gt; 0.05, so there is no significance. Moreover, correlation score is very close to 0 (0.05), meaning that even if weight increases, Task3 times remains pretty the same (this is what was obvious from the graph). To apply the other two types of test, just specify ‚Äúkendall‚Äù or ‚Äúspearman‚Äù in the method argument. "],["farewell.html", "20 Farewell", " 20 Farewell Well, this has been an incredible journey into R programming language. I hope I have introduced you well to this powerful tool, and that you have appreciated the potentialities of it for your work. In the next future, I will upload more examples of data analysis, plots, functions, codes etc. Stay tuned, and good luck with your analysis! You can reach me whenever you want at miotsdata@gmail.com, or on linkedin. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
